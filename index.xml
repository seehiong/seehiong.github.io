<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/</link>
    <description>Recent content on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Sep 2024 08:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Talos Linux: Setting Up a Secure, Immutable Kubernetes Cluster</title>
      <link>https://seehiong.github.io/2024/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</link>
      <pubDate>Sun, 01 Sep 2024 08:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</guid>
      <description>&lt;p&gt;In this post, I will walk you through the process of setting up a Kubernetes cluster using &lt;a href=&#34;https://www.talos.dev/&#34; target=&#34;_blank&#34;&gt;Talos Linux&lt;/a&gt;, an operating system specifically designed for Kubernetes that is secure, immutable, and minimal by design. Talos Linux is distinguished by its unique architecture: it is hardened by default, has no shell (bash), no SSH access, and no systemd. Instead, all management is conducted through an API.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After downloading the &lt;a href=&#34;https://www.talos.dev/v1.7/talos-guides/install/bare-metal-platforms/iso/&#34; target=&#34;_blank&#34;&gt;ISO&lt;/a&gt; image, I used &lt;a href=&#34;https://etcher.balena.io/&#34; target=&#34;_blank&#34;&gt;balenaEtcher&lt;/a&gt; to create a bootable USB installation media. My setup consists of one control plane node and two worker nodes. The following IP addresses were assigned:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Audio Generation with NVIDIA Jetson Orin NX</title>
      <link>https://seehiong.github.io/2024/audio-generation-with-nvidia-jetson-orin-nx/</link>
      <pubDate>Sun, 25 Aug 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/audio-generation-with-nvidia-jetson-orin-nx/</guid>
      <description>&lt;p&gt;Building on my exploration of &lt;a href=&#34;https://seehiong.github.io/2024/unleashing-text-generation-with-nvidia-jetson-orin-nx/&#34; target=&#34;_blank&#34;&gt;text generation with NVIDIA Jetson Orin NX&lt;/a&gt;, this post delves into the audio generation capabilities of the Jetson platform.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;transcribing-audio-with-whisper&#34;&gt;Transcribing Audio with Whisper&lt;/h2&gt;&#xA;&lt;p&gt;Following the &lt;a href=&#34;https://www.jetson-ai-lab.com/tutorial_whisper.html&#34; target=&#34;_blank&#34;&gt;Tutorial Whisper&lt;/a&gt;, after starting the container with the command below, you can access Jupyter Lab at https://192.168.68.100:8888 (password: &lt;em&gt;nvidia&lt;/em&gt;):&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;jetson-containers run &lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;autotag whisper&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Unleashing Text Generation with NVIDIA Jetson Orin NX</title>
      <link>https://seehiong.github.io/2024/unleashing-text-generation-with-nvidia-jetson-orin-nx/</link>
      <pubDate>Sat, 17 Aug 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/unleashing-text-generation-with-nvidia-jetson-orin-nx/</guid>
      <description>&lt;p&gt;Navigating through the &lt;a href=&#34;https://www.jetson-ai-lab.com/&#34; target=&#34;_blank&#34;&gt;NVIDIA Jetson AI Lab&lt;/a&gt; has been an exhilarating experience, showcasing the potential of generative AI powered by NVIDIA® Jetson™. With a plethora of labs to explore, it’s challenging to cover everything in a limited time. In this post, I&amp;rsquo;ll focus on labs related to text generation.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;If you follow my &lt;a href=&#34;https://seehiong.github.io/2024/exploring-nvidia-jetson-orin-nx-flashing-and-setup-guide/&#34; target=&#34;_blank&#34;&gt;Jetson Orin NX flashing guide&lt;/a&gt;, you might have noticed that a browser is not pre-installed. I recommend installing  &lt;a href=&#34;https://brave.com/&#34; target=&#34;_blank&#34;&gt;Brave&lt;/a&gt;, a browser that blocks ads and conserves data. To install it, simply run:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scaling Kafka Workloads with KEDA in Kubernetes</title>
      <link>https://seehiong.github.io/2024/scaling-kafka-workloads-with-keda-in-kubernetes/</link>
      <pubDate>Sat, 10 Aug 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/scaling-kafka-workloads-with-keda-in-kubernetes/</guid>
      <description>&lt;p&gt;Building up my previous &lt;a href=&#34;https://seehiong.github.io/2024/setting-up-kafka-with-microk8s-and-multipass/&#34; target=&#34;_blank&#34;&gt;Kafka post&lt;/a&gt;, this article focuses on leveraging KEDA to scale Kafka consumer workloads dynamically. &lt;a href=&#34;https://keda.sh/&#34; target=&#34;_blank&#34;&gt;KEDA&lt;/a&gt; is a Kubernetes-based Event Driven Autoscaler that enables automatic scaling of pods based on the volume of events to be processed.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;keda-setup&#34;&gt;KEDA Setup&lt;/h2&gt;&#xA;&lt;p&gt;To begin, after accessing the &lt;em&gt;mk8s-vm&lt;/em&gt;, simply install KEDA with the following commands:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo microk8s enable community&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo microk8s enable keda&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Disable KEDA if necessary&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo microk8s disable keda&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Exploring NVIDIA Jetson Orin NX: Flashing and Setup Guide</title>
      <link>https://seehiong.github.io/2024/exploring-nvidia-jetson-orin-nx-flashing-and-setup-guide/</link>
      <pubDate>Fri, 09 Aug 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/exploring-nvidia-jetson-orin-nx-flashing-and-setup-guide/</guid>
      <description>&lt;p&gt;On this special &lt;a href=&#34;https://www.visitsingapore.com/whats-happening/all-happenings/festivals/singapore-national-day/&#34; target=&#34;_blank&#34;&gt;Singapore National Day&lt;/a&gt;, I am delighted to extend my best wishes as we celebrate Singapore&amp;rsquo;s 59th birthday. Today, I&amp;rsquo;m particularly excited to share that I’ve finally received the long-awaited &lt;a href=&#34;https://files.seeedstudio.com/wiki/reComputer/reComputer-J40.pdf&#34; target=&#34;_blank&#34;&gt;NVIDIA® Jetson Orin™ NX&lt;/a&gt; board. I’ll be documenting the flashing process for this powerful device. &lt;a href=&#34;https://www.nvidia.com/en-sg/autonomous-machines/embedded-systems/jetson-orin/&#34; target=&#34;_blank&#34;&gt;NVIDIA Jetson Orin&lt;/a&gt; is a game-changer, bringing next-generation products to life with the world’s most advanced embedded AI computers, perfect for generative AI, computer vision, and cutting-edge robotics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setting Up Kafka with MicroK8s and Multipass</title>
      <link>https://seehiong.github.io/2024/setting-up-kafka-with-microk8s-and-multipass/</link>
      <pubDate>Sat, 03 Aug 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/setting-up-kafka-with-microk8s-and-multipass/</guid>
      <description>&lt;p&gt;My homelab is a playground for experimenting with various tools and setups. However, for Proof of Concept (POC) environments, a lightweight and portable setup is often more suitable. In this post, I will guide you through setting up a MicroK8s environment in a virtual machine using Multipass. This POC demonstrates how Kafka can be set up in this environment. &lt;a href=&#34;https://multipass.run/&#34; target=&#34;_blank&#34;&gt;Multipass&lt;/a&gt; is a CLI tool for launching and managing VMs on Windows, Mac, and Linux, simulating a cloud environment with support for cloud-init.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Your First Kubeflow Pipeline: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/2024/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</link>
      <pubDate>Sat, 20 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.kubeflow.org/docs/components/pipelines/overview/&#34; target=&#34;_blank&#34;&gt;Kubeflow Pipelines (KFP)&lt;/a&gt; is a powerful platform for creating and deploying scalable machine learning (ML) workflows using Docker containers. It enables data scientists and ML engineers to author workflows in Python, manage and visualize pipeline runs, and efficiently utilize compute resources. KFP supports custom ML components, leverages existing ones, and ensures cross-platform portability with a platform-neutral &lt;a href=&#34;https://www.kubeflow.org/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/#ir-yaml&#34; target=&#34;_blank&#34;&gt;IR YAML definition&lt;/a&gt;. In this post, I’ll share my learnings about KFP v2.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integrating Draw.io and PlantUML with GitLab</title>
      <link>https://seehiong.github.io/2024/integrating-draw-io-and-plantuml-with-gitlab/</link>
      <pubDate>Sat, 06 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integrating-draw-io-and-plantuml-with-gitlab/</guid>
      <description>&lt;p&gt;As we are migrating away from &lt;a href=&#34;https://www.lucidchart.com/&#34; target=&#34;_blank&#34;&gt;Lucidchart&lt;/a&gt; to &lt;a href=&#34;https://www.drawio.com/&#34; target=&#34;_blank&#34;&gt;draw.io&lt;/a&gt;, a security-first diagramming for teams, I will be documenting the steps to integrate draw.io with GitLab.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;configure-diagramsnet-server&#34;&gt;Configure Diagrams.net Server&lt;/h2&gt;&#xA;&lt;p&gt;Referencing the official &lt;a href=&#34;https://docs.gitlab.com/ee/administration/integration/diagrams_net.html&#34; target=&#34;_blank&#34;&gt;Diagrams.net&lt;/a&gt; documentation, I run the diagrams.net container in Docker, using the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --rm --name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;draw&amp;#34;&lt;/span&gt; -p 8888:8080 -p 8443:8443 jgraph/drawio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Setting Up and Using KServe with Kubeflow</title>
      <link>https://seehiong.github.io/2024/setting-up-and-using-kserve-with-kubeflow/</link>
      <pubDate>Sun, 30 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/setting-up-and-using-kserve-with-kubeflow/</guid>
      <description>&lt;p&gt;Expanding on my previous post on &lt;a href=&#34;https://seehiong.github.io/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/&#34; target=&#34;_blank&#34;&gt;Kubeflow&lt;/a&gt;, I will explore &lt;a href=&#34;https://kserve.github.io/website/latest/&#34; target=&#34;_blank&#34;&gt;KServe&lt;/a&gt;, a standard Model Inference Platform on Kubernetes built for highly scalable use cases.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;first-kserve-endpoint&#34;&gt;First KServe Endpoint&lt;/h2&gt;&#xA;&lt;p&gt;Referencing &lt;a href=&#34;https://github.com/kserve/kserve/blob/master/docs/samples/istio-dex/README.md&#34; target=&#34;_blank&#34;&gt;KServe on Kubeflow with Istio-Dex&lt;/a&gt;, below is the &lt;em&gt;sklearn.yaml&lt;/em&gt; configuration. Note the sidecar annotation, which instructs not to inject the istio sidecar. Without this annotation, you may encounter error (refer to the troubleshooting section):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setting Up Kubeflow on Kubernetes: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</link>
      <pubDate>Mon, 24 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</guid>
      <description>&lt;p&gt;The car inspection went well, and I will spend the rest of my half-day leave documenting the steps for setting up &lt;a href=&#34;https://www.kubeflow.org/&#34; target=&#34;_blank&#34;&gt;Kubeflow&lt;/a&gt;, the machine learning toolkit for kubernetes.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt; introduces a template-free way to customize application configuration, simplifying the use of off-the-shelf application. The simplest way to get started is to download the precompiled binaries:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -s &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh&amp;#34;&lt;/span&gt;  | bash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Moves kustomize to a system-wide location&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo mv kustomize /usr/local/bin/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Building Advanced RAG Applications with MyScaleDB and LlamaIndex</title>
      <link>https://seehiong.github.io/2024/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</link>
      <pubDate>Sat, 15 Jun 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</guid>
      <description>&lt;p&gt;In this post, I will explore &lt;a href=&#34;https://github.com/myscale/myscaledb&#34; target=&#34;_blank&#34;&gt;MyScaleDB&lt;/a&gt;, an open-source, high-performance SQL vector database built on ClickHouse, and &lt;a href=&#34;https://www.llamaindex.ai/&#34; target=&#34;_blank&#34;&gt;LlamaIndex&lt;/a&gt;, the leading data framework for building LLM applications.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;After installing &lt;a href=&#34;https://vscodium.com/#install&#34; target=&#34;_blank&#34;&gt;VSCodium&lt;/a&gt; as my primary IDE, I proceeded with installing the Python extension via &lt;a href=&#34;https://open-vsx.org/vscode/item?itemName=ms-python.python&#34; target=&#34;_blank&#34;&gt;Marketplace Link&lt;/a&gt;.&#xA;Next, I created the &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34; target=&#34;_blank&#34;&gt;virtual environment&lt;/a&gt; using venv:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create the envrionment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python -m venv myscaledb&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Activate the environment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;myscaledb&lt;span style=&#34;color:#ae81ff&#34;&gt;\S&lt;/span&gt;cripts&lt;span style=&#34;color:#ae81ff&#34;&gt;\a&lt;/span&gt;ctivate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Planning Gift Deliveries With QGIS</title>
      <link>https://seehiong.github.io/2024/planning-gift-deliveries-with-qgis/</link>
      <pubDate>Sat, 08 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/planning-gift-deliveries-with-qgis/</guid>
      <description>&lt;p&gt;In this post, I will document my journey into learning &lt;a href=&#34;https://qgis.org/en/site/&#34; target=&#34;_blank&#34;&gt;QGIS&lt;/a&gt;, a free and open-source geographic information system, with the goal of visualizing and calculating the total time required for me to deliver gifts to friends and relatives during special occasions.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After installing QGIS 3.36 desktop version on my Windows PC, I proceeded to the &lt;a href=&#34;https://docs.qgis.org/3.34/en/docs/training_manual/index.html&#34; target=&#34;_blank&#34;&gt;QGIS Training Manual&lt;/a&gt;, to familiarize myself with the software.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Routing Models to MIP: Solving Capacitated Vehicle Routing Problem</title>
      <link>https://seehiong.github.io/2024/from-routing-models-to-mip-solving-capacitated-vehicle-routing-problem/</link>
      <pubDate>Sat, 01 Jun 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/from-routing-models-to-mip-solving-capacitated-vehicle-routing-problem/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;&#xA;&lt;p&gt;In this post, I will follow the &lt;a href=&#34;https://developers.google.com/optimization/routing/vrp&#34; target=&#34;_blank&#34;&gt;Vehicle Routing Problem&lt;/a&gt; (VRP) with a focus on the capacitated vehicle routing problem (CVRP) and utilizing an alternative Mixed Integer Programming (MIP) approach.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;routing-model&#34;&gt;Routing Model&lt;/h2&gt;&#xA;&lt;p&gt;The main section of the program creates the index manager and the routing model.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Create Routing Index Manager&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;RoutingIndexManager manager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; RoutingIndexManager(data.&lt;span style=&#34;color:#a6e22e&#34;&gt;distanceMatrix&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;, data.&lt;span style=&#34;color:#a6e22e&#34;&gt;vehicleNumber&lt;/span&gt;, data.&lt;span style=&#34;color:#a6e22e&#34;&gt;depot&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Create Routing Model&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;RoutingModel routing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; RoutingModel(manager);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Solving Facility Location Problem with OR-Tools and Micronaut</title>
      <link>https://seehiong.github.io/2024/solving-facility-location-problem-with-or-tools-and-micronaut/</link>
      <pubDate>Mon, 27 May 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/solving-facility-location-problem-with-or-tools-and-micronaut/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;&#xA;&lt;p&gt;In this post, I&amp;rsquo;ll tackle the &lt;a href=&#34;https://en.wikipedia.org/wiki/Facility_location_problem&#34; target=&#34;_blank&#34;&gt;Facility Location Problem&lt;/a&gt;, which involves deciding the optimal placement of facilities to minimize costs. Unlike my previous post on using a &lt;a href=&#34;https://seehiong.github.io/2024/optimizing-tsp-with-genetic-algorithms-in-micronaut/&#34; target=&#34;_blank&#34;&gt;genetic algorithm for the TSP&lt;/a&gt;, I&amp;rsquo;ll utilize &lt;a href=&#34;https://developers.google.com/optimization/&#34; target=&#34;_blank&#34;&gt;OR-Tools&lt;/a&gt; to solve this problem.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;defining-the-solver&#34;&gt;Defining the solver&lt;/h2&gt;&#xA;&lt;p&gt;Referencing the &lt;a href=&#34;https://developers.google.com/optimization/mip/mip_example&#34; target=&#34;_blank&#34;&gt;MIP example&lt;/a&gt;,  I&amp;rsquo;ll solve the FLP using the MIP approach. Let&amp;rsquo;s place all solver codes in &lt;strong&gt;FLPService.java&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optimizing TSP with Genetic Algorithms in Micronaut</title>
      <link>https://seehiong.github.io/2024/optimizing-tsp-with-genetic-algorithms-in-micronaut/</link>
      <pubDate>Thu, 23 May 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/optimizing-tsp-with-genetic-algorithms-in-micronaut/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;&#xA;&lt;p&gt;Research suggests that the  &lt;a href=&#34;https://www.math.uwaterloo.ca/tsp/pla85900/index.html&#34; target=&#34;_blank&#34;&gt;Optimal 85,900-Point Tour&lt;/a&gt; is the largest solved instance of the Traveling Salesman Problem (TSP). For this post, I will attempt to solve the TSP problem involving 200 cities.&lt;/p&gt;&#xA;&lt;p&gt;Genetic algorithms simulate the process of natural selection, mimicking &amp;ldquo;survival of the fittest&amp;rdquo; to solve problems. These algorithms evolve over generations, with each generation comprising individuals better adapted to solving the problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Efficient TSP Solver API with Micronaut</title>
      <link>https://seehiong.github.io/2024/efficient-tsp-solver-api-with-micronaut/</link>
      <pubDate>Sat, 27 Apr 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/efficient-tsp-solver-api-with-micronaut/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;m embarking on a journey to solve the &lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34; target=&#34;_blank&#34;&gt;Travelling Salesman Problem (TSP)&lt;/a&gt; utilizing &lt;a href=&#34;https://choco-solver.org/&#34; target=&#34;_blank&#34;&gt;Choco-solver&lt;/a&gt;, an effective Java library for constraint programming. Additionally, I&amp;rsquo;ll be converting this solution into an API using &lt;a href=&#34;https://micronaut.io/&#34; target=&#34;_blank&#34;&gt;Micronaut&lt;/a&gt;, a cutting-edge, JVM-based framework for building modular, testable microservices and serverless applications.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-with-micronaut&#34;&gt;Setting up with Micronaut&lt;/h2&gt;&#xA;&lt;p&gt;Following the &lt;a href=&#34;https://guides.micronaut.io/latest/creating-your-first-micronaut-app-gradle-java.html&#34; target=&#34;_blank&#34;&gt;official Micronaut guide&lt;/a&gt;, I downloaded the source and proceeded with the setup.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Log Management with Graylog</title>
      <link>https://seehiong.github.io/2024/log-management-with-graylog/</link>
      <pubDate>Fri, 19 Apr 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/log-management-with-graylog/</guid>
      <description>&lt;p&gt;In this blog post, I&amp;rsquo;ll guide you through the setup of &lt;a href=&#34;https://graylog.org/&#34; target=&#34;_blank&#34;&gt;Graylog&lt;/a&gt;, an open-source log management platform, within a HomeLab environment, providing a comprehensive solution for log analysis and monitoring.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-graylog-with-docker&#34;&gt;Setting up Graylog with Docker&lt;/h2&gt;&#xA;&lt;p&gt;To initiate our exploration of Graylog, we&amp;rsquo;ll opt for a &lt;a href=&#34;https://go2docs.graylog.org/5-2/downloading_and_installing_graylog/docker_installation.htm&#34; target=&#34;_blank&#34;&gt;Docker Installation&lt;/a&gt;, which ensures simplicity and ease of deployment. Follow the steps outlined in the official documentation to set up Graylog via Docker. Upon successful installation, access the Graylog interface by navigating to &lt;em&gt;http://localhost:9000/&lt;/em&gt;, and use the default credentials: &lt;strong&gt;admin/admin&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Coding with CrewAI: AI Orchestration Simplified</title>
      <link>https://seehiong.github.io/2024/coding-with-crewai-ai-orchestration-simplified/</link>
      <pubDate>Fri, 29 Mar 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/coding-with-crewai-ai-orchestration-simplified/</guid>
      <description>&lt;p&gt;Welcome to an exploration of &lt;a href=&#34;https://www.crewai.io/&#34; target=&#34;_blank&#34;&gt;CrewAI&lt;/a&gt;, a state-of-the-art framework designed to orchestrate autonomous AI agents. In this post, we&amp;rsquo;ll dive into the practical aspects of CrewAI, discovering its functionalities and potential applications.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To dive into the world of AI-driven creativity, let&amp;rsquo;s start by setting up our environment. We&amp;rsquo;ll create a dedicated Conda environment to ensure seamless integration with CrewAI:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a new Conda environment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n crewai python&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3.10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Active the environment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate crewai&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>AutoPilot Setup for VS Code</title>
      <link>https://seehiong.github.io/2024/autopilot-setup-for-vs-code/</link>
      <pubDate>Sat, 16 Mar 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/autopilot-setup-for-vs-code/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;m going to demonstrate the setup of &lt;a href=&#34;https://continue.dev/docs/quickstart&#34; target=&#34;_blank&#34;&gt;Continue&lt;/a&gt;, an open-source autopilot designed for VS Code.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;Once you&amp;rsquo;ve installed the plugin from the market place, let&amp;rsquo;s proceed by adding Continue to the right sidebar of VS Code, as recommended.&lt;/p&gt;&#xA;&lt;h3 id=&#34;provider---lm-studio&#34;&gt;Provider - LM Studio&lt;/h3&gt;&#xA;&lt;p&gt;First, on my Windows machine, I&amp;rsquo;ll execute &lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34;&gt;LM Studio&lt;/a&gt; and download &lt;a href=&#34;https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF&#34; target=&#34;_blank&#34;&gt;Google&amp;rsquo;s Gemma 2B Instruct&lt;/a&gt; model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configuring Appwrite Functions with K3s</title>
      <link>https://seehiong.github.io/2024/configuring-appwrite-functions-with-k3s/</link>
      <pubDate>Sun, 10 Mar 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/configuring-appwrite-functions-with-k3s/</guid>
      <description>&lt;p&gt;Following up on my previous post about &lt;a href=&#34;https://seehiong.github.io/2024/deploying-appwrite-in-homelab-with-k3s/&#34; target=&#34;_blank&#34;&gt;deploying Appwrite with K3s&lt;/a&gt;, I will now guide you through configuring K3s to support Appwrite Functions.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prepartion&#34;&gt;Prepartion&lt;/h2&gt;&#xA;&lt;h3 id=&#34;install-ngrok&#34;&gt;Install Ngrok&lt;/h3&gt;&#xA;&lt;p&gt;Since I am running Appwrite in my HomeLab, I need to utilize &lt;a href=&#34;https://ngrok.com/&#34; target=&#34;_blank&#34;&gt;ngrok&lt;/a&gt; to enable external network access (such as GitHub) to our internal network. After signing up, install ngrok via Chocolatey:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;choco install ngrok&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ngrok config add-authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ngrok http http://appwrite.local/                      &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Deploying Budibase in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-budibase-in-homelab/</link>
      <pubDate>Sun, 25 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-budibase-in-homelab/</guid>
      <description>&lt;p&gt;In this guide, we&amp;rsquo;ll delve into the process of installing &lt;a href=&#34;https://budibase.com/&#34; target=&#34;_blank&#34;&gt;Budibase&lt;/a&gt; within our HomeLab environment. Budibase offers the capability to craft robust applications and workflows from various data sources, enabling the secure deployment of professional-grade solutions across our teams.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;testing-budibase-with-docker-desktop&#34;&gt;Testing Budibase with Docker Desktop&lt;/h2&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start our exploration by testing Budibase using &lt;a href=&#34;https://docs.budibase.com/docs/docker-compose&#34; target=&#34;_blank&#34;&gt;Docker compose&lt;/a&gt;. To begin, download both the &lt;em&gt;docker-compose.yaml&lt;/em&gt; and &lt;em&gt;.env&lt;/em&gt; files, then launch the platform with the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying Appwrite in HomeLab with K3s</title>
      <link>https://seehiong.github.io/2024/deploying-appwrite-in-homelab-with-k3s/</link>
      <pubDate>Fri, 16 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-appwrite-in-homelab-with-k3s/</guid>
      <description>&lt;p&gt;In this post, we&amp;rsquo;ll embark on installing &lt;a href=&#34;https://appwrite.io/&#34; target=&#34;_blank&#34;&gt;Appwrite&lt;/a&gt;, an open-source platform designed to facilitate the integration of authentication, databases, functions, and storage, enabling the development of scalable applications within our HomeLab setup.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prepartion&#34;&gt;Prepartion&lt;/h2&gt;&#xA;&lt;p&gt;Referencing my &lt;a href=&#34;https://seehiong.github.io/archives/2023/setting-up-k3s/&#34; target=&#34;_blank&#34;&gt;previous K3s setup post&lt;/a&gt;, let&amp;rsquo;s initiate the installation process by deploying K3s server, this time with Traefik disabled:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--disable traefik&amp;#34;&lt;/span&gt; K3S_KUBECONFIG_MODE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;644&amp;#34;&lt;/span&gt; sh -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Text-to-Image with StableDiffusionPipeline</title>
      <link>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</link>
      <pubDate>Sat, 10 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll delve into the capabilities of the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline&#34; target=&#34;_blank&#34;&gt;StableDiffusionPipeline&lt;/a&gt; for generating photorealistic images based on textual inputs.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;text-to-image&#34;&gt;Text-to-Image&lt;/h2&gt;&#xA;&lt;p&gt;Continuing from the &lt;a href=&#34;https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I initiated the environment setup:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd stable-diffusion&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate ldm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Subsequently, I installed the necessary libraries, &lt;a href=&#34;https://pypi.org/project/diffusers/&#34; target=&#34;_blank&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://pypi.org/project/transformers/&#34; target=&#34;_blank&#34;&gt;transformers&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install --upgrade diffusers&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; transformers&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Stable Diffusion: Text-to-Image Modeling Journey</title>
      <link>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</link>
      <pubDate>Sat, 03 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</guid>
      <description>&lt;p&gt;In this article, we will delve into &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34; target=&#34;_blank&#34;&gt;Stable Diffusion&lt;/a&gt;, a latent text-to-image diffusion model. In simple terms, diffusion models in machine learning represent a type of sophisticated computer program designed to learn how patterns evolve over time. Comprising three essential components – a forward process, a reverse process, and a sampling procedure – these models aim to comprehend and generate intricate patterns within a given dataset.&lt;/p&gt;&#xA;&lt;p&gt;Consider having a blurry image that needs enhancement. Diffusion models act as intelligent tools that learn to eliminate blurriness by grasping how images blur and then effectively reversing that process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenVINO, Optimum-Intel, CPU: An Exploration in Model Optimization</title>
      <link>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</link>
      <pubDate>Sat, 27 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.openvino.ai/2023.3/home.html&#34; target=&#34;_blank&#34;&gt;OpenVINO&lt;/a&gt; represents an open-source toolkit designed for the optimization and deployment of deep learning models. Acting as the interface between the Transformers and Diffusers libraries, &lt;a href=&#34;https://huggingface.co/docs/optimum/intel/inference&#34; target=&#34;_blank&#34;&gt;Optimum-Intel&lt;/a&gt; seamlessly integrates with various Intel tools and libraries, facilitating the acceleration of end-to-end pipelines on Intel architectures. This post documents my journey as I set up and execute example code on my aging laptop, exploring the application of Quantization-aware Training (QAT) and the Token Merging method to optimize the UNet model within the Stable Diffusion pipeline.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Java Integration with Jupyter Notebooks</title>
      <link>https://seehiong.github.io/2024/java-integration-with-jupyter-notebooks/</link>
      <pubDate>Sun, 21 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/java-integration-with-jupyter-notebooks/</guid>
      <description>&lt;p&gt;In this post, I am delighted to share my journey of seamlessly integrating Java programming within Jupyter notebooks.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;&#xA;&lt;p&gt;Commencing with the selection of a pertinent Jupyter Docker Stack image, as detailed in the &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html&#34; target=&#34;_blank&#34;&gt;Jupyter Docker Stacks documentation&lt;/a&gt;, the following Docker command initializes the setup:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker pull quay.io/jupyter/minimal-notebook:notebook-7.0.6&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Subsequently, the Docker image is run on a Windows WSL environment, with the host IP set to &lt;em&gt;192.168.68.114&lt;/em&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Autogen Studio</title>
      <link>https://seehiong.github.io/2024/exploring-autogen-studio/</link>
      <pubDate>Sun, 14 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/exploring-autogen-studio/</guid>
      <description>&lt;p&gt;In this comprehensive exploration, we delve into the realm of &lt;a href=&#34;https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/&#34; target=&#34;_blank&#34;&gt;Autogen Studio&lt;/a&gt;, a powerful tool designed to streamline the rapid prototyping of multi-agent solutions for various tasks.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;The journey begins with the initial setup. A new Python virtual environment is created using Conda, followed by the installation of Autogen Studio and the essential configuration of API keys.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n autogenstudio python&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3.10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate autogenstudio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install autogenstudio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export OPENAI_API_KEY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sk-xxxx&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;autogenstudio ui --port &lt;span style=&#34;color:#ae81ff&#34;&gt;8081&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>&lt;p&gt;In this post, we delve into the deployment of Lightweight Language Models (LLMs) using &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge&#34; target=&#34;_blank&#34;&gt;WasmEdge&lt;/a&gt;, a lightweight, high-performance, and extensible WebAssembly runtime. This setup is tailored to run LLMs in our previously configured HomeLab environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;To establish an OpenAI-compatible &lt;a href=&#34;https://github.com/LlamaEdge/LlamaEdge/tree/main/llama-api-server&#34; target=&#34;_blank&#34;&gt;API server&lt;/a&gt;, begin by downloading the API server application:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For Rust-based &lt;a href=&#34;https://wasmedge.org/docs/develop/rust/wasinn/llm_inference/&#34; target=&#34;_blank&#34;&gt;Llama 2 inference&lt;/a&gt;, we require the &lt;a href=&#34;https://wasmedge.org/docs/contribute/source/plugin/wasi_nn/&#34; target=&#34;_blank&#34;&gt;Wasi-NN&lt;/a&gt; plugin. The &lt;em&gt;Dockerfile&lt;/em&gt; below reflects this configuration:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integrating NFS for Improved Scalability</title>
      <link>https://seehiong.github.io/2024/integrating-nfs-for-improved-scalability/</link>
      <pubDate>Sun, 07 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integrating-nfs-for-improved-scalability/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;ve been following the &lt;a href=&#34;https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, you might have observed that deploying LLM may not be as scalable. In this post, we delve into the integration of NFS (Network File System) to externalize model environment variables. This approach eliminates the need to rebuild a new image each time a new LLM (Language Model) is introduced into your workflow.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-nfs&#34;&gt;Setting up NFS&lt;/h2&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start by setting up NFS to connect to my recently acquired TerraMaster NAS.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integration of Kong into AI Workflow</title>
      <link>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</link>
      <pubDate>Sat, 06 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</guid>
      <description>&lt;p&gt;This post will guide you through the process of configuring Kong Gateway OSS and Kong Ingress Controller (KIC) separately and integrating Kong into our AI workflow.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;integrate-via-kong-gateway-oss-configuration&#34;&gt;Integrate via Kong Gateway OSS Configuration&lt;/h2&gt;&#xA;&lt;p&gt;If you followed my earlier guide on &lt;a href=&#34;https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/&#34; target=&#34;_blank&#34;&gt;setting up Kong Gateway&lt;/a&gt; setup, you likely use api.local:8000 to access the API.&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s revisit and update &lt;em&gt;KONG_ADMIN_GUI_URL&lt;/em&gt; environment  variable in the &lt;em&gt;kong-deploy-svc.yaml&lt;/em&gt; file:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Kong Ingress Controller (KIC)</title>
      <link>https://seehiong.github.io/2024/exploring-kong-ingress-controller-kic/</link>
      <pubDate>Mon, 01 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/exploring-kong-ingress-controller-kic/</guid>
      <description>&lt;p&gt;Wishing everyone a Happy New Year 2024! In this post, I shift focus from my previous discussion on &lt;a href=&#34;https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/&#34; target=&#34;_blank&#34;&gt;Kong Gateway&lt;/a&gt; to delve into the setup of the Kong Ingress Controller (KIC). Keeping it concise and celebratory for the New Year!&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt; serves as a Kubernetes package manager. To install it, execute the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo snap install helm --classic&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Streamlining API Management with Kong</title>
      <link>https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/</link>
      <pubDate>Sun, 31 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/</guid>
      <description>&lt;p&gt;In this comprehensive guide, we will walk through the process of integrating &lt;a href=&#34;https://konghq.com/&#34; target=&#34;_blank&#34;&gt;Kong&lt;/a&gt;, a robust unified API platform, into our home lab environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prerequistes&#34;&gt;Prerequistes&lt;/h2&gt;&#xA;&lt;p&gt;To begin, I will start with a fresh Ubuntu server instance. We&amp;rsquo;ll start by installing Docker and configuring it for non-root usage:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run Docker without sudo by logging back in or executing this&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;su - pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>AI Integration: LocalAI, Chroma, and Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/ai-integration-localai-chroma-langchain4j/</link>
      <pubDate>Fri, 29 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/ai-integration-localai-chroma-langchain4j/</guid>
      <description>&lt;p&gt;Referring to the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;Building an AI application with Langchaing4j&lt;/a&gt; guide, the deployment of necessary Docker images, LocalAI, and Chroma to our Home Lab is outlined.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;creating-custom-localai-image&#34;&gt;Creating custom LocalAI image&lt;/h2&gt;&#xA;&lt;p&gt;Begin with pulling the latest image using the &lt;a href=&#34;https://localai.io/howtos/easy-setup-docker/&#34; target=&#34;_blank&#34;&gt;easy docker setup&lt;/a&gt; guide:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker pull quay.io/go-skynet/local-ai:v2.2.0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, run LocalAI from the &lt;em&gt;~/localai&lt;/em&gt; folder and download a model:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrading My Tech Blog: Hugo, Giscus, and Pagefind Integration</title>
      <link>https://seehiong.github.io/archives/2023/upgrading-my-tech-blog-hugo-giscus-pagefind-integration/</link>
      <pubDate>Tue, 26 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/upgrading-my-tech-blog-hugo-giscus-pagefind-integration/</guid>
      <description>&lt;p&gt;In this post, I will provide updates on my transition to utilizing &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt; for my tech blog.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setup-process&#34;&gt;Setup Process&lt;/h2&gt;&#xA;&lt;p&gt;To begin, I recommend installing &lt;a href=&#34;https://chocolatey.org/install&#34; target=&#34;_blank&#34;&gt;Chocolately&lt;/a&gt;, a free and open-source package manager designed for Windows.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Set-ExecutionPolicy Bypass -Scope Process -Force; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;System.Net.ServicePointManager&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;::SecurityProtocol &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;System.Net.ServicePointManager&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;::SecurityProtocol -bor 3072; iex &lt;span style=&#34;color:#f92672&#34;&gt;((&lt;/span&gt;New-Object System.Net.WebClient&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;.DownloadString&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://community.chocolatey.org/install.ps1&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>GitLab Setup: Installation, Migration, and CI/CD Simplified</title>
      <link>https://seehiong.github.io/archives/2023/gitlab-setup-installation-migration-and-ci-cd-simplified/</link>
      <pubDate>Sun, 24 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/gitlab-setup-installation-migration-and-ci-cd-simplified/</guid>
      <description>&lt;p&gt;In this guide, I&amp;rsquo;ll walk you through the process of installing &lt;a href=&#34;https://docs.gitlab.com/omnibus/installation/&#34; target=&#34;_blank&#34;&gt;GitLab&lt;/a&gt;, a comprehensive suite of tools for version control, continuous integration, continuous delivery, and more, in my Home Lab collection.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After obtaining the latest &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34;&gt;Ubuntu Server&lt;/a&gt;, I utilized &lt;a href=&#34;https://rufus.ie/en/&#34; target=&#34;_blank&#34;&gt;Rufus&lt;/a&gt;, a utility for formatting and creating bootable USB flash drives.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Following the &lt;a href=&#34;https://packages.gitlab.com/gitlab/gitlab-ce/install&#34; target=&#34;_blank&#34;&gt;installation instructions&lt;/a&gt;, initiate a quick installation using the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying OpenAI-Compatible LLAMA CPP Server with K3S</title>
      <link>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</link>
      <pubDate>Fri, 22 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</guid>
      <description>&lt;p&gt;Commencing my week-long Christmas break, I extend the concepts from my &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; to establish an OpenAI-compatible server in my &lt;a href=&#34;https://seehiong.github.io/archives/2023/setting-up-k3s/&#34; target=&#34;_blank&#34;&gt;Home Lab&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;technical-setup&#34;&gt;Technical Setup&lt;/h2&gt;&#xA;&lt;p&gt;After fine-tuning a sample &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python/blob/main/docker/openblas_simple/Dockerfile&#34; target=&#34;_blank&#34;&gt;Dockerfile&lt;/a&gt;, I reinstalled my Ubuntu server, incorporating necessary adjustments. The subsequent setup commands, reflecting my Home Lab&amp;rsquo;s new IP address (192.168.68.115), include:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt update &amp;amp; sudo apt upgrade -y&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install docker&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install Anaconda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chmod +x Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;./Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Init conda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source /home/pi/anaconda3/bin/activate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda init&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n docker-llama python&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate docker-llama &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Unveiling Agent AutoBuild in Autogen</title>
      <link>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</link>
      <pubDate>Sun, 17 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</guid>
      <description>&lt;p&gt;Discover the capabilities of &lt;a href=&#34;https://microsoft.github.io/autogen/blog/2023/11/26/Agent-AutoBuild/&#34; target=&#34;_blank&#34;&gt;Agent AutoBuild&lt;/a&gt; in my recent exploration with Autogen using &lt;em&gt;app.py&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setup-configuration&#34;&gt;Setup Configuration&lt;/h2&gt;&#xA;&lt;p&gt;In my model setup configuration, defined in &lt;em&gt;OAI_CONFIG_LIST&lt;/em&gt;, I&amp;rsquo;m leveraging the latest version of Autogen (&lt;em&gt;pyautogen==0.2.2&lt;/em&gt;) with the following specifications:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gpt-4&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;api_key&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NULL&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://192.168.68.114:1234/v1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>&lt;p&gt;In the pursuit of enhancing Autogen&amp;rsquo;s capabilities, I drew inspiration from &lt;a href=&#34;https://github.com/0xlws/autogen&#34; target=&#34;_blank&#34;&gt;0xlws&amp;rsquo; fork&lt;/a&gt; supporting JavaScript. This led me to embark on a journey to modify Autogen, enabling robust support for Java code execution.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up&#34;&gt;Setting up&lt;/h2&gt;&#xA;&lt;p&gt;Begin by ensuring that Java is installed on your Windows Subsystem for Linux (WSL) using the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install openjdk-17-jdk-headless&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Multi-agent Conservation with Autogen</title>
      <link>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</link>
      <pubDate>Fri, 08 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post &lt;a href=&#34;https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/&#34; target=&#34;_blank&#34;&gt;Exploration with Autogen&lt;/a&gt; and following the example of &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat&#34; target=&#34;_blank&#34;&gt;Automated Multi Agent Chat&lt;/a&gt;, we&amp;rsquo;ll delve into the steps to create a dynamic debate environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;agent-setup&#34;&gt;Agent Setup&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll be setting up two agents: &lt;strong&gt;for_motion&lt;/strong&gt; and &lt;strong&gt;against_motion&lt;/strong&gt;. Each agent will engage in a debate on a given topic, providing examples and substantiating their points. A facilitator will oversee the debate rounds, ensuring that each response exceeds 300 words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring AutoGen with LM Studio and Local LLM</title>
      <link>https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/</link>
      <pubDate>Sat, 02 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/</guid>
      <description>&lt;p&gt;AutoGen, an innovative framework available on &lt;a href=&#34;https://github.com/microsoft/autogen&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;, empowers the development of LLM (Large Language Model) applications. These applications utilize multiple agents that engage in conversation to collaboratively solve tasks.&lt;/p&gt;&#xA;&lt;p&gt;In conjunction with AutoGen, &lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34;&gt;LM Studio&lt;/a&gt; provides a platform to discover, download, and run local LLMs. In this blog post, we&amp;rsquo;ll delve into the integration of AutoGen with LM Studio, showcasing a step-by-step guide on setting up a local LLM application served through LM Studio.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Boosting Inference Speed: SSD and GPU Acceleration</title>
      <link>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</link>
      <pubDate>Thu, 30 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</guid>
      <description>&lt;p&gt;In the relentless pursuit of optimal disk space and lightning-fast inference speeds, I embarked on an exciting upgrade journey by integrating the formidable &lt;a href=&#34;https://www.lexar.com/product/lexar-nm790-m-2-2280-pcie-gen-4x4-nvme-ssd/&#34; target=&#34;_blank&#34;&gt;Lexar NM790 M.2 2280 PCIe SSD&lt;/a&gt;. This blog post unfolds in two parts: the first chronicles the meticulous migration of my Windows 11 to this powerhouse SSD, while the second unveils the secrets behind the enhanced inferencing speed for the &lt;a href=&#34;https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;Langchain4j application&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;part-1-seamless-os-migration-with-clonezilla&#34;&gt;Part 1: Seamless OS Migration with Clonezilla&lt;/h2&gt;&#xA;&lt;p&gt;Amidst a sea of software promising seamless disk cloning, I found solace in the reliability of &lt;a href=&#34;https://clonezilla.org/&#34; target=&#34;_blank&#34;&gt;Clonezilla&lt;/a&gt;, a robust open-source tool for disk imaging and cloning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG over Java code with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</link>
      <pubDate>Sat, 11 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</guid>
      <description>&lt;p&gt;Expanding upon the concepts introduced in the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; and drawing inspiration from &lt;a href=&#34;https://python.langchain.com/docs/use_cases/question_answering/code_understanding&#34; target=&#34;_blank&#34;&gt;RAG over code&lt;/a&gt;, this article dives into the integration of a Retrieval-Augmented Generation (RAG) service. The goal is to empower users to query their Java codebase effectively.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To embark on this journey, I&amp;rsquo;ve opted for &lt;a href=&#34;https://javaparser.org/&#34; target=&#34;_blank&#34;&gt;Java Parser&lt;/a&gt; , a powerful tool for traversing Java source code. Let&amp;rsquo;s begin by incorporating the latest version of Java Parser into our build.gradle file:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building an AI Application with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</link>
      <pubDate>Tue, 07 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</guid>
      <description>&lt;p&gt;In this blog post, I&amp;rsquo;ll walk you through my journey of harnessing the capabilities of &lt;a href=&#34;https://github.com/langchain4j/langchain4j/&#34; target=&#34;_blank&#34;&gt;langchain4j&lt;/a&gt; to craft a powerful AI application using Java, specifically with a local language model. Unlike my previous exploration with Python, this post focuses on the Java implementation with Langchain4j.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To kick things off, I&amp;rsquo;ve chosen &lt;a href=&#34;https://spring.io/tools&#34; target=&#34;_blank&#34;&gt;STS4&lt;/a&gt; as my Integrated Development Environment (IDE) and opted for &lt;a href=&#34;https://adoptium.net/temurin/archive/?version=17&#34; target=&#34;_blank&#34;&gt;Java 17&lt;/a&gt; as my programming language. Leveraging &lt;a href=&#34;https://www.postman.com/&#34; target=&#34;_blank&#34;&gt;Postman&lt;/a&gt; as my API platform and &lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34;&gt;Spring Boot&lt;/a&gt; as the framework of choice, let&amp;rsquo;s delve into the process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unlocking the Power of Machine Learning with MLC LLM</title>
      <link>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</link>
      <pubDate>Sat, 02 Sep 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</guid>
      <description>&lt;p&gt;Machine Learning Compilation for LLM, or &lt;a href=&#34;https://mlc.ai/mlc-llm/docs/index.html&#34; target=&#34;_blank&#34;&gt;MLC LLM&lt;/a&gt;, is a cutting-edge universal deployment solution for large language models. In this blog post, we&amp;rsquo;ll guide you through the setup process and show you how to harness the immense potential of MLC LLM.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-your-environment&#34;&gt;Setting Up Your Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started with MLC LLM, you need to set up your environment properly. Follow these steps:&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-install-tvm&#34;&gt;1. Install TVM&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://tvm.apache.org/docs/install/index.html&#34; target=&#34;_blank&#34;&gt;TVM&lt;/a&gt;  is a critical component for MLC LLM. You can install it locally using pip:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://vllm.ai/&#34; target=&#34;_blank&#34;&gt;vLLM&lt;/a&gt; is an open-source library designed for rapid LLM (Large Language Model) inference and deployment. It leverages their novel algorithm called &lt;strong&gt;PagedAttention&lt;/strong&gt;, which optimizes the management of attention keys and values.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;In this blog post, I will share my experience of utilizing vLLM on a WSL (Windows Subsystem for Linux) instance running Ubuntu 22.04. Let&amp;rsquo;s start by setting up the environment:&lt;/p&gt;&#xA;&lt;h3 id=&#34;installing-wsl-and-configuring-ubuntu&#34;&gt;Installing WSL and Configuring Ubuntu&lt;/h3&gt;&#xA;&lt;p&gt;Begin by installing WSL and configuring it to use Ubuntu as the default distribution:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setting up K3s</title>
      <link>https://seehiong.github.io/archives/2023/setting-up-k3s/</link>
      <pubDate>Sun, 30 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/setting-up-k3s/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.k3s.io/&#34; target=&#34;_blank&#34;&gt;K3S&lt;/a&gt; is a lightweight and easy-to-install Kubernetes distribution, making it an ideal choice for running a Kubernetes cluster in your home lab. In this blog post, we will walk you through the step-by-step process of setting up K3s on an Ubuntu Server 22.04.2 LTS.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-setting-up-k3s&#34;&gt;1 Setting up K3S&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-installing-ubuntu-server-22042-lts&#34;&gt;1.1 Installing Ubuntu Server 22.04.2 LTS&lt;/h3&gt;&#xA;&lt;p&gt;To start, we&amp;rsquo;ll install &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34;&gt;Ubuntu server 22.04.2 LTS&lt;/a&gt; on our laptop. You can verify the Linux distribution using the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unleashing the Power of LLaMA Server in Docker Container</title>
      <link>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</link>
      <pubDate>Sat, 15 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</guid>
      <description>&lt;p&gt;Having recently completed the enlightening &lt;a href=&#34;https://www.coursera.org/learn/generative-ai-with-llms&#34; target=&#34;_blank&#34;&gt;Generative AI with Large Language Models&lt;/a&gt; course, where we gained invaluable knowledge and hands-on skills, we are now excited to share an exhilarating experience of running the LLaMA model in a Dockerized container.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, we&amp;rsquo;ll walk you through the setup and demonstrate how to unleash the full potential of running LLaMA Server within a Docker container.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;the-setup&#34;&gt;The Setup&lt;/h2&gt;&#xA;&lt;p&gt;Before we delve into the magic of LLaMA, let&amp;rsquo;s set up our application structure. To ensure smooth execution, we&amp;rsquo;ve structured our project as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (II)</title>
      <link>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</link>
      <pubDate>Fri, 16 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</guid>
      <description>&lt;p&gt;In continuation with the &lt;a href=&#34;https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/&#34; target=&#34;_blank&#34;&gt;previous&lt;/a&gt; post, we will explore the power of AI by leveraging the &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34; target=&#34;_blank&#34;&gt;whisper.cpp&lt;/a&gt; library to convert audio to text, extracting audio from YouTube videos using &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34; target=&#34;_blank&#34;&gt;yt-dlp&lt;/a&gt;, and demonstrating how to utilize AI models like GPT4All and OpenAI for summarization.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-the-environment&#34;&gt;Setting Up the Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started, we need to set up the necessary tools and libraries. Follow the steps below:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (I)</title>
      <link>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/</link>
      <pubDate>Sat, 10 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/</guid>
      <description>&lt;p&gt;Hey there, readers! Today, I&amp;rsquo;m thrilled to introduce you to an incredible tool that will completely transform the way you summarize YouTube videos. Get ready to dive into the captivating world of video content summarization using the powerful GPT4All. Trust me, this is an opportunity you don&amp;rsquo;t want to miss!&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-the-magic&#34;&gt;Setting up the Magic&lt;/h2&gt;&#xA;&lt;p&gt;Before we embark on this exciting journey, let&amp;rsquo;s ensure we have everything we need to get started. Install the necessary dependencies by running the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Receipt OCR with LangChain, OpenAI and PyTesseract</title>
      <link>https://seehiong.github.io/archives/2023/receipt-ocr-with-langchain-and-openai/</link>
      <pubDate>Tue, 06 Jun 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/receipt-ocr-with-langchain-and-openai/</guid>
      <description>&lt;p&gt;Recently, I embarked on an exhilarating journey into the realm of receipt OCR using LangChain and OpenAI, inspired by the captivating course on &lt;a href=&#34;https://learn.deeplearning.ai/langchain/lesson/1/introduction&#34; target=&#34;_blank&#34;&gt;LangChain for LLM Application Development&lt;/a&gt;. This exploration allowed me to unlock the full potential of PyTesseract, an extraordinary Python tool that serves as my guiding light for optical character recognition (OCR). By harnessing the power of OpenCV and seamlessly integrating OpenAI into the workflow, I aimed to compile the most optimal OCR results and validate them using LangChain&amp;rsquo;s impressive llm-math tool. Join me on this exciting adventure as we unravel the intricacies of receipt OCR and discover the true potential of LangChain, OpenAI, and PyTesseract.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Autofill PDF with LangChain and LangFlow</title>
      <link>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</link>
      <pubDate>Fri, 26 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</guid>
      <description>&lt;p&gt;In this blog post, we will explore the usage of &lt;a href=&#34;https://pypi.org/project/langflow/&#34; target=&#34;_blank&#34;&gt;LangFlow&lt;/a&gt;, a Python library available on PyPI, to streamline the process of capturing ideas and conducting proof-of-concepts for our intended use case. Considering the current &amp;ldquo;trend&amp;rdquo; of tech layoffs, there might be a time (touch wood) where there is a need to go for interviews and fill-up various interview forms that require filling out personal information. Building upon the previous blog post on running GPT4All for PostgreSQL with LangChain (referenced &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), we will now leverage LangFlow and OpenAI to automate the population of a sample employment form with our personal data stored in PostgreSQL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running GPT4All for your PostgreSQL with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/</link>
      <pubDate>Sun, 21 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/</guid>
      <description>&lt;p&gt;In this post, I will walk you through the process of setting up &lt;a href=&#34;https://github.com/nomic-ai/gpt4all/blob/main/gpt4all-bindings/python/README.md&#34; target=&#34;_blank&#34;&gt;Python GPT4All&lt;/a&gt; on my Windows PC. Additionally, I will demonstrate how to utilize the power of GPT4All along with &lt;a href=&#34;https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html&#34; target=&#34;_blank&#34;&gt;SQL Chain&lt;/a&gt; for querying a postgreSQL database.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;&#xA;&lt;p&gt;Before we proceed with the installation process, it is important to have the necessary prerequisites in place.&lt;/p&gt;&#xA;&lt;p&gt;To follow along with this guide, make sure you have the following:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA server in local machine</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</link>
      <pubDate>Sat, 13 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</guid>
      <description>&lt;p&gt;Referencing the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, we will run a web server which aims to act as a drop-in replacement for the OpenAI API, which can in turn be used by &lt;a href=&#34;https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/&#34; target=&#34;_blank&#34;&gt;byogpt&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(3 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://packaging.python.org/en/latest/key_projects/#pipenv&#34; target=&#34;_blank&#34;&gt;Pipenv&lt;/a&gt; aims to help users manage environments, dependencies and imported packages and I will be using it in this guide.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install pipenv uvicorn fastapi sse_starlette&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipenv shell&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Building ChatBot for your PDF files with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</link>
      <pubDate>Sun, 07 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</guid>
      <description>&lt;p&gt;Extending the use case on the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I will demostrate how you could ingest your own PDF file to your own &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;LLaMa model in local machine&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start off by installing &lt;a href=&#34;https://github.com/chroma-core/chroma&#34; target=&#34;_blank&#34;&gt;Chroma&lt;/a&gt;, the open-source embedding database:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install chromadb pypdf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pdf/langchain-chromadb-install.png&#34; alt=&#34;langchain-chromadb-install&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;ingesting-your-pdf&#34;&gt;Ingesting your PDF&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 mins)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a basic Chain with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</link>
      <pubDate>Mon, 01 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://js.langchain.com/docs/&#34; target=&#34;_blank&#34;&gt;LangChain&lt;/a&gt; is a framework for developing applications powered by language models. With the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; setup, I will follow closely to using &lt;a href=&#34;https://python.langchain.com/en/latest/ecosystem/llamacpp.html&#34; target=&#34;_blank&#34;&gt;Llama.cpp within LangChain&lt;/a&gt; for building the simplest form of chain with LangChain.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, installs the required python packages:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo pip install llama-cpp-python langchain &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/langchain/langchain-llama-dependencies.png&#34; alt=&#34;langchain-llama-dependencies&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA model locally</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-model-locally/</link>
      <pubDate>Sun, 30 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-model-locally/</guid>
      <description>&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(30 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34; target=&#34;_blank&#34;&gt;LLaMA&lt;/a&gt; is a collection of foundation language models ranging from 7B to 65B parameters.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will be using and following &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34; target=&#34;_blank&#34;&gt;Georgi Gergano&amp;rsquo;s llama.cpp&lt;/a&gt;, a inference of LLaMA model in pure C/C++.&lt;/p&gt;&#xA;&lt;p&gt;I will be setting this up in a Ubuntu machine with 32Gb.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/llama/hp-system-info.png&#34; alt=&#34;hp-system-info&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;To prepare for the build system, I installed these:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Developing BYO-GPT with Flutter</title>
      <link>https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/</link>
      <pubDate>Sat, 29 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/byogpt/byo-gpt-feature.png&#34; alt=&#34;byo-gpt-feature&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Developing a user-friendly interface to converse with ChatGPT via OpenAI&amp;rsquo;s API with your own openAI API token.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;developing-byo-gpt-with-flutter&#34;&gt;Developing BYO-GPT with Flutter&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 10 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this post, I will develop a &amp;ldquo;Bring Your Own - Generative Pre-Trained Transformer&amp;rdquo;, a user-friendly interface to converse with ChatGPT via &lt;a href=&#34;https://platform.openai.com/&#34; target=&#34;_blank&#34;&gt;OpenAI&amp;rsquo;s API&lt;/a&gt; with &lt;a href=&#34;https://flutter.dev/&#34; target=&#34;_blank&#34;&gt;Flutter&lt;/a&gt;. You may download &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.seehiong.byogpt&#34; target=&#34;_blank&#34;&gt;BYO-GPT&lt;/a&gt; and check it out!&lt;/p&gt;</description>
    </item>
    <item>
      <title>CI/CD Pipeline on Pi Cluster (IV)</title>
      <link>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-4/</link>
      <pubDate>Sun, 18 Jul 2021 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-4/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pipeline4/raspberry-pi-enclosed-in-lego-structure.jpeg&#34; alt=&#34;raspberry-pi-enclosed-in-lego-structure&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Building a CI/CD pipeline on a Raspberry PI Cluster (Part 4), with SonarQube integrating into Jenkins and Gitlab&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;building-a-cicd-pipeline-on-a-raspberry-pi-cluster-part-iv&#34;&gt;Building a CI/CD pipeline on a Raspberry PI Cluster (Part IV)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 15 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In continuation from &lt;a href=&#34;https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-3/&#34; target=&#34;_blank&#34;&gt;part 3&lt;/a&gt; of this guide, I will add &lt;a href=&#34;https://www.sonarqube.org/&#34; target=&#34;_blank&#34;&gt;SonarQube&lt;/a&gt; into my CI/CD pipeline. This will enahnce our workflow with continuous code quality and code security.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CI/CD Pipeline on Pi Cluster (III)</title>
      <link>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-3/</link>
      <pubDate>Sun, 04 Jul 2021 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-3/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pipeline3/raspberry-pi-enclosed-in-lego-structure.jpeg&#34; alt=&#34;raspberry-pi-enclosed-in-lego-structure&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Building a CI/CD pipeline on a Raspberry PI Cluster (Part 3), with JFrog Container Registry supporting our Docker containers and Helm Chart repositories&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;building-a-cicd-pipeline-on-a-raspberry-pi-cluster-part-iii&#34;&gt;Building a CI/CD pipeline on a Raspberry PI Cluster (Part III)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 15 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Continue from &lt;a href=&#34;https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-2/&#34; target=&#34;_blank&#34;&gt;part 2&lt;/a&gt; of this guide, I will add &lt;a href=&#34;https://jfrog.com/container-registry/&#34; target=&#34;_blank&#34;&gt;JFrog Container Registry&lt;/a&gt; to my CI/CD pipeline.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installing-container-registry&#34;&gt;Installing Container Registry&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 mins)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>CI/CD Pipeline on Pi Cluster (II)</title>
      <link>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-2/</link>
      <pubDate>Mon, 21 Jun 2021 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-2/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pipeline2/raspberry-pi-enclosed-in-lego-structure.jpeg&#34; alt=&#34;raspberry-pi-enclosed-in-lego-structure&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Building a CI/CD pipeline part 2 on a Raspberry PI Cluster, with JFrog Artifactory as the repository manager&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;building-a-cicd-pipeline-on-a-raspberry-pi-cluster-part-ii&#34;&gt;Building a CI/CD pipeline on a Raspberry PI Cluster (Part II)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 40 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Continuing from &lt;a href=&#34;https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-1/&#34; target=&#34;_blank&#34;&gt;part I&lt;/a&gt; of this guide, I will add &lt;a href=&#34;https://jfrog.com/artifactory/&#34; target=&#34;_blank&#34;&gt;JFrog Artifactory&lt;/a&gt; to my CI/CD pipeline.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(1 min)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Referencing from my previous post on &lt;a href=&#34;https://seehiong.github.io/archives/2020/jenkins-maven-agent-for-kubernetes/&#34; target=&#34;_blank&#34;&gt;maven agent&lt;/a&gt;, let&amp;rsquo;s configure maven-agent to mount Longhorn volume. Navigate to &lt;em&gt;Manage Jenkins &amp;gt; Manage Nodes and Clouds &amp;gt; Configure Clouds&lt;/em&gt;. Expand on Pod Template details and add a volume:&lt;/p&gt;</description>
    </item>
    <item>
      <title>CI/CD Pipeline on Pi Cluster (I)</title>
      <link>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-1/</link>
      <pubDate>Sun, 13 Jun 2021 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2021/building-a-ci-cd-pipeline-on-a-raspberry-pi-cluster-part-1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pipeline1/raspberry-pi-enclosed-in-lego-structure.jpeg&#34; alt=&#34;raspberry-pi-enclosed-in-lego-structure&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Building a CI/CD pipeline on a Raspberry PI Cluster, with 3 master and 1 worker nodes, enclosed in a custom-made LEGO structure&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;building-a-cicd-pipeline-on-a-raspberry-pi-cluster-part-i&#34;&gt;Building a CI/CD pipeline on a Raspberry PI Cluster (Part I)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 40 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this series, I will build my own CI/CD pipeline, with tools that are configured to run on &lt;a href=&#34;https://seehiong.github.io/archives/2021/raspberry-pi-cluster-with-longhorn/&#34; target=&#34;_blank&#34;&gt;Raspberry PI Cluster and Longhorn&lt;/a&gt;, a HA Raspberry PI Cluster. By end of this guide, you will have a &lt;a href=&#34;https://gitea.io/en-us/&#34; target=&#34;_blank&#34;&gt;self-hosted Git service&lt;/a&gt;, working hand-in-hand with &lt;a href=&#34;https://www.jenkins.io/&#34; target=&#34;_blank&#34;&gt;Jenkins&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pi Cluster with Longhorn</title>
      <link>https://seehiong.github.io/archives/2021/raspberry-pi-cluster-with-longhorn/</link>
      <pubDate>Sun, 06 Jun 2021 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2021/raspberry-pi-cluster-with-longhorn/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/longhorn/highly-available-kubernetes-pi-cluster.png&#34; alt=&#34;highly-available-kubernetes-pi-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;By running Raspberry PI Cluster with Longhorn, you will have a simplified, easy to deploy cloud-native persistent block storage&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;raspberry-pi-cluster-and-longhorn&#34;&gt;Raspberry Pi Cluster and Longhorn&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 30 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this section, I will install &lt;a href=&#34;https://longhorn.io/&#34; target=&#34;_blank&#34;&gt;Longhorn&lt;/a&gt;, a highly available persistance storage for Kubernetes. This guide assumes that you have a working Raspberry PI Cluster. If you do not have, please  follow &lt;a href=&#34;https://seehiong.github.io/archives/2021/kubernetes-pi-cluster-with-ansible/&#34; target=&#34;_blank&#34;&gt;Kubernetes Cluster with Ansible&lt;/a&gt; or &lt;a href=&#34;https://seehiong.github.io/archives/2020/highly-available-kubernetes-pi-cluster-part-1/&#34; target=&#34;_blank&#34;&gt;HA Kubernetes Pi Cluster (Part I)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K8s Pi Cluster with Ansible</title>
      <link>https://seehiong.github.io/archives/2021/kubernetes-pi-cluster-with-ansible/</link>
      <pubDate>Sat, 29 May 2021 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2021/kubernetes-pi-cluster-with-ansible/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/ansible/highly-available-kubernetes-pi-cluster.png&#34; alt=&#34;highly-available-kubernetes-pi-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;By provisioning a Kubernetes PI Cluster with Ansible, you can easily spin off a Raspberry PI cluster&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;kubernetes-cluster-with-ansible&#34;&gt;Kubernetes Cluster with Ansible&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 50 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will configure a Kubernetes Cluster using &lt;a href=&#34;https://www.ansible.com/&#34; target=&#34;_blank&#34;&gt;Ansible&lt;/a&gt;. This guide follows closely to the &lt;a href=&#34;https://github.com/raspbernetes/k8s-cluster-installation&#34; target=&#34;_blank&#34;&gt;Raspbernetes Cluster Installation&lt;/a&gt;. I will be using 3x Raspberry Pi 4 Model B 8GB as the master nodes and 1x Raspberry Pi 3 Model B as the only worker node.&lt;/p&gt;</description>
    </item>
    <item>
      <title>HA K8s Pi Cluster (II)</title>
      <link>https://seehiong.github.io/archives/2020/highly-available-kubernetes-pi-cluster-part-2/</link>
      <pubDate>Mon, 17 Aug 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/highly-available-kubernetes-pi-cluster-part-2/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/ha-k8s2/highly-available-kubernetes-pi-cluster.png&#34; alt=&#34;highly-available-kubernetes-pi-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;By having a Highly Available Kubernetes Pi Cluster, you will have full control over your production grade environment on-premise&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;ha-kubernetes-pi-cluster-part-ii&#34;&gt;HA Kubernetes Pi Cluster (Part II)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 20 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this follow up guide, I will configure the HA Kubernetes Cluster onto the previous &lt;a href=&#34;https://seehiong.github.io/archives/2020/highly-available-kubernetes-pi-cluster-part-1/&#34; target=&#34;_blank&#34;&gt;external etcd setup&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installing-docker&#34;&gt;Installing Docker&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Firstly, installs docker &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/container-runtimes/&#34; target=&#34;_blank&#34;&gt;container runtimes&lt;/a&gt; to all master nodes:&lt;/p&gt;</description>
    </item>
    <item>
      <title>HA K8s Pi Cluster (I)</title>
      <link>https://seehiong.github.io/archives/2020/highly-available-kubernetes-pi-cluster-part-1/</link>
      <pubDate>Sun, 09 Aug 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/highly-available-kubernetes-pi-cluster-part-1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/ha-k8s1/highly-available-kubernetes-pi-cluster.png&#34; alt=&#34;highly-available-kubernetes-pi-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;By having a Highly Available Kubernetes Pi Cluster, you will have full control over your production grade environment on-premise&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;ha-kubernetes-pi-cluster-part-i&#34;&gt;HA Kubernetes Pi Cluster (Part I)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 25 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;On this special day, I will like to wish all Singaporeans and Singapore a Happy 55th &lt;a href=&#34;https://www.ndp.gov.sg/&#34; target=&#34;_blank&#34;&gt;National Day&lt;/a&gt;!&lt;/p&gt;&#xA;&lt;p&gt;With the newly purchase 2x Raspberry Pi Model B 8GB and 64GB SD card to my collection, I will setup a Highly Available Kubernetes Pi Cluster. In this guide, I will setup an &lt;a href=&#34;https://etcd.io/&#34; target=&#34;_blank&#34;&gt;external etcd&lt;/a&gt; key-value store. In the &lt;a href=&#34;https://seehiong.github.io/archives/2020/highly-available-kubernetes-pi-cluster-part-2/&#34; target=&#34;_blank&#34;&gt;next article&lt;/a&gt;, I will continue with the HA configuration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Private Registry for K8s Cluster</title>
      <link>https://seehiong.github.io/archives/2020/private-registry-for-kubernetes-cluster/</link>
      <pubDate>Fri, 07 Aug 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/private-registry-for-kubernetes-cluster/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/registry/raspberrypi4_modelb.png&#34; alt=&#34;raspberrypi4_modelb&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With your own Private Registry for Kubernetes Cluster, you can have full control over the docker registry and improve overall performance&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;private-registry-on-kubernetes-cluster&#34;&gt;Private Registry on Kubernetes Cluster&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 10 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/_/registry/&#34; target=&#34;_blank&#34;&gt;Docker Registry&lt;/a&gt; is the official implementation for storing and distributing Docker images.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparing-private-registry&#34;&gt;Preparing Private Registry&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, create the &lt;a href=&#34;https://docs.docker.com/registry/insecure/&#34; target=&#34;_blank&#34;&gt;self-signed certificate&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p certs&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openssl req &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -x509 -days &lt;span style=&#34;color:#ae81ff&#34;&gt;365&lt;/span&gt; -out certs/domain.crt&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>OpenWrt Router on Pi 3</title>
      <link>https://seehiong.github.io/archives/2020/openwrt-as-router-on-raspberry-pi-3/</link>
      <pubDate>Sun, 02 Aug 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/openwrt-as-router-on-raspberry-pi-3/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/router/openwrt-on-raspberry-pi-3.png&#34; alt=&#34;openwrt-on-raspberry-pi-3&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;By running OpenWrt Router on Raspberry Pi 3, you will be able to customize the device to suit your application usage&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;openwrt-router-for-raspberry&#34;&gt;OpenWrt Router for Raspberry&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 20 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will config &lt;a href=&#34;https://openwrt.org/&#34; target=&#34;_blank&#34;&gt;OpenWrt&lt;/a&gt; as router on Raspberry Pi 3. The Wireless LAN (wlan0) is connected to home network while the LAN interface (eth0) is connected to the Kubernetes Cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jenkins Maven Agent</title>
      <link>https://seehiong.github.io/archives/2020/jenkins-maven-agent-for-kubernetes/</link>
      <pubDate>Fri, 31 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/jenkins-maven-agent-for-kubernetes/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/jenkins-agent/jenkins-on-kubernetes.png&#34; alt=&#34;jenkins-maven-agent-on-kubernetes&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;By creating Jenkins Maven Agent for Kubernetes Cluster, you can improve build time of your maven builds&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;jenkins-maven-agent-on-kubernetes&#34;&gt;Jenkins Maven Agent on Kubernetes&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 15 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Following up on the &lt;a href=&#34;https://seehiong.github.io/archives/2020/jenkins-pipeline-for-kubernetes-cluster/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I will create a Jenkins Maven Agent for Kubernetes. By configuring a local maven m2 repository, you can save previous time on your builds.&lt;/p&gt;&#xA;&lt;h2 id=&#34;configuring-jenkins&#34;&gt;Configuring Jenkins&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(1 min)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jenkins Pipeline for K8s Cluster</title>
      <link>https://seehiong.github.io/archives/2020/jenkins-pipeline-for-kubernetes-cluster/</link>
      <pubDate>Fri, 31 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/jenkins-pipeline-for-kubernetes-cluster/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/jenkins-pipeline/jenkins-on-kubernetes.png&#34; alt=&#34;jenkins-pipeline-for-kubernetes-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With Jenkins Pipeline for Kubernetes Cluster, you can create a continuous integration environment for your project&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;jenkins-pipeline-on-kubernetes-cluster&#34;&gt;Jenkins Pipeline on Kubernetes Cluster&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 10 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will create my own &lt;a href=&#34;https://www.jenkins.io/doc/book/pipeline/jenkinsfile/&#34; target=&#34;_blank&#34;&gt;declarative Jenkins pipeline&lt;/a&gt;. With this, I can build, package and run my &lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34;&gt;Spring Boot&lt;/a&gt; Hello-World application.&lt;/p&gt;&#xA;&lt;h2 id=&#34;configuring-jenkins-and-gitea&#34;&gt;Configuring Jenkins and Gitea&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(1 min)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integrating Jenkins and Gitea</title>
      <link>https://seehiong.github.io/archives/2020/integrating-jenkins-and-gitea/</link>
      <pubDate>Sun, 26 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/integrating-jenkins-and-gitea/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/jenkins-gitea/integrating-jenkins-and-gitea.png&#34; alt=&#34;integrating-jenkins-and-gitea&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;By integrating Jenkins and Gitea with webhook, you have full control over your own self-hosted continuous integration (CI) environment&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;integration-between-jenkins-and-gitea&#34;&gt;Integration between Jenkins and Gitea&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 8 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Following up with my previous posts, I will integrate &lt;a href=&#34;https://seehiong.github.io/archives/2020/jenkins-for-kubernetes-cluster-part-2/&#34; target=&#34;_blank&#34;&gt;Jenkins&lt;/a&gt; and &lt;a href=&#34;https://seehiong.github.io/archives/2020/gitea-on-kubernetes-pi-cluster/&#34; target=&#34;_blank&#34;&gt;Gitea&lt;/a&gt; in this guide.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installing-gitea-plugin-on-jenkins&#34;&gt;Installing Gitea Plugin on Jenkins&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Navigate to Manage Jenkins &amp;gt; Manage Plugins, click on the Available tab. Search for Gitea, check on it and start installation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Helm for K8s Cluster</title>
      <link>https://seehiong.github.io/archives/2020/helm-for-kubernetes-cluster/</link>
      <pubDate>Fri, 24 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/helm-for-kubernetes-cluster/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/helm/helm-on-kubernetes-cluster.png&#34; alt=&#34;helm-for-kubernetes-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With Helm as the package manager for Kubernetes Cluster on Raspberry, you can find, share and and use software built for Kubernetes&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;helm-on-kubernetes-cluster&#34;&gt;Helm on Kubernetes Cluster&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 10 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt; is the package manager for Kubernetes. In this guide, I will install helm and setup ingress nginx controller with metallb as the layer 2 load balancer.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installing-helm&#34;&gt;Installing Helm&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(3 mins)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jenkins for K8s Cluster (II)</title>
      <link>https://seehiong.github.io/archives/2020/jenkins-for-kubernetes-cluster-part-2/</link>
      <pubDate>Sun, 19 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/jenkins-for-kubernetes-cluster-part-2/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/jenkins-k8s2/jenkins-on-kubernetes.png&#34; alt=&#34;jenkins-on-kubernetes&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With Jenkins Agent for Kubernetes Cluster, you can orchestrate your build, test and scale deployment pipelines automatically&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;jenkins-on-kubernetes-cluster-part-ii&#34;&gt;Jenkins on Kubernetes Cluster (Part II)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 25 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will setup Jenkins agents for &lt;a href=&#34;https://seehiong.github.io/archives/2020/jenkins-for-kubernetes-cluster-part-1/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;. With these agents, you can expand your Kubernetes cluster capabilities to handle additional loads.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(15 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, you need to create the jenkins agent docker image for raspberry Pi cluster. You may start by creating a new &lt;em&gt;Dockerfile&lt;/em&gt; and insert the following:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jenkins for K8s Cluster (I)</title>
      <link>https://seehiong.github.io/archives/2020/jenkins-for-kubernetes-cluster-part-1/</link>
      <pubDate>Sun, 12 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/jenkins-for-kubernetes-cluster-part-1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/jenkins-k8s1/jenkins-on-kubernetes.png&#34; alt=&#34;jenkins-on-kubernetes&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With Jenkins for Kubernetes Cluster, you can orchestrate your build, test and deployment pipelines&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;jenkins-on-kubernetes-cluster-part-i&#34;&gt;Jenkins on Kubernetes Cluster (Part I)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 40 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.jenkins.io/&#34; target=&#34;_blank&#34;&gt;Jenkins&lt;/a&gt; is the leading open source automation server. It provides hundreds of plugins for supporting the building, deploying and automating of any project.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(25 min)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I am going to build the docker image for Jenkins on &lt;a href=&#34;https://seehiong.github.io/archives/2020/kubernetes-cluster-on-raspberry-pi/&#34; target=&#34;_blank&#34;&gt;Kubernetes Cluster on Pi&lt;/a&gt;. You may use the &lt;a href=&#34;https://hub.docker.com/r/balenalib/raspberrypi4-64-debian&#34; target=&#34;_blank&#34;&gt;base image&lt;/a&gt; from &lt;a href=&#34;https://www.balena.io/&#34; target=&#34;_blank&#34;&gt;Balena&lt;/a&gt; in any Docker environment. In addition, you can find more details about the Balena base images &lt;a href=&#34;https://www.balena.io/docs/reference/base-images/base-images&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gitea for K8s Cluster</title>
      <link>https://seehiong.github.io/archives/2020/gitea-on-kubernetes-pi-cluster/</link>
      <pubDate>Fri, 10 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/gitea-on-kubernetes-pi-cluster/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/gitea-k8s/gitea-on-raspberry-pi-cluster.png&#34; alt=&#34;gitea-on-kubernetes-pi-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Having Gitea on Kubernetes Pi cluster, you will have full control over your personal Git repositories&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;gitea-for-kubernetes-cluster-on-pi&#34;&gt;Gitea for Kubernetes Cluster on Pi&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 40 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Similar to the previous post on &lt;a href=&#34;https://seehiong.github.io/archives/2020/gitea-for-raspberry-pi-cluster/&#34; target=&#34;_blank&#34;&gt;Gitea for MicroK8s Cluster&lt;/a&gt;, I will be setting up Git in the newly created &lt;a href=&#34;https://seehiong.github.io/archives/2020/kubernetes-cluster-on-raspberry-pi/&#34; target=&#34;_blank&#34;&gt;Kubernetes Cluster&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;setup-mysql&#34;&gt;Setup MySQL&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(15 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, I download the &lt;a href=&#34;https://dev.mysql.com/doc/mysql-installation-excerpt/8.0/en/docker-mysql-getting-started.html&#34; target=&#34;_blank&#34;&gt;mysql-server docker&lt;/a&gt; image:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes Cluster on Pi</title>
      <link>https://seehiong.github.io/archives/2020/kubernetes-cluster-on-raspberry-pi/</link>
      <pubDate>Sat, 04 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/kubernetes-cluster-on-raspberry-pi/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/kubernetes/kubernetes-cluster-on-raspberry-pi.png&#34; alt=&#34;kubernetes-cluster-on-raspberry-pi&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With Kubernetes Cluster on Raspberry Pi, you may orchestrate and manage your Docker containers with full control&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;kubernetes-cluster&#34;&gt;Kubernetes Cluster&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 70 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Similar to the previous &lt;a href=&#34;https://seehiong.github.io/archives/2020/microk8s-cluster-on-raspberry-pi-8gb-part-1/&#34; target=&#34;_blank&#34;&gt;MicroK8s cluster&lt;/a&gt; setup, I am using &lt;a href=&#34;https://ubuntu.com/download/raspberry-pi&#34; target=&#34;_blank&#34;&gt;Ubuntu Server 20.04 LTS (64-bit)&lt;/a&gt; as my OS. Having a Kubernetes Cluster on Raspberry Pi, you will have more control over how the cluster configured.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gitea for MicroK8s Cluster</title>
      <link>https://seehiong.github.io/archives/2020/gitea-for-raspberry-pi-cluster/</link>
      <pubDate>Mon, 29 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/gitea-for-raspberry-pi-cluster/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/gitea/gitea-on-raspberry-pi-cluster.png&#34; alt=&#34;gitea-for-raspberry-pi-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With Gitea for Raspberry Pi cluster, you can have your own self-hosted Git Service&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;gitea-for-raspberry-pi&#34;&gt;Gitea for Raspberry Pi&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 45 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://gitea.io/en-us/&#34; target=&#34;_blank&#34;&gt;Gitea&lt;/a&gt; is a painless self-hosted Git service. By hosting Gitea locally, our team is able to save cost and you also have more control over your server.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 min)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;If you are following my &lt;a href=&#34;https://seehiong.github.io/archives/2020/microk8s-cluster-on-raspberry-pi-8gb-part-1/&#34; target=&#34;_blank&#34;&gt;MicroK8s cluster&lt;/a&gt; setup, for each kubectl command, you need to append with microk8s. With this section, you can simply use &lt;em&gt;kubectl&lt;/em&gt;. First, install kubectl:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Docker for MicroK8s Cluster</title>
      <link>https://seehiong.github.io/archives/2020/docker-on-raspberry-pi-cluster/</link>
      <pubDate>Sun, 21 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/docker-on-raspberry-pi-cluster/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/docker/docker-on-pi-cluster.png&#34; alt=&#34;docker-for-raspberry-pi&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;With Docker on Raspberry Pi cluster, you can run any containerized applications on your Pi Cluster&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;docker-on-raspberry&#34;&gt;Docker on Raspberry&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 15 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;To install &lt;a href=&#34;https://microk8s.io/docs/registry-images&#34; target=&#34;_blank&#34;&gt;Docker&lt;/a&gt; for Raspberry Pi Cluster, add &lt;strong&gt;ubuntu&lt;/strong&gt; user to the docker group:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker ubuntu&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;su - ubuntu &lt;span style=&#34;color:#75715e&#34;&gt;# open a new shell with updated membership for the user&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>GNU Health</title>
      <link>https://seehiong.github.io/archives/2020/gnu-health-embedded-on-raspberry-pi-3/</link>
      <pubDate>Sun, 21 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/gnu-health-embedded-on-raspberry-pi-3/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/health/gnu-health-on-raspberry-pi-3.png&#34; alt=&#34;gnu-health-on-raspberry-pi-3&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;GNU Health Embedded on Raspberry Pi 3, your own personal one-node EMR (Electronics Medical Record)&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;gnu-health-on-pi-3&#34;&gt;GNU Health on Pi 3&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 45 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;As Singapore moves into its phase 2 of reopening from 19 Jun, personal hygiene and a healthy lifestyle is still of paramount importance in fighting the &lt;a href=&#34;https://www.worldometers.info/coronavirus/&#34; target=&#34;_blank&#34;&gt;COVID-19 coronavirus pandemic&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;GNU Health embedded is a full server, runs on &lt;a href=&#34;https://www.opensuse.org/&#34; target=&#34;_blank&#34;&gt;openSUSE&lt;/a&gt; and has its own database. It allows storing of information locally and has a demo DB to play with locally. Today, I would like to test drive the GNU Health Embedded on Raspberry Pi 3.&lt;/p&gt;</description>
    </item>
    <item>
      <title>External Storage</title>
      <link>https://seehiong.github.io/archives/2020/external-storage-for-raspberry-pi-cluster/</link>
      <pubDate>Fri, 19 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/external-storage-for-raspberry-pi-cluster/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/storage/external-storage-for-pi.png&#34; alt=&#34;external-storage-for-pi&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Expand storage capacity by using external storage for your Raspberry Pi Cluster&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;external-storage-for-pi-cluster&#34;&gt;External Storage for Pi Cluster&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 35 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;With the &lt;a href=&#34;https://seehiong.github.io/archives/2020/microk8s-cluster-on-raspberry-pi-8gb-part-1/&#34; target=&#34;_blank&#34;&gt;MicroK8s cluster&lt;/a&gt; in place, I decided to expand the External Storage for Raspberry Pi cluster. For this to work, I will use my external 640GB USB hard disk and configure MicroK8s default storage.&lt;/p&gt;&#xA;&lt;h2 id=&#34;mounting-external-storage&#34;&gt;Mounting External Storage&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 mins)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Plasma Bigscreen</title>
      <link>https://seehiong.github.io/archives/2020/plasma-bigscreen-on-pi-8gb/</link>
      <pubDate>Sun, 14 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/plasma-bigscreen-on-pi-8gb/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/plasma/plasma_bigscreen_raspberrypi.png&#34; alt=&#34;plasma-bigscreen-setup&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Plasma Bigscreen is a completely open UI stack for your personal TV box&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;plasma-bigscreen-on-raspberry-pi-4-model-b-8gb&#34;&gt;Plasma Bigscreen on Raspberry Pi 4 Model B 8GB&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 30 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://plasma-bigscreen.org/&#34; target=&#34;_blank&#34;&gt;Plasma Bigscreen&lt;/a&gt; will transform your regular TV into a smart one. With built-in Mycroft AI voice assistant, you can use voice command to control. So, even though it is still in beta stage, with the arrival of the microHDMI cable, I am eager to test drive Plasma Bigscreen on Raspberry Pi 4 Model B 8GB!&lt;/p&gt;</description>
    </item>
    <item>
      <title>MicroK8s on Pi 4 8GB (II)</title>
      <link>https://seehiong.github.io/archives/2020/microk8s-cluster-on-raspberry-pi-8gb-part-2/</link>
      <pubDate>Tue, 09 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/microk8s-cluster-on-raspberry-pi-8gb-part-2/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/microk8s2/leaf_nodes.png&#34; alt=&#34;raspberry-pi-leaf-nodes-for-microk8s-cluster&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Adding few low-cost Raspberry Pi nodes to improve your MicroK8s performance&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;microk8s-cluster-on-raspberry-pi-4-model-b-8gb-part-ii&#34;&gt;MicroK8s Cluster on Raspberry Pi 4 Model B 8GB (Part II)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 45 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Following up from the &lt;a href=&#34;https://seehiong.github.io/archives/2020/jenkins-for-kubernetes-cluster-part-1/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, for this second part, I will be adding new Raspberry Pi nodes to the MicroK8s Cluster. However, other than Raspberry Pi, you can also re-purpose some of your older unused laptop or PC and add them to your MicroK8s cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MicroK8s on Pi 4 8GB (I)</title>
      <link>https://seehiong.github.io/archives/2020/microk8s-cluster-on-raspberry-pi-8gb-part-1/</link>
      <pubDate>Sat, 06 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/microk8s-cluster-on-raspberry-pi-8gb-part-1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/microk8s1/raspberrypi4_modelb.png&#34; alt=&#34;raspberry-pi-4-model-b&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Making your Raspberry Pi works for your MicroK8s cluster&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;microk8s-cluster-on-raspberry-pi-4-model-b-8gb-part-i&#34;&gt;MicroK8s Cluster on Raspberry Pi 4 Model B 8GB (Part I)&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 40 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;I am very excited to receive my Raspberry Pi 4 today. Since I do not have the microHDMI cable, I decided to go for a headless install. With this new Pi 8GB, I plan to check out on the &lt;a href=&#34;https://microk8s.io/&#34; target=&#34;_blank&#34;&gt;MicroK8s&lt;/a&gt;, a lightweight upstream K8s. This tutorial shows my steps for setting up the MicroK8s Cluster on Raspberry Pi 4 Model B 8GB.&lt;/p&gt;</description>
    </item>
    <item>
      <title>NAS Server</title>
      <link>https://seehiong.github.io/archives/2020/openmediavault-on-raspberry-pi/</link>
      <pubDate>Fri, 05 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2020/openmediavault-on-raspberry-pi/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/omv/raspberrypi3_setup.png&#34; alt=&#34;raspberry-pi-setup&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Your own personal NAS server using OpenMediaVault on Raspberry Pi&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;openmediavault-on-pi&#34;&gt;OpenMediaVault on Pi&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 90 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;With my old Raspberry Pi 3 Mobel B V1.2 laying around, I plan to re-purpose it into a NAS server. OpenMediaVault (&lt;a href=&#34;https://www.openmediavault.org/&#34; target=&#34;_blank&#34;&gt;OMV&lt;/a&gt;) is a network attached storage (NAS) solution based on Debian Linux. So, let&amp;rsquo;s start trying openmediavault on Raspberry Pi out.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://seehiong.github.io/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://seehiong.github.io/about/about/</guid>
      <description>&lt;h2 id=&#34;about-me&#34;&gt;About Me&lt;/h2&gt;&#xA;&lt;p&gt;I’m an infrastructure and AI enthusiast with a focus on cloud-native technologies, AI integration, and automation. My blog documents my journey through Kubernetes orchestration, scalable AI/ML solutions, and modern CI/CD pipelines.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kubernetes Ecosystem&lt;/strong&gt;: From managing K3s clusters to deploying distributed applications with MicroK8s, I streamline cloud infrastructure for reliability and scale.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;AI/ML Integrations&lt;/strong&gt;: I specialize in deploying models like LLaMA, GPT4All, and Langchain4j, optimizing MLOps with Kubeflow, and integrating real-time AI workflows into Kubernetes.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
