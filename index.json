[{"content":"Following up on my previous post, Building a LangGraph Multi-Agent System\r, this article extends that foundation into a fully autonomous, DeepAgents-powered multi-agent pipeline for HDB insights.\nThis iteration brings together:\nGeospatial reasoning across MRT exits, HDB building polygons, and a foundation that can be extended to schools, parks, and amenities in the future A LangGraph state machine orchestrating intent ‚Üí SQL ‚Üí geospatial queries ‚Üí MRT enrichment ‚Üí summarization A streamlined CLI (and optional Gradio UI) for natural-language property search The result is an autonomous HDB insights assistant that can answer complex, real-world questions such as:\n‚ÄúFind 4-room units in Toa Payoh under 600k near MRT exits.‚Äù ‚ÄúShow me flats within 500m of Bukit Batok MRT with good value.‚Äù ‚ÄúWhich Bishan blocks have resale units closest to public transport?‚Äù Setup PostgreSQL Extensions We begin by enabling PostGIS, which powers all geospatial queries in this pipeline:\nCREATE EXTENSION IF NOT EXISTS postgis; SELECT extname, extversion FROM pg_extension WHERE extname IN (\u0026#39;postgis\u0026#39;); -- Expected resulta: -- extversion\textname -- 3.6.1\tpostgis\rCopy\rInit Project git clone https://github.com/seehiong/autonomous-hdb-deepagents.git cd autonomous-hdb-deepagents uv sync .venv\\Scripts\\activate\rCopy\rData Ingestion This project relies on three primary data pillars:\nHDB building geometries MRT station exits Geocoded points of interest (via OneMap) These form the critical spatial substrate that allows the deep agent to ‚Äúreason‚Äù about distance, proximity, and locality.\nHDB - Existing Building Dataset Dataset:\nHousing \u0026amp; Development Board. (2023). HDB Existing Building (2025) [Dataset]. data.gov.sg. Retrieved December 3, 2025 from https://data.gov.sg/datasets/d_16b157c52ed637edd6ba1232e026258d/view\rThis dataset is used to build the infrastructure table for HDB block shapes and centroids. Create a Jupyter notebook named hdb-existing-building.ipynb.\n1. SQL Table Creation CREATE TABLE IF NOT EXISTS public.hdb_blocks ( objectid BIGINT, blk_no TEXT, street_code TEXT, entity_id BIGINT, postal_code TEXT, inc_crc TEXT, updated_at TEXT, shape_area DOUBLE PRECISION, shape_len DOUBLE PRECISION, geom_4326 geometry(MultiPolygon, 4326), geom_3414 geometry(MultiPolygon, 3414), centroid_4326 geometry(Point, 4326), centroid_3414 geometry(Point, 3414) );\rCopy\r2. Load GeoJSON This step extracts polygons and centroids, preparing them for spatial indexing.\nimport json import pandas as pd from shapely.geometry import shape from shapely.wkt import dumps as wkt_dumps geo_path = \u0026#34;data/HDBExistingBuilding.geojson\u0026#34; with open(geo_path, \u0026#34;r\u0026#34;) as f: gj = json.load(f) rows = [] for feature in gj[\u0026#34;features\u0026#34;]: geom = shape(feature[\u0026#34;geometry\u0026#34;]) props = feature[\u0026#34;properties\u0026#34;] rows.append({ \u0026#34;objectid\u0026#34;: props.get(\u0026#34;OBJECTID\u0026#34;), \u0026#34;blk_no\u0026#34;: props.get(\u0026#34;BLK_NO\u0026#34;), \u0026#34;street_code\u0026#34;: props.get(\u0026#34;ST_COD\u0026#34;), \u0026#34;entity_id\u0026#34;: props.get(\u0026#34;ENTITYID\u0026#34;), \u0026#34;postal_code\u0026#34;: props.get(\u0026#34;POSTAL_COD\u0026#34;), \u0026#34;inc_crc\u0026#34;: props.get(\u0026#34;INC_CRC\u0026#34;), \u0026#34;updated_at\u0026#34;: props.get(\u0026#34;FMEL_UPD_D\u0026#34;), \u0026#34;shape_area\u0026#34;: props.get(\u0026#34;SHAPE.AREA\u0026#34;), \u0026#34;shape_len\u0026#34;: props.get(\u0026#34;SHAPE.LEN\u0026#34;), \u0026#34;geom_wkt\u0026#34;: wkt_dumps(geom), \u0026#34;centroid_wkt\u0026#34;: wkt_dumps(geom.centroid) }) df = pd.DataFrame(rows) df.head() # Sample Outputs # objectid\tblk_no\tstreet_code\tentity_id\tpostal_code\tinc_crc\tupdated_at\tshape_area\tshape_len\tgeom_wkt\tcentroid_wkt # 0\t898584\t514\tBUS14E\t8235\t650514\t74585E59F18D2E73\t20130426120328\t1033.685604\t253.971941\tPOLYGON ((103.7529821808343655 1.3544208894918...\tPOINT (103.7525891826131073 1.3544681945247203) # 1\t898585\t21\tTEG02M\t6192\t600021\t1550C06FF96161F6\t20130426120305\t1046.092028\t397.417077\tPOLYGON ((103.7392355494047109 1.3236971900920...\tPOINT (103.7392896961472388 1.3236959487788726)\rCopy\r3. Insert into Postgres This batch insertion ensures efficient ingestion of ~13k block geometries.\nfrom sqlalchemy import create_engine, text import time import requests import pandas as pd import time from sqlalchemy import create_engine, text from tqdm.auto import tqdm # DB connection engine = create_engine(\u0026#34;postgresql://postgres:postgres@postgres-postgresql.postgres:5432/postgres\u0026#34;) BATCH_SIZE = 50 MAX_RETRIES = 3 def insert_batch(batch_df, retry_count=0): try: with engine.begin() as conn: for _, row in batch_df.iterrows(): conn.execute(text(\u0026#34;\u0026#34;\u0026#34; INSERT INTO public.hdb_blocks ( objectid, blk_no, street_code, entity_id, postal_code, inc_crc, updated_at, shape_area, shape_len, geom_4326, geom_3414, centroid_4326, centroid_3414 ) VALUES ( :objectid, :blk_no, :street_code, :entity_id, :postal_code, :inc_crc, :updated_at, :shape_area, :shape_len, ST_SetSRID(ST_GeomFromText(:geom_4326), 4326), ST_Transform(ST_SetSRID(ST_GeomFromText(:geom_4326), 4326), 3414), ST_SetSRID(ST_GeomFromText(:centroid_4326), 4326), ‚Ä¶","date":"2025-12-07","permalink":"https://seehiong.github.io/posts/2025/12/autonomous-deepagents-for-hdb-insights/","summary":"This post demonstrates how to build an autonomous DeepAgents-powered pipeline that performs intent extraction, SQL retrieval, and geospatial reasoning ‚Ä¶","tags":["DeepAgents","LangGraph","LangChain","MCP Toolbox","Postgres","PostGIS","Geospatial","OpenRouter","Multi-Agent System","HDB Insights","Natural Language Querying","Real Estate Data","Docker","AI Engineering","Singapore Open Data"],"title":"Autonomous DeepAgents for HDB Insights"},{"content":"Having recently completed Coursera‚Äôs Agentic AI with LangChain and LangGraph\r, I was eager to apply the concepts to a real project. This post walks through the full journey‚Äîfrom building a minimal LangGraph chain, to implementing a ReAct agent, and finally orchestrating a fully automated multi-agent system (MAS) optimized for Singapore HDB data analysis.\nSetup As usual, we begin by initializing a clean Python project using uv\r, an ultra-fast Python package manager and environment tool:\nmkdir multi-agent-system-using-langgraph cd multi-agent-system-using-langgraph # Initialize project uv init . # Create and activate virtual environment uv sync .venv\\Scripts\\activate # Add dependencies uv add langgraph langchain langchain_openai langchain_tavily toolbox-langchain\rCopy\rUse Case 1: TOTO Generator (Deterministic Chain) To warm up, let‚Äôs start with a fun and simple example: a TOTO number generator.\nTOTO\ris a Singaporean lottery where players choose six numbers from 1 to 49. This use case is perfect for demonstrating a simple LangGraph loop with conditional stopping.\nWe implement this in a new notebook: toto_generator.ipynb.\n1. Define ChainState import random from typing import TypedDict from langgraph.graph import StateGraph, END class ChainState(TypedDict): n: int number: int used_numbers: set[int]\rCopy\r2. Add, dump, and stop-condition functions def add(state: ChainState) -\u0026gt; ChainState: used_numbers = state.get(\u0026#34;used_numbers\u0026#34;, set()) if len(used_numbers) \u0026gt;= 49: raise ValueError(\u0026#34;All numbers from 1-49 have been used\u0026#34;) available = set(range(1, 50)) - used_numbers random_number = random.choice(list(available)) new_used_numbers = used_numbers.copy() new_used_numbers.add(random_number) return { **state, \u0026#34;n\u0026#34;: state[\u0026#34;n\u0026#34;] + 1, \u0026#34;number\u0026#34;: random_number, \u0026#34;used_numbers\u0026#34;: new_used_numbers } def dump(state: ChainState) -\u0026gt; ChainState: print(\u0026#34;Current n:\u0026#34;, state[\u0026#34;n\u0026#34;], \u0026#34;number:\u0026#34;, state[\u0026#34;number\u0026#34;]) print(\u0026#34;Winning numbers:\u0026#34;, sorted(state.get(\u0026#34;used_numbers\u0026#34;, set()))) return state def stop_condition(state: ChainState) -\u0026gt; bool: return state[\u0026#34;n\u0026#34;] \u0026gt;= 6\rCopy\r3. Build the graph graph = StateGraph(ChainState) graph.add_node(\u0026#34;add\u0026#34;, add) graph.add_node(\u0026#34;dump\u0026#34;, dump) graph.add_edge(\u0026#34;add\u0026#34;, \u0026#34;dump\u0026#34;) graph.add_conditional_edges(\u0026#34;dump\u0026#34;, stop_condition, { True: END, False: \u0026#34;add\u0026#34;, }) graph.set_entry_point(\u0026#34;add\u0026#34;) app = graph.compile()\rCopy\r4. Run the graph result = app.invoke({\u0026#34;n\u0026#34;: 0, \u0026#34;number\u0026#34;: \u0026#34;\u0026#34;}) # Sample result # Current n: 1 number: 43 # Winning numbers: [43] # Current n: 2 number: 19 # Winning numbers: [19, 43] # Current n: 3 number: 9 # Winning numbers: [9, 19, 43] # Current n: 4 number: 36 # Winning numbers: [9, 19, 36, 43] # Current n: 5 number: 38 # Winning numbers: [9, 19, 36, 38, 43] # Current n: 6 number: 37 # Winning numbers: [9, 19, 36, 37, 38, 43]\rCopy\rCongratulations ‚Äî you‚Äôve just built your first LangGraph-based deterministic generator!\n5. Visualize the chain from IPython.display import Image, display display(Image(app.get_graph().draw_mermaid_png()))\rCopy\rThis diagram clearly illustrates the add ‚Üí dump ‚Üí add loop until six numbers are generated.\nUse Case 2: Full ReAct Agent with MCP, Tavily \u0026amp; SQL In this section, we significantly expand the system: A ReAct-style agent equipped with:\nMCP Toolbox for Postgres (HDB resale data) Tavily for real-world web search Custom Python amenities search tool LangGraph for orchestration This uses the same HDB dataset from my previous article: üëâ ADK Web Multi-Agent System\rThis use case will showcase the full power of your MCP Toolbox SQL suite combined with Tavily + Python tools, inside a ReAct agent powered by LangGraph.\nMCP Toolbox ‚Äì Postgres Configuration To unify the various HDB resale datasets, I first combine them:\nCREATE TABLE hdb_combined_resale_flat_prices AS SELECT month, town, flat_type, block, street_name, storey_range, floor_area_sqm, flat_model, lease_commence_date, resale_price FROM raw_resale_flat_prices_from_jan_2017_onwards UNION ALL SELECT * FROM raw_resale_flat_prices_from_jan_2015_to_dec_2016 UNION ALL SELECT * FROM raw_resale_flat_prices_from_mar_2012_to_dec_2014 UNION ALL SELECT * FROM raw_resale_flat_prices_2000_to_feb_2012 UNION ALL SELECT * FROM raw_resale_flat_prices_1990_1999;\rCopy\rThen I define tools.yaml pointing to my homelab Postgres instance.\nsources: my-pg-source: kind: postgres host: postgres.local port: 5432 database: postgres user: postgres password: postgres tools: postgres-list-tables: kind: postgres-list-tables source: my-pg-source description: Retrieves schema information for all or specified tables. execute-sql-tool: kind: postgres-execute-sql source: my-pg-source description: Executes an arbitrary SQL statement. list-hdb-flats-by-town: kind: postgres-sql source: my-pg-source description: Returns the latest 5 resale transaction records for flats in a given ‚Ä¶","date":"2025-11-23","permalink":"https://seehiong.github.io/posts/2025/11/building-a-langgraph-multi-agent-system/","summary":"This post walks through building a complete multi-agent system using LangGraph‚Äîfrom a simple deterministic chain to a fully orchestrated ReAct agent ‚Ä¶","tags":["LangGraph","LangChain","LangSmith Studio","OpenRouter","Travily","Postgres","MCP Toolbox","Multi-Agent System"],"title":"Building a LangGraph Multi-Agent System"},{"content":"Building upon my earlier post ‚Äî SmartFlat AI: Building AWS Multi-Agent System\r‚Äî this article explores how to use Google\u0026rsquo;s ADK (Agent Development Kit)\rto create an intelligent, web-based multi-agent system. The agents will answer questions about Singapore‚Äôs HDB market, leveraging free models available through OpenRouter.\nWhile the concept is similar to the AWS-based version, this project focuses on Google‚Äôs ADK ecosystem and demonstrates how to integrate it with the MCP Toolbox and PostgreSQL database for structured reasoning.\nPrerequisites For this post, I will be using MCP Toolbox for Databases\rto handle database connections and tool registration.\n$VERSION = \u0026#34;0.18.0\u0026#34; Invoke-WebRequest -Uri \u0026#34;https://storage.googleapis.com/genai-toolbox/v$VERSION/windows/amd64/toolbox.exe\u0026#34; -OutFile \u0026#34;toolbox.exe\u0026#34; # To run Toolbox from binary .\\toolbox # Or with the Toolbox UI .\\toolbox --ui\rCopy\rWe‚Äôll start by initializing a new Python project using uv\r‚Äî a fast Python package manager and environment tool.\nmkdir adk-web-multi-agent cd adk-web-multi-agent # Initialize project uv init # Create and activate virtual environment uv sync .venv\\Scripts\\activate # Add dependencies uv add google-adk litellm toolbox-core\rCopy\rSample pyproject.toml:\n[project] name = \u0026#34;adk-web-multi-agent-system\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Multi Agent System for Google ADK\u0026#34; readme = \u0026#34;README.md\u0026#34; requires-python = \u0026#34;\u0026gt;=3.12\u0026#34; dependencies = [ \u0026#34;google-adk\u0026gt;=1.17.0\u0026#34;, \u0026#34;litellm\u0026gt;=1.78.7\u0026#34;, \u0026#34;toolbox-core\u0026gt;=0.5.2\u0026#34;, ]\rCopy\rSetting up PostgreSQL Database For this demonstration, I used a locally hosted PostgreSQL database containing resale flat prices from Jan 2017 onwards\r.\nBelow is the DDL for the resale_transactions table:\nCREATE TABLE public.resale_transactions ( \u0026#34;month\u0026#34; text NULL, town text NULL, flat_type text NULL, block text NULL, street_name text NULL, storey_range text NULL, floor_area_sqm text NULL, flat_model text NULL, lease_commence_date text NULL, resale_price int4 NULL );\rCopy\rConfiguring MCP Toolbox Toolbox acts as the interface layer between ADK and the database.\nHere‚Äôs a sample configuration using the default tools.yaml file:\nsources: my-pg-source: kind: postgres host: 127.0.0.1 port: 5432 database: postgres user: postgres password: postgres tools: postgres-list-tables: kind: postgres-list-tables source: my-pg-source description: Use this tool to retrieve schema information for all or specified tables. Output format can be simple (only table names) or detailed. execute-sql-tool: kind: postgres-execute-sql source: my-pg-source description: Use this tool to execute sql statement. list-hdb-flats-by-town: kind: postgres-sql source: my-pg-source description: Finds the most recent 5 resale transaction records for HDB flats in the specified town parameters: - name: town type: string description: The town of the HDB flat statement: SELECT * FROM public.resale_transactions t WHERE t.town ILIKE \u0026#39;%\u0026#39; || $1 || \u0026#39;%\u0026#39; ORDER BY t.month DESC LIMIT 5; count-hdb-flats-by-town: kind: postgres-sql source: my-pg-source description: Calculates the total number of HDB resale transactions that have occurred in the specified town since 2017 parameters: - name: town type: string description: The town of the HDB flat statement: SELECT COUNT(*) as count FROM public.resale_transactions t WHERE t.town ILIKE \u0026#39;%\u0026#39; || $1 || \u0026#39;%\u0026#39;; percentile-price-by-town: kind: postgres-sql source: my-pg-source description: Calculates a specific resale price percentile (e.g., median price) based on all recorded HDB resale transactions in the specified town parameters: - name: town type: string description: The town to check the percentile price for - name: percentile type: float description: The percentile to calculate (e.g., 0.5 for median, 0.9 for 90th percentile) default: 0.5 statement: SELECT PERCENTILE_CONT($2) WITHIN GROUP (ORDER BY t.resale_price) AS percentile_price FROM public.resale_transactions t WHERE t.town ILIKE \u0026#39;%\u0026#39; || $1 || \u0026#39;%\u0026#39;; average-price-by-flat-type: kind: postgres-sql source: my-pg-source description: Calculates the average resale price for a specific flat type across all of Singapore or filtered by town parameters: - name: flat_type type: string description: The type of flat (e.g., \u0026#39;3 ROOM\u0026#39;, \u0026#39;4 ROOM\u0026#39;, \u0026#39;5 ROOM\u0026#39;) - name: town type: string description: Optional town filter. Leave empty or use \u0026#39;%\u0026#39; for all of Singapore default: \u0026#39;%\u0026#39; statement: SELECT AVG(t.resale_price) as avg_price FROM public.resale_transactions t WHERE t.flat_type = $1 AND t.town ILIKE \u0026#39;%\u0026#39; || $2 || \u0026#39;%\u0026#39;; toolsets: my-toolset: - postgres-list-tables - execute-sql-tool - list-hdb-flats-by-town - count-hdb-flats-by-town - percentile-price-by-town - average-price-by-flat-type\rCopy\rBuilding the Agent With the tools configured, we can now create an ADK-powered database agent that interacts with PostgreSQL. ‚Ä¶","date":"2025-11-02","permalink":"https://seehiong.github.io/posts/2025/11/adk-web-multi-agent-system/","summary":"This blog post details how I built a multi-agent system using Google\u0026rsquo;s ADK, powered by OpenRouter models, for a web application focused on ‚Ä¶","tags":["Google ADK","Multi-Agent System","OpenRouter","MCP Toolbox","PostgreSQL","DataCommons","LangChain","LangGraph"],"title":"ADK Web Multi-Agent System"},{"content":"In this post, I‚Äôll walk you through how I built SmartFlat AI, a cost-optimized multi-agent AI system for Singapore\u0026rsquo;s HDB market analysis using AWS Bedrock AgentCore orchestrated by Amazon Nova Lite.\nPrerequisites Start by installing the AWS Bedrock AgentCore Starter Toolkit\r:\npip install aws-bedrock-agentcore-starter-toolkit\rCopy\rEnsure that AWS CLI and SAM CLI are installed and configured properly:\nagentcore --help # Usage: agentcore [OPTIONS] COMMAND [ARGS]... # BedrockAgentCore CLI aws --version # aws-cli/2.30.5 Python/3.13.7 Windows/11 exe/AMD64 aws configure # AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE # AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Default region name [None]: # Default output format [None]: sam --version # SAM CLI, version 1.144.0\rCopy\rNote\rAWS Bedrock AgentCore is currently unavailable in Singapore, so the deployment targets ap-southeast-2 (Sydney). Amazon Nova Lite, an efficient reasoning model, powers the Supervisor Agent for intelligent query routing.\rMulti-Agent Architecture Overview The system implements a supervisor-agent architecture where a top-level orchestrator delegates queries to specialized sub-agents for optimal cost and performance.\nKey Components:\nSupervisor Agent (Nova Lite) ‚Äì Routes queries intelligently based on cost and complexity. Property Agent (Athena) ‚Äì Handles analytical and statistical queries. PostgreSQL Agent (RDS) ‚Äì Serves frequent and simple queries at low cost. Demographics Agent ‚Äì Fetches live data from government APIs. Phased Infrastructure Deployment Deployment is handled in modular CloudFormation stacks, allowing incremental development and testing.\nPhase 1: Foundation Stack Purpose: Establish the web interface, chat handler, and session memory.\nKey Components:\nS3 Static Website: Hosts the chat UI API Gateway HTTP API: Routes user queries to Lambda Lambda Chat Handler: Connects frontend with AgentCore Supervisor DynamoDB: Stores session memory IAM Roles: Secure access to Bedrock and data stores # Phase 1 creates the foundation Resources: WebHostingBucket: Type: \u0026#39;AWS::S3::Bucket\u0026#39; Properties: BucketName: !Sub \u0026#34;${ProjectPrefix}-web-${EnvironmentName}-${VersionTag}\u0026#34; WebsiteConfiguration: IndexDocument: index.html ErrorDocument: index.html ChatHandlerLambda: Type: \u0026#39;AWS::Lambda::Function\u0026#39; Properties: FunctionName: !Sub \u0026#34;${ProjectPrefix}-chat-handler-${EnvironmentName}-${VersionTag}\u0026#34; Runtime: python3.13 Environment: Variables: AGENT_ARN: !Ref AgentArn DDB_TABLE_NAME: !Ref AgentMemoryTable\rCopy\rPhase 2: Athena Analytics Stack Purpose: Enable serverless analytics for large-scale HDB resale data.\nKey Components:\nAWS Glue Database: Schema catalog for HDB datasets Amazon Athena: Query engine for analytical workloads S3 Data Lake: 300K+ resale transactions from data.gov.sg\rAthena API Lambda: RESTful interface for SQL analytics Query Results Bucket: Stores Athena query outputs with lifecycle policies # Phase 2 adds analytical capabilities Resources: HDBGlueTable: Type: AWS::Glue::Table Properties: TableInput: Name: \u0026#39;resale_transactions\u0026#39; StorageDescriptor: Columns: - Name: \u0026#39;month\u0026#39; Type: \u0026#39;string\u0026#39; - Name: \u0026#39;town\u0026#39; Type: \u0026#39;string\u0026#39; - Name: \u0026#39;flat_type\u0026#39; Type: \u0026#39;string\u0026#39; - Name: \u0026#39;resale_price\u0026#39; Type: \u0026#39;double\u0026#39; Location: !Sub \u0026#39;s3://${ProjectPrefix}-rawdata-${EnvironmentName}-${VersionTag}/\u0026#39;\rCopy\rPhase 3: Cost-Optimized PostgreSQL Stack Purpose: Handle frequent, low-latency queries at minimal cost.\nKey Components:\nPostgreSQL RDS: Mirrors HDB data for transactional queries RDS Proxy: Improves scalability and connection pooling PostgreSQL API Lambda: Executes fast SQL lookups Secrets Manager: Manages DB credentials securely VPC Integration: Provides secure private networking # Phase 3 adds cost optimization Resources: PostgreSQLAPIFunction: Type: AWS::Lambda::Function Properties: Runtime: python3.13 VpcConfig: SecurityGroupIds: - !Ref LambdaSecurityGroup SubnetIds: - subnet-05ef955b7c5edb66e - subnet-0be15cf15ab7853eb Environment: Variables: SECRET_ARN: !Ref DatabaseSecret CONNECTION_TYPE: direct_connection\rCopy\rIntelligent Query Routing: The Cost Optimization Brain The Supervisor Agent, powered by Amazon Nova Lite, dynamically routes queries across agents based on complexity, latency, and cost.\n1. Demographics Path Triggers: population, citizen, statistics, demographic Routes to: Data Commons API\rExample: \u0026ldquo;What is the population of Singapore?\u0026rdquo; Cost: Free (public data source) 2. Complex Analytics Path Triggers: correlation, trend, statistical, complex Routes to: Amazon Athena Example: ‚ÄúComplex price trend analysis for 5-room flats in Tampines over 5 years‚Äù Cost: ~$0.59/query 3. Cost-Optimized Path Triggers: average, count, latest, basic, quick Routes to: PostgreSQL RDS Example: \u0026ldquo;Average HDB prices in Bishan for 4-room flats\u0026rdquo; Cost: ~$0.01/query (~70% cheaper than Athena) Routing Logic Implementation def ‚Ä¶","date":"2025-10-11","permalink":"https://seehiong.github.io/posts/2025/10/smartflat-ai-building-aws-multi-agent-system/","summary":"This post explores how I built SmartFlat AI, a multi-agent system powered by AWS Bedrock AgentCore and Amazon Nova Lite to analyze Singapore‚Äôs HDB ‚Ä¶","tags":["AWS","Bedrock","AgentCore","Nova Lite","Multi-Agent System","Serverless","CloudFormation","Lambda","Athena","RDS","PostgreSQL","AI Agent","Cost Optimization","HDB"],"title":"SmartFlat AI: Building AWS Multi-Agent System"},{"content":"In this post, I‚Äôll walk you through how I built ATS Buddy, an AI assistant powered by AWS Bedrock, developed for the AWS AI Agent Global Hackathon\r.\nThe inspiration came from a frustration many job seekers share: with over 75% of resumes filtered out by ATS systems before reaching a recruiter, countless qualified candidates are overlooked daily. ATS Buddy addresses this challenge while also tackling a critical concern ‚Äî the privacy of sensitive resume data.\nTo solve this, I designed ATS Buddy with a privacy-first architecture, ensuring PII redaction and strict data lifecycle management. The goal was simple: help candidates get noticed while protecting their personal information.\nSetup I used KIRO\r, an AI IDE for rapid prototyping to production. After joining the waitlist and obtaining an invite code, I started building ATS Buddy.\nFor each specification, KIRO automatically generates requirements.md, design.md, and tasks.md. The updated UI now also shows usage stats conveniently in the corner.\nHigh-Level Architecture Here‚Äôs the high-level architecture diagram of ATS Buddy:\nAWS Managed Multi-AZ High Availability AWS automatically distributes the application across multiple physically isolated data centers (Availability Zones) within a region:\nInfrastructure As Code All infrastructure is defined with AWS SAM (Serverless Application Model). A detailed DEPLOYMENT_GUIDE.md is included in the project repository.\nPrerequisites Install the AWS CLI\r. aws --version # aws-cli/2.30.5 Python/3.13.7 Windows/11 exe/AMD64\rCopy\rConfigure IAM user access keys following Manage Access Keys for IAM Users: aws configure # AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE # AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Default region name [None]: # Default output format [None]:\rCopy\rInstall the AWS SAM CLI\rfor deploying serverless applications via IaC. Key Components The ATS Buddy infrastructure is defined using AWS SAM, which extends CloudFormation with serverless-specific syntax. Let me walk you through the critical components that make this application work.\n1. Storage Layer: S3 Buckets ResumesBucket: Temporary storage, auto-deletes after 24h ReportsBucket: Secure report storage with presigned URL access WebUIBucket: Hosts the static web app LifecycleConfiguration: Rules: - Id: DeleteOldResumes Status: Enabled ExpirationInDays: 1 NoncurrentVersionExpirationInDays: 1\rCopy\r2. Privacy-First Design: PII Redaction Pipeline This is ATS Buddy‚Äôs most innovative feature: resumes are sanitized before processing.\nS3 Access Point ‚Üí PIIRedaction Lambda (Comprehend) ‚Üí Object Lambda Access Point This ensures no raw PII ever reaches downstream functions.\nPIIRedactionObjectLambdaAccessPoint: Type: AWS::S3ObjectLambda::AccessPoint Properties: Name: !Sub \u0026#34;pii-redacted-resumes-${Environment}\u0026#34; ObjectLambdaConfiguration: SupportingAccessPoint: !Sub \u0026#34;${ResumesBucketAccessPoint.Arn}\u0026#34; TransformationConfigurations: - Actions: - GetObject ContentTransformation: AwsLambda: FunctionArn: !GetAtt PIIRedactionFunction.Arn\rCopy\r3. Processing Layer: Lambda Functions This is the heart of ATS Buddy. It orchestrates the entire workflow:\nUpload via API Gateway Extract text with Textract Read redacted data only Analyze with Bedrock (Nova Lite) Generate ATS reports The function is configured with 512MB memory and a 5-minute timeout to handle large resumes and complex AI processing.\nEnvironment: Variables: RESUMES_BUCKET: !Ref ResumesBucket REPORTS_BUCKET: !Ref ReportsBucket RESUME_CACHE_TABLE: !Ref ResumeCacheTable PII_REDACTED_ACCESS_POINT: !GetAtt PIIRedactionObjectLambdaAccessPoint.Arn\rCopy\r4. Caching Layer: DynamoDB Resumes are hashed and cached for 24h using TTL, reducing costs and re-processing.\nResumeCacheTable: Type: AWS::DynamoDB::Table Properties: TableName: !Sub \u0026#34;ats-buddy-resume-cache-${Environment}\u0026#34; BillingMode: PAY_PER_REQUEST TimeToLiveSpecification: AttributeName: ttl Enabled: true\rCopy\r5. API Layer: API Gateway RESTful endpoints power the web UI with CORS support.\nPOST /analyze: Upload and analyze a resume POST /enhance: Generate an enhanced version based on analysis OPTIONS endpoints: Handle CORS preflight requests Cors: AllowMethods: \u0026#34;\u0026#39;GET,POST,PUT,DELETE,OPTIONS\u0026#39;\u0026#34; AllowHeaders: \u0026#34;\u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token,X-Requested-With\u0026#39;\u0026#34; AllowOrigin: \u0026#34;\u0026#39;*\u0026#39;\u0026#34;\rCopy\r6. Security and IAM Each Lambda follows least-privilege policies, tightly scoped to their role (e.g., redaction function cannot access DynamoDB).\nATSBuddyLambdaRole: Grants access to S3 (both regular and Object Lambda access points), Textract for document processing, Bedrock for AI analysis, and DynamoDB for caching. PIIRedactionLambdaRole: Limited to only S3 Object Lambda operations and Amazon Comprehend for PII detection. This function can\u0026rsquo;t access any other AWS resources, minimizing the attack surface. Policies: - PolicyName: ComprehendAccess PolicyDocument: ‚Ä¶","date":"2025-10-05","permalink":"https://seehiong.github.io/posts/2025/10/ats-buddy-privacy-first-resume-ai/","summary":"ATS Buddy is an AI-powered resume analysis tool built with AWS Bedrock, Textract, and Comprehend. Designed with a privacy-first approach, it redacts ‚Ä¶","tags":["ATS","AWS","AI","Resume Analysis","Serverless","Bedrock","Textract","Comprehend","Hackathon","Privacy"],"title":"ATS Buddy: Privacy-First Resume AI"},{"content":" Building upon my previous post\r, this time I‚Äôll demonstrate how to connect n8n with Blender via MCP. By combining n8n‚Äôs automation capabilities with Blender‚Äôs modeling power, we can drive 3D creation workflows with natural language and AI agents.\nUpdating to the Latest n8n Image As mentioned in Automating Workflows with n8n\r, I‚Äôm updating to the latest n8n image and configuring a runner as a sidecar. Task runners\rprovide a secure and performant mechanism to execute tasks.\nHere‚Äôs my deployment configuration:\n# n8n-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: io.kompose.service: n8n name: n8n namespace: n8n spec: replicas: 1 selector: matchLabels: io.kompose.service: n8n strategy: type: Recreate template: metadata: labels: io.kompose.service: n8n spec: containers: # Main n8n container - env: - name: DB_POSTGRESDB_HOST value: postgres - name: DB_POSTGRESDB_PASSWORD value: postgres - name: DB_POSTGRESDB_USER value: postgres - name: DB_TYPE value: postgresdb - name: N8N_DIAGNOSTICS_ENABLED value: \u0026#34;false\u0026#34; - name: N8N_PERSONALIZATION_ENABLED value: \u0026#34;false\u0026#34; - name: OLLAMA_HOST value: ollama:11434 # Task runner configuration - name: N8N_RUNNERS_ENABLED value: \u0026#34;true\u0026#34; - name: N8N_RUNNERS_MODE value: external - name: N8N_RUNNERS_BROKER_LISTEN_ADDRESS value: 0.0.0.0 - name: N8N_RUNNERS_AUTH_TOKEN value: my-n8n-runners-secure-token - name: N8N_NATIVE_PYTHON_RUNNER value: \u0026#34;true\u0026#34; envFrom: - configMapRef: name: env image: n8nio/n8n:1.111.0 imagePullPolicy: Always name: n8n ports: - containerPort: 5678 protocol: TCP - containerPort: 5679 # Task runner broker port protocol: TCP volumeMounts: - mountPath: /home/node/.n8n name: n8n-storage - mountPath: /data/shared name: n8n-claim2 - mountPath: /demo-data name: demo-data # Task runners container - env: - name: N8N_RUNNERS_TASK_BROKER_URI value: http://localhost:5679 - name: N8N_RUNNERS_AUTH_TOKEN value: my-n8n-runners-secure-token - name: N8N_RUNNERS_AUTO_SHUTDOWN_TIMEOUT value: \u0026#34;15\u0026#34; image: n8nio/runners:1.111.0 imagePullPolicy: Always name: n8n-runners volumeMounts: - mountPath: /data/shared name: n8n-claim2 hostname: n8n restartPolicy: Always volumes: - name: n8n-storage persistentVolumeClaim: claimName: n8n-storage - name: n8n-claim2 persistentVolumeClaim: claimName: n8n-claim2 - name: demo-data persistentVolumeClaim: claimName: demo-data-pvc\rCopy\rDeploy with:\nkubectl apply -f n8n-deployment.yaml kubectl rollout restart deployment n8n\rCopy\rAI Agent Chat Workflow To start, let‚Äôs build a simple chat workflow following the AI Agent Chat template\r. I added the following nodes:\nChat Trigger\rAI Agent\rOpenRouter Chat Model\rWikipedia\rWhen I tested with:\nMy name is seehiong Where is Singapore Whats my name ‚Ä¶the chat model responded, but it didn‚Äôt retain my name.\nAdding a Simple Memory node\r, fixed this‚Äîallowing the agent to recall previous messages.\nPreparing the Blender Bridge Server The next step was bridging the AI agent to Blender‚Äôs MCP tool. While ideally we‚Äôd run uvx blender-mcp directly inside n8n, that would require a custom node. For now, I created a lightweight wrapper service: blender-bridge-server.py.\nThis service acts as a Fast HTTP-to-MCP bridge, using:\nPersistent connection pooling for low latency Structured chat processing with OpenRouter models Tool invocation (scene info, object info, code execution) # blender-bridge-server.py \u0026#34;\u0026#34;\u0026#34; Fast HTTP-to-MCP Bridge Service for Blender Uses persistent connections for speed matching original raw TCP client \u0026#34;\u0026#34;\u0026#34; from flask import Flask, request, jsonify import asyncio import os import json import threading import time import uuid from datetime import datetime, timedelta from typing import Dict, List, Optional, Any from dataclasses import dataclass import logging from openai import OpenAI from dotenv import load_dotenv import queue load_dotenv() # Configuration @dataclass class Config: model: str = os.getenv(\u0026#34;MODEL\u0026#34;, \u0026#34;openrouter/anthropic/claude-3-sonnet\u0026#34;) blender_host: str = os.getenv(\u0026#34;BLENDER_HOST\u0026#34;, \u0026#34;127.0.0.1\u0026#34;) blender_port: int = int(os.getenv(\u0026#34;BLENDER_PORT\u0026#34;, 9876)) openrouter_api_key: str = os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;) max_messages: int = 30 session_timeout: int = 1800 temperature: float = 0.0 seed: int = 42 max_connections: int = 20 # Connection pool size config = Config() # Logging setup logging.basicConfig(level=logging.INFO, format=\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) logger = logging.getLogger(__name__) class PersistentMCPConnection: \u0026#34;\u0026#34;\u0026#34;Single persistent connection to Blender MCP\u0026#34;\u0026#34;\u0026#34; def __init__(self, connection_id: str): self.id = connection_id self.reader: Optional[asyncio.StreamReader] = None self.writer: Optional[asyncio.StreamWriter] = None self.connected = False self.last_used = time.time() self.use_count = 0 self._lock = asyncio.Lock() async def connect(self): \u0026#34;\u0026#34;\u0026#34;Connect to Blender MCP server\u0026#34;\u0026#34;\u0026#34; try: self.reader, ‚Ä¶","date":"2025-09-21","permalink":"https://seehiong.github.io/posts/2025/09/ai-driven-3d-workflows-with-n8n/","summary":"This post explores how to integrate n8n, OpenRouter, and Blender MCP to create AI-driven 3D modeling workflows. Starting with an AI agent chat in n8n, ‚Ä¶","tags":["n8n","MCP","Blender","AI","3D","OpenRouter","Automation"],"title":"AI-Driven 3D Workflows with n8n"},{"content":"In this post, I‚Äôll walk you through how I experimented with the Model Context Protocol (MCP) and connected it with Blender to create a datacenter model.\nSetup Following the MCP Python SDK\r, I used the recommended uv\r‚Äî an extremely fast Python package and project manager ‚Äî to create a uv-managed project:\nuv init blender-mcp cd blender-mcp\rCopy\rInstall dependencies:\nuv sync .venv\\Scripts\\activate uv add mcp[cli] openai python-dotenv Copy\rUsing MCP Servers with OpenRouter Referencing the Using MCP Servers with OpenRouter\rguide, I created a simple mcp-client.py:\n# mcp-client.py import asyncio from typing import Optional from contextlib import AsyncExitStack from mcp import ClientSession, StdioServerParameters from mcp.client.stdio import stdio_client from openai import OpenAI from dotenv import load_dotenv import json import os user_home_path = os.path.expanduser(\u0026#34;~\u0026#34;).replace(\u0026#34;\\\\\u0026#34;, \u0026#34;/\u0026#34;) load_dotenv() # load environment variables from .env MODEL = \u0026#34;openrouter/sonoma-dusk-alpha\u0026#34; SERVER_CONFIG = { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, \u0026#34;C:\\\\Program Files\u0026#34;], \u0026#34;env\u0026#34;: None } def convert_tool_format(tool): converted_tool = { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: tool.name, \u0026#34;description\u0026#34;: tool.description, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: tool.inputSchema[\u0026#34;properties\u0026#34;], \u0026#34;required\u0026#34;: tool.inputSchema[\u0026#34;required\u0026#34;] } } } return converted_tool class MCPClient: def __init__(self): self.session: Optional[ClientSession] = None self.exit_stack = AsyncExitStack() self.openai = OpenAI( base_url=\u0026#34;https://openrouter.ai/api/v1\u0026#34;, api_key=os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;) ) async def connect_to_server(self, server_config): server_params = StdioServerParameters(**server_config) stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params)) self.stdio, self.write = stdio_transport self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write)) await self.session.initialize() # List available tools from the MCP server response = await self.session.list_tools() print(\u0026#34;\\nConnected to server with tools:\u0026#34;, [tool.name for tool in response.tools]) self.messages = [] async def process_query(self, query: str) -\u0026gt; str: self.messages.append({ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query }) response = await self.session.list_tools() available_tools = [convert_tool_format(tool) for tool in response.tools] response = self.openai.chat.completions.create( model=MODEL, tools=available_tools, messages=self.messages ) self.messages.append(response.choices[0].message.model_dump()) final_text = [] content = response.choices[0].message if content.tool_calls is not None: tool_name = content.tool_calls[0].function.name tool_args = content.tool_calls[0].function.arguments tool_args = json.loads(tool_args) if tool_args else {} # Execute tool call try: result = await self.session.call_tool(tool_name, tool_args) final_text.append(f\u0026#34;[Calling tool {tool_name} with args {tool_args}]\u0026#34;) except Exception as e: print(f\u0026#34;Error calling tool {tool_name}: {e}\u0026#34;) result = None self.messages.append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: content.tool_calls[0].id, \u0026#34;name\u0026#34;: tool_name, \u0026#34;content\u0026#34;: result.content }) response = self.openai.chat.completions.create( model=MODEL, max_tokens=1000, messages=self.messages, ) final_text.append(response.choices[0].message.content) else: final_text.append(content.content) return \u0026#34;\\n\u0026#34;.join(final_text) async def chat_loop(self): \u0026#34;\u0026#34;\u0026#34;Run an interactive chat loop\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\nMCP Client Started!\u0026#34;) print(\u0026#34;Type your queries or \u0026#39;quit\u0026#39; to exit.\u0026#34;) while True: try: query = input(\u0026#34;\\nQuery: \u0026#34;).strip() result = await self.process_query(query) print(\u0026#34;Result:\u0026#34;) print(result) except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) async def cleanup(self): await self.exit_stack.aclose() async def main(): client = MCPClient() try: await client.connect_to_server(SERVER_CONFIG) await client.chat_loop() finally: await client.cleanup() if __name__ == \u0026#34;__main__\u0026#34;: import sys asyncio.run(main())\rCopy\rNote\rI made slight adjustments to the referenced code: configuring the API key and server paths for Windows compatibility.\rRunning the MCP Client Execute the client:\npython mcp-client.py\rCopy\rExample outputs Using anthropic/claude-3-7-sonnet Using openrouter/sonoma-dusk-alpha MCP Inspector The MCP inspector\ris a handy developer tool for testing and debugging MCP servers.\nnpx @modelcontextprotocol/inspector\rCopy\rThe server starts a http://localhost:6274, giving you a web UI to explore.\nBlender MCP Server BlenderMCP\rconnects directly to Blender\r, one of the most powerful open-source 3D tools available.\nInstallation steps ‚Ä¶","date":"2025-09-13","permalink":"https://seehiong.github.io/posts/2025/09/blender-meets-mcp/","summary":"A step-by-step guide to integrating the Model Context Protocol (MCP) with Blender. From setup and server connections to building a datacenter model, ‚Ä¶","tags":["Blender","MCP","AI","3D","OpenRouter"],"title":"Blender Meets MCP"},{"content":"While working on one of the hackathon projects, I encountered a tricky issue when uploading a PDF document to a web-based application. As it was a race against time, I quickly decided to use pdf.js\rto get things going. As a reminder, PDF\ris a file format developed by Adobe.\nSetup Since I already use Notepad++\r, I installed the HEX-Editor plugin via Plugins \u0026gt; Plugins Admin\u0026hellip;. Simply search and install it from there.\nFor the purpose of this post, I will use the reserach paper Attention Is All You Need\ras the example PDF.\nTo manipulate and inspect the PDF structure, I installed qpdf\r, a C++ library that enables structural, content-preserving transformations on PDF files. The latest version e.g. qpdf 12.2.0\rcan be downloaded from GitHub.\nHEX Editing with NotePad++ Dragging the Attention Is All You Need PDF into Notepad++ gives us a hex representation of the file. However, since it‚Äôs in raw hex, it\u0026rsquo;s not easy to interpret.\nUsing qpdf for Extraction Let\u0026rsquo;s now try some cli commands\r. Using the --qdf option, we can create a human-readable version of the PDF:\nqpdf 1706.03762v7.pdf --qdf file.pdf\rCopy\rHere‚Äôs a snippet of the output:\n%PDF-1.5 %¬ø√∑¬¢√æ %QDF-1.0 %% Original object ID: 5953 0 1 0 obj \u0026lt;\u0026lt; /Names 3 0 R /OpenAction 4 0 R /Outlines 5 0 R /PageMode /UseOutlines /Pages 6 0 R /Type /Catalog \u0026gt;\u0026gt; endobj %% Original object ID: 5954 0 2 0 obj \u0026lt;\u0026lt; /Author () /CreationDate (D:20240410211143Z) /Creator (LaTeX with hyperref) /Keywords () /ModDate (D:20240410211143Z) /PTEX.Fullbanner (This is pdfTeX, Version 3.141592653-2.6-1.40.25 \\(TeX Live 2023\\) kpathsea version 6.3.5) /Producer (pdfTeX-1.40.25) /Subject () /Title () /Trapped /False \u0026gt;\u0026gt; endobj %% Original object ID: 5952 0 3 0 obj \u0026lt;\u0026lt; /Dests 7 0 R \u0026gt;\u0026gt; endobj\rCopy\rWe can also output the PDF in JSON format:\nqpdf 1706.03762v7.pdf --json-output inline.json\rCopy\rHere‚Äôs a sample:\n{ \u0026#34;qpdf\u0026#34;: [ { \u0026#34;jsonversion\u0026#34;: 2, \u0026#34;pdfversion\u0026#34;: \u0026#34;1.5\u0026#34;, \u0026#34;pushedinheritedpageresources\u0026#34;: false, \u0026#34;calledgetallpages\u0026#34;: false, \u0026#34;maxobjectid\u0026#34;: 5957 }, { \u0026#34;obj:1 0 R\u0026#34;: { \u0026#34;value\u0026#34;: { \u0026#34;/D\u0026#34;: \u0026#34;u:section.1\u0026#34;, \u0026#34;/S\u0026#34;: \u0026#34;/GoTo\u0026#34; } }, \u0026#34;obj:2 0 R\u0026#34;: { \u0026#34;stream\u0026#34;: { \u0026#34;data\u0026#34;: ‚Ä¶","date":"2025-08-31","permalink":"https://seehiong.github.io/posts/2025/08/reverse-engineering-pdfs-with-ai-tools/","summary":"What started as a corrupted PDF upload led to a deep exploration of PDF internals using tools like qpdf, PyMuPDF, and Grok Code Fast 1. This post ‚Ä¶","tags":["PDF Reconstruction","AI tools","Reverse Engineering","PyMuPDF","qpdf","Embedded Data","Tech Tutorial","Document Parsing","AI","Data Extraction"],"title":"Reverse-Engineering PDFs with AI Tools"},{"content":"With the recent release of Introducing gpt-oss\rfrom OpenA, I decided to put it to the test on my newly purchased AMD Ryzen AI Max+ 395\rmini PC. I allocated 96 GB of 128 GB RAM exclusively to the GPU to see how well it could handle the model.\nMy goal: build a simple chat application that sends a single query to multiple models (up to five) and displays all responses in one unified interface.\nPrerequistes The first step was setting up Cursor\r, an AI-powered code editor. After signing up for the free plan (which comes with limited completions and agents), I installed the Kilo Code\rplugin for extended coding assistance.\nRunning gpt-oss with Ollama I used Ollama\rto download and run the gpt-oss 120B model.\nTo serve the model locally, I ran:\nset OLLAMA_HOST=0.0.0.0 set OLLAMA_ORIGINS=\u0026#34;*\u0026#34; ollama serve\rCopy\rNote\rFrom my early prompts, the Ollama chat UI felt noticeably more responsive compared to the raw CLI via ollama serve.\rVibe Coding with Cursor With Cursor‚Äôs free plan, I began with this vibe coding prompt:\nCreate a modern single-page web application (SPA) for a chat system with the following features: 1. **Frontend UI** - Clean, minimalistic, and responsive chat interface - Input box for user questions - Display area for model responses, grouped by model name - Option to select number of models to query (1 to 5) - Loading indicators while waiting for responses - Support for markdown rendering in model replies 2. **Backend Logic (Serverless Function)** - Uses OpenRouter API to query multiple LLMs concurrently - Accepts user input and number of models to query - Sends the same prompt to selected models via OpenRouter - Waits for responses and returns them as a structured JSON payload - Handles timeouts and partial responses gracefully 3. **Model Selection** - Default models include: GPT-3.5, Claude 2, Mistral, LLaMA 2, Gemini - Models can be hardcoded or dynamically fetched from OpenRouter 4. **Tech Stack** - Frontend: React + Tailwind CSS (or similar modern stack) - Backend: Serverless function (e.g., Vercel, Netlify, or Supabase Edge Functions) - API: OpenRouter with OpenAI-compatible endpoints 5. **Extensibility** - Easy to add/remove models - Modular architecture for future features like chat history, user auth, or feedback Generate the full codebase including: - React components for UI - Serverless function to handle OpenRouter requests - Utility functions for concurrent model querying - Basic styling and layout\rCopy\rCursor + Kilo Code generated a decent foundation for the app with minimal effort.\nIntegrate Kilo Code with Ollama Next, I attempted to pair Kilo Code with my local Ollama instance. Under Settings ‚Üí Providers ‚Üí API Provider, I selected OpenAI Compatible, entered my Mini PC‚Äôs IP as the Base URL, and used a placeholder API key:\nhttp://192.168.68.120:11434/v1\rCopy\rI then created a custom model gpt-oss:120b and tested with:\ncurl http://192.168.68.120:11434/api/generate -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;{ \\\u0026#34;model\\\u0026#34;: \\\u0026#34;gpt-oss:120b\\\u0026#34;, \\\u0026#34;prompt\\\u0026#34;: \\\u0026#34;How are you today?\\\u0026#34; }\u0026#34;\rCopy\rInfo\rPerformance was too slow to be practical. Likely, some optimizations or configurations are still needed.\rIntegrate Kilo Code with OpenRouter Switching to OpenRouter was smoother. This time, I selected OpenRouter as the provider, entered my API key, and chose from the available models (both free and paid).\nKilo Code worked seamlessly with multiple models, editing files via both inline edits and diffs.\nExample using the free DeepSeek model:\nMulti-Model Chat Application After some trial-and-error (and a few resets), I finally got the multi-model chat app working end-to-end.\nHere‚Äôs an example query sent to 4 models in parallel:\nWith this setup, I no longer need to open multiple browser tabs and paste the same prompt repeatedly‚Äîone query fans out to all selected models instantly.\nThat said, local gpt-oss is still significantly slower compared to cloud-hosted commercial models.\nWhat\u0026rsquo;s Next I‚Äôll continue experimenting with ways to optimize performance (especially for local inference).\nüëâ Repo link: https://github.com/seehiong/multi-model-chat\rHave fun exploring, and feel free to fork and customize!\n","date":"2025-08-19","permalink":"https://seehiong.github.io/posts/2025/08/run-gpt-oss-locally-on-ryzen-ai/","summary":"Learn how to run OpenAI‚Äôs new gpt-oss model locally on a Ryzen AI Max+ 395 mini PC. This guide covers setting up Ollama, Cursor with Kilo Code, and ‚Ä¶","tags":["gpt-oss","Ryzen AI","mini PC","Ollama","Cursor","Kilo Code","OpenRouter","multi-model chat","AI development"],"title":"Run gpt-oss Locally on Ryzen AI"},{"content":"As I decided to discontinue my bolt.new\rsubscription‚Äîmainly because I wasn‚Äôt actively using it‚ÄîI chose to make the most of my remaining tokens during my recent annual leave. In this post, I‚Äôll showcase two promising apps I created during that time. Sadly, I wasn‚Äôt able to fully utilize all ~4 million tokens, but the journey was fun and rewarding.\nüöÄCreating NoteFlow With the recent vibe coding trend, building rapid prototypes has taken center stage over lengthy specifications. The AI tools we now have at our fingertips make it incredibly easy‚Äîand tempting‚Äîto explore ideas quickly.\nInterestingly, NoteFlow reminds me of my Final Year Project (FYP) at Singapore Polytechnic. That project was much more ambitious and feature-complete than this prototype. In a team of four, we:\nBuilt a MIDI-based music editor from scratch using Pascal\rInterfaced an actual Sound Blaster\rsound card with a real MIDI keyboard Supported multi-channel playback (up to 4 channels) Enabled channel-based instrument selection (e.g., piano, guitar, organ) Supported real-time play and record directly from the keyboard In contrast, NoteFlow was a quick prototype built within the bolt.new interface using less than 1M tokens. Still, it was fun to see how rapidly we can now recreate basic ideas in the browser with modern tooling and AI.\nüîó GitHub Repository: seehiong/noteflow\rüõ†Ô∏è Building NoteFlow Clone and run the project locally:\ngit clone https://github.com/seehiong/noteflow cd noteflow npm install npm run dev\rCopy\rOnce it\u0026rsquo;s running, visit http://localhost:5173 to try it out!\nYour browser does not seem to support the HTML5 video tag. You can download the video instead.\rüåç Deploying to Netlify Netlify\rmakes it incredibly easy to push your app live‚Äîfrom frontend sites to AI experiments. To get started, install the Netlify CLI:\nnpm install -g netlify-cli\rCopy\rThen initialize and build your project:\nnetlify login netlify init netlify build\rCopy\rAnd when you\u0026rsquo;re ready for production:\nnetlify deploy --prod\rCopy\rüß™ Try the live demo here: üëâ https://wondrous-conkies-5d34fe.netlify.app/\r‚úçÔ∏è Creating MD-Editor-PRO The second project I built is MD-Editor-PRO\r, a modern markdown editor powered by React, TypeScript, and Tailwind CSS. It‚Äôs designed for a clean, distraction-free writing experience with all the markdown features you‚Äôd expect.\nüß± Building It Just like NoteFlow, run it locally with:\ngit clone https://github.com/seehiong/md-editor-pro cd md-editor-pro npm install npm run dev\rCopy\rOpen your browser to http://localhost:5173 and start writing!\nüö¢ Deploying MD-Editor-PRO Deployment is equally simple with Netlify:\nüß™ Check out the live demo: üëâ https://endearing-frangollo-73728e.netlify.app/\rüí¨ Let‚Äôs Vibe Code Together Have you tried vibe coding and deployed anything fun to the web recently? I‚Äôd love to see what you‚Äôve built‚Äîor hear what ideas you‚Äôre brewing!\nDrop me a comment below! Let\u0026rsquo;s share and inspire.\n","date":"2025-08-02","permalink":"https://seehiong.github.io/posts/2025/08/noteflow-prototyping-with-ai-speed/","summary":"During my final days with bolt.new, I built two exciting prototypes‚ÄîNoteFlow and MD-Editor-PRO‚Äîusing the remaining tokens. Inspired by my feature-rich ‚Ä¶","tags":["NoteFlow","bolt.new","netlify","MD-Editor-PRO","Vibe Coding"],"title":"NoteFlow: Prototyping with AI Speed"},{"content":"As part of my journey through the Convolutional Neural Networks\rcourse (part of the Deep Learning Specialization), I‚Äôve been implementing foundational concepts such as skip connections in deep ResNet architectures using Keras. One of the labs also introduced transfer learning to create an Alpaca/Not Alpaca classifier using a pre-trained CNN.\nTo further reinforce what I‚Äôve learned, I decided to apply transfer learning using MobileNetV2 for a binary classification task‚ÄîCats vs. Dogs. This post walks through the full process: from data preprocessing to model training, evaluation, and visualization.\nPrerequiste Setup For this experiment, I set up a new tensorflow-full notebook environment in Kubeflow, running in my homelab. The notebook is configured with:\n2 CPUs 24Gi memory This environment comes pre-installed with TensorFlow and commonly used ML libraries, making it an ideal starting point for deep learning workflows. To load datasets from TensorFlow Datasets (TFDS), simply install:\npip install tensorflow_datasets\rCopy\rBy leveraging Kubeflow Notebooks, I can easily scale experiments, persist environments, and integrate with other ML components (like Pipelines or Katib) when needed‚Äîall within a self-hosted setup.\nIntroduction Keras Applications\roffer a collection of pre-trained models, such as MobileNetV2 and ResNet50, trained on the ImageNet dataset. These are highly useful in transfer learning, where I reuse parts of a pre-trained model for a new but related task.\nAccording to the Keras Transfer Learning Guide\r, the typical workflow includes:\nLoad a pre-trained model. Freeze its layers to preserve learned features. Add new trainable layers tailored for the new task. Train only the new layers with your dataset. In this example, I\u0026rsquo;ll classify images as either cats or dogs using this approach with MobileNetV2.\nSetup \u0026amp; Imports import tensorflow as tf import tensorflow_datasets as tfds from tensorflow.keras.applications import MobileNetV2 from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau import matplotlib.pyplot as plt import numpy as np # For reproducibility tf.random.set_seed(42) np.random.seed(42)\rCopy\rData Loading \u0026amp; Preprocessing I use the cats_vs_dogs dataset from TensorFlow Datasets (TFDS), splitting it into 80% training and 20% validation. Images are resized to 224x224 and normalized. To improve generalization, I apply basic augmentations to training data such as flipping, brightness, contrast adjustments, and rotation.\ndef preprocess_data(image, label): image = tf.image.resize(image, [224, 224]) image = tf.cast(image, tf.float32) / 255.0 return image, label def augment_data(image, label): image = tf.image.random_flip_left_right(image) image = tf.image.random_brightness(image, max_delta=0.2) image = tf.image.random_contrast(image, lower=0.8, upper=1.2) image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32)) return image, label def load_cats_vs_dogs_dataset(batch_size=32): print(\u0026#34;Downloading cats_vs_dogs dataset...\u0026#34;) (ds_train, ds_test), ds_info = tfds.load( \u0026#39;cats_vs_dogs\u0026#39;, split=[\u0026#39;train[:80%]\u0026#39;, \u0026#39;train[80%:]\u0026#39;], as_supervised=True, with_info=True, ) print(f\u0026#34;Training samples: {ds_info.splits[\u0026#39;train\u0026#39;].num_examples * 0.8:.0f}\u0026#34;) print(f\u0026#34;Validation samples: {ds_info.splits[\u0026#39;train\u0026#39;].num_examples * 0.2:.0f}\u0026#34;) ds_train = ds_train.map(preprocess_data).map(augment_data).cache().shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE) ds_test = ds_test.map(preprocess_data).cache().batch(batch_size).prefetch(tf.data.AUTOTUNE) return ds_train, ds_test, ds_info\rCopy\rModel Architecture I use MobileNetV2 as the base model, excluding its top classification layer and freezing its weights. On top of it, I add:\nGlobal Average Pooling Dense + ReLU Dropout for regularization A final Dense layer with sigmoid for binary output def create_cats_dogs_classifier(input_shape=(224, 224, 3)): base_model = MobileNetV2(weights=\u0026#39;imagenet\u0026#39;, include_top=False, input_shape=input_shape) base_model.trainable = False inputs = tf.keras.Input(shape=input_shape) x = base_model(inputs, training=False) x = GlobalAveragePooling2D()(x) x = Dropout(0.2)(x) x = Dense(64, activation=\u0026#39;relu\u0026#39;)(x) x = Dropout(0.1)(x) outputs = Dense(1, activation=\u0026#39;sigmoid\u0026#39;)(x) model = Model(inputs, outputs) return model, base_model\rCopy\rTraining Setup I train only the top classification layers, leaving the pre-trained base frozen. I use binary_crossentropy as the loss function and include callbacks to manage training:\nModelCheckpoint to save the best-performing model. EarlyStopping to halt training when no improvement is observed. ReduceLROnPlateau to adapt learning rate. def train_cats_dogs_classifier_transfer_only(epochs=8, batch_size=64, patience=2): ‚Ä¶","date":"2025-07-20","permalink":"https://seehiong.github.io/posts/2025/07/transfer-learning-with-mobilenetv2/","summary":"In this post, I explore transfer learning using MobileNetV2 to build a binary image classifier for cats and dogs. Running on a tensorflow-full ‚Ä¶","tags":["MobileNetV2","Transfer Learning","Convolutional Neural Networks","CNN","Keras","TensorFlow"],"title":"Transfer Learning with MobileNetV2"},{"content":"In this post, I‚Äôll share my journey exploring n8n\r‚Äîa flexible, open-source workflow automation tool with built-in AI integrations. I‚Äôll walk through how I set it up locally on my Windows machine and later deployed it to my homelab environment.\nPrerequiste: Cloning and Running n8n Locally The Self-hosted AI Starter Kit\ris an open-source Docker Compose template designed to quickly spin up a comprehensive local AI and low-code automation environment.\nClone the Repository git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git cd self-hosted-ai-starter-kit.git\rCopy\rConfigure Local Settings Start by copying the sample environment file:\ncp .env.example .env\rCopy\rSince I‚Äôm working on a Windows setup, and the Docker images are primarily Linux-based, I ran everything through WSL (Windows Subsystem for Linux). I also modified the following environment variables to suit my setup:\nN8N_RUNNERS_ENABLED=true N8N_LISTEN_ADDRESS=0.0.0.0\rCopy\rStart the Services Spin up the necessary containers using:\ndocker compose --profile gpu-nvidia up\rCopy\rOnce everything is up and running, visit http://localhost:5678 to access the n8n dashboard.\nDemo Workflow To get a quick feel of how things work, click on the Demo workflow from the Overview tab, or directly navigate to:\nhttp://localhost:5678/workflow/srOnR8PAY3u4RSwb\rCopy\rMake sure to configure the required fallback model. In my case, I used the OpenRouter Chat Model.\nLearn by Examples One of the best ways to explore n8n is by diving into real workflows. The official Workflow Automation Templates\rare a great starting point.\nFirst Example: API Fundamentals Let‚Äôs start with the tutorial: Learn API Fundamentals with an Interactive Hands-On Tutorial\rOpen a Node Select a node, press Enter or double-click to open its configuration panel.\nAccess a Simple Value Use the expression:\n{{ $(\u0026#39;Source Data\u0026#39;).item.json.name }}\rCopy\rUsing n8n Selectors Handy helpers like .first(), .last(), .all():\nAccessing Array Elements Example:\n{{ $(\u0026#39;Source Data\u0026#39;).last().json.skills[1] }}\rCopy\rWorking with Nested Data Example:\n{{ $(\u0026#39;Source Data\u0026#39;).last().json.contact.email }}\rCopy\rAccessing Data in Object Arrays Example:\n{{ $(\u0026#39;Source Data\u0026#39;).last().json.projects[0].status }}\rCopy\rUsing JavaScript Functions Example:\n{{ $(\u0026#39;Source Data\u0026#39;).last().json.name.toUpperCase() }}\rCopy\rInspecting Objects Useful when exploring dynamic JSON:\nObject.keys($(\u0026#39;Source Data\u0026#39;).item.json)\rCopy\rStringify Object Data Example:\nJSON.stringify($(\u0026#39;Source Data\u0026#39;).item.json)\rCopy\rWorking with Multiple Items Combine $items with arrow functions for batch processing:\nOptional ‚Äì Running n8n in My Homelab For those who prefer deploying n8n on a Kubernetes cluster, I‚Äôve included a streamlined setup using Kompose\rto convert the Docker Compose setup into Kubernetes manifests. Below is my working configuration deployed in my homelab.\nPostgreSQL Deployment Use the following commands to deploy the PostgreSQL database:\nkubectl apply -f postgres-storage-persistentvolumeclaim.yaml kubectl apply -f postgres-deployment.yaml kubectl apply -f postgres-service.yaml\rCopy\rPostgres K8s files Persistent Volume Claim # postgres-storage-persistentvolumeclaim.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: labels: io.kompose.service: postgres-storage name: postgres-storage namespace: n8n spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi\rCopy\rDeployment # postgres-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: io.kompose.service: postgres name: postgres namespace: n8n spec: replicas: 1 selector: matchLabels: io.kompose.service: postgres strategy: type: Recreate template: metadata: labels: io.kompose.service: postgres spec: containers: - env: - name: POSTGRES_DB value: n8n - name: POSTGRES_PASSWORD value: postgres - name: POSTGRES_USER value: postgres image: postgres:16-alpine livenessProbe: exec: command: - pg_isready -h localhost -U postgres -d n8n failureThreshold: 10 periodSeconds: 5 timeoutSeconds: 5 name: postgres volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-storage hostname: postgres restartPolicy: Always volumes: - name: postgres-storage persistentVolumeClaim: claimName: postgres-storage\rCopy\rService # postgres-service.yaml apiVersion: v1 kind: Service metadata: labels: io.kompose.service: postgres name: postgres namespace: n8n spec: ports: - name: \u0026#34;5432\u0026#34; port: 5432 targetPort: 5432 selector: io.kompose.service: postgres type: ClusterIP\rCopy\rn8n Deployment Use the following commands to deploy n8n:\nkubectl apply -f n8n-demo-persistentVolumeClaim.yaml kubectl apply -f n8n-storage-persistentVolumeClaim.yaml kubectl apply -f n8n-claim2-persistentVolumeClaim.yaml kubectl apply -f n8n-job.yaml kubectl apply -f n8n-configmap.yaml kubectl apply -f n8n-deployment.yaml kubectl apply -f n8n-service.yaml\rCopy\rn8n K8s files Persistent Volume Claims Multiple PVCs are used to separate storage for runtime data, demo workflows, and ‚Ä¶","date":"2025-07-13","permalink":"https://seehiong.github.io/posts/2025/07/automating-workflows-with-n8n/","summary":"Explore how to set up and run n8n, a powerful open-source workflow automation tool, locally using Docker and WSL on Windows. This post also walks ‚Ä¶","tags":["n8n","nodemation","Workflow","Automation","Kompose","Homelab"],"title":"Automating Workflows with n8n"},{"content":"It was an incredibly busy June, as I had the chance to participate in the World\u0026rsquo;s Largest Hackathon\r. Through this, I gained first-hand experience with Vibe Coding\r‚Äîa term coined by Andrej Karpathy\r‚Äîwhich refers to coding with the support of LLMs in an interactive and exploratory manner.\nI started the project using Bolt.new\r, set up my backend quickly with Supabase\r, and deployed the frontend seamlessly using Netlify\r. In this post, I‚Äôll share how I took the project further by running bolt.diy\rlocally‚Äîexperimenting with different models through OpenRouter and deploying my own LLM-assisted text utility tool, TextForge.\nOne-Shot Challenge Entry: TextForge For the hackathon‚Äôs One-Shot Challenge, I submitted TextForge\r‚Äîa multi-functional toolkit tailored for text and data manipulation. It‚Äôs a developer-focused playground for quickly cleaning, formatting, converting, and analyzing text.\nAfter iterating on various prompts and refining the features, I settled on a version that delivered just the right blend of utility and user experience. You can try the hosted TextForge demo here\r.\nPrerequiste: Cloning and Running bolt.diy Locally bolt.diy is the open-source counterpart to bolt.new, giving you the freedom to self-host and configure your preferred LLMs for prompt generation and code synthesis.\nClone the Repository git clone https://github.com/stackblitz-labs/bolt.diy.git cd bolt.diy\rCopy\rInstall Dependencies and Run npm install -g pnpm pnpm install pnpm run dev\rCopy\rYou should see the startup CLI confirming the dev server is active:\nThen, navigate to your local instance, typically at http://localhost:5173:\nTo interface with different models, I configured bolt.diy to use OpenRouter\r‚Äîa flexible gateway that provides API access to a variety of leading LLMs including Anthropic Claude, GPT variants, Mistral, and more.\nMy One-Shot Prompt for TextForge Here\u0026rsquo;s the prompt that I crafted to bootstrap the generation of TextForge. It serves as a specification for a multi-tool, developer-centric utility with robust UX/UI features and thoughtful tooling:\nOne-shot prompt TextForge: The Ultimate Text \u0026amp; Data Manipulation Toolkit A responsive, multi-tool web application designed for developers and writers to clean, format, convert, and generate text and data snippets efficiently. 1. Text Cleanup \u0026amp; Analysis Features: Whitespace \u0026amp; Lines: Remove extra spaces, remove all line breaks, convert to a single line. Text Analysis: Live word count, character count, and line count. Clear Input: A quick button to clear the text area. Implementation: Generated javascript // Example React state and handler const [input, setInput] = useState(\u0026#34;\u0026#34;); const [output, setOutput] = useState(\u0026#34;\u0026#34;); // Analysis is derived from input state const wordCount = input.trim().split(/\\s+/).filter(Boolean).length; const charCount = input.length; const handleCleanup = (action) =\u0026gt; { let result = \u0026#34;\u0026#34;; switch (action) { case \u0026#34;removeExtraSpaces\u0026#34;: result = input.replace(/\\s+/g, \u0026#39; \u0026#39;).trim(); break; case \u0026#34;removeLineBreaks\u0026#34;: result = input.replace(/(\\r\\n|\\n|\\r)/gm, \u0026#39; \u0026#39;); break; case \u0026#34;clear\u0026#34;: setInput(\u0026#34;\u0026#34;); result = \u0026#34;\u0026#34;; break; default: result = input; } setOutput(result); }; Use code with caution. JavaScript 2. Case Converter Features: Standard Cases: UPPERCASE, lowercase, Sentence case. Programming Cases: camelCase, PascalCase, snake_case, kebab-case. Instant conversion on button click. Implementation: Use a combination of native string methods and regular expressions for complex cases. Generated javascript // Example conversion logic const toCamelCase = (str) =\u0026gt; { return str.toLowerCase().replace(/([-_ ][a-z])/g, g =\u0026gt; g.toUpperCase().replace(/[-_ ]/g, \u0026#39;\u0026#39;)); }; const toPascalCase = (str) =\u0026gt; { const camel = toCamelCase(str); return camel.charAt(0).toUpperCase() + camel.slice(1); }; const handleCaseChange = (caseType) =\u0026gt; { let result = \u0026#34;\u0026#34;; switch (caseType) { case \u0026#34;uppercase\u0026#34;: result = input.toUpperCase(); break; case \u0026#34;camelCase\u0026#34;: result = toCamelCase(input); break; // ... other cases } setOutput(result); }; Use code with caution. JavaScript 3. Code \u0026amp; Data Formatter Features: JSON Tools: Beautify (pretty-print) and Minify JSON. Data Conversion: Convert delimited Key-Value pairs (e.g., name=John\\nage=30) into a JSON object. CSS Minifier: Strip comments and whitespace from CSS code. Implementation: Use native browser APIs and a lightweight library for CSS. Generated javascript // JSON Formatting const formatJSON = (beautify = true) =\u0026gt; { try { const jsonObj = JSON.parse(input); const space = beautify ? 2 : 0; setOutput(JSON.stringify(jsonObj, null, space)); } catch (error) { setOutput(\u0026#34;Error: Invalid JSON\u0026#34;); } }; // Key-Value to JSON const convertKvToJson = () =\u0026gt; { const lines = input.split(\u0026#39;\\n\u0026#39;).filter(line =\u0026gt; line.includes(\u0026#39;=\u0026#39;)); const obj = lines.reduce((acc, line) =\u0026gt; { const [key, ‚Ä¶","date":"2025-07-06","permalink":"https://seehiong.github.io/posts/2025/07/running-bolt.diy-with-openrouter/","summary":"This post explores running the open-source bolt.diy locally and integrating it with OpenRouter to experiment with various LLMs. I document the ‚Ä¶","tags":["Bolt.new","Supabase","Netlify","Vibe Coding","bolt.diy","OpenRouter"],"title":"Running Bolt.diy with OpenRouter"},{"content":"In this post, we‚Äôll deploy the HDB Price Predictor model to Oracle Cloud‚Äôs Kubernetes Engine (OKE) using KServe. The setup includes provisioning an OKE cluster, configuring Istio for networking, and serving the model using KServe.\nPrerequistes First, install the required tools:\nchoco install kubernetes-helm choco install wget\rCopy\rStep 1: Create Oracle Kubernetes Engine (OKE) Start by creating a new compartment.\nNavigate to Identity \u0026amp; Security \u0026gt; Compartments and click Create Compartment. Next, go to Developer Services \u0026gt; Kubernetes Clusters (OKE) and click Create Cluster. For this demo, I created a 1-node basic cluster and waited until the node status changed to Ready and Active:\nStep 2: OCI Command Line Interface (CLI) Follow the official OCI CLI guide\rto install the CLI. I extracted the oci-cli-3.55.0.zip\rto F:\\oci-cli. The API key setup is covered in the Oracle documentation, so I‚Äôll skip it here.\nSet up the environment:\nF: python -m venv oracle-cli oracle-cli\\Scripts\\activate cd \\oci-cli pip install oci_cli-3.55.0-py3-none-any.whl\rCopy\rConfigure kubectl to access the cluster:\noci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.ap-singapore-2.aaaaaaaaayk5sgnphyr7rxdnwpdwfji25zcoodj52zeqlk2r3cpfpp64csya --file %USERPROFILE%/.kube/config --region ap-singapore-2 --token-version 2.0.0 --kube-endpoint PUBLIC_ENDPOINT set KUBECONFIG=%USERPROFILE%/.kube/config kubectl get no # (oracle-cli) F:\\oci-cli\u0026gt;kubectl get no # NAME STATUS ROLES AGE VERSION # 10.0.10.184 Ready node 40s v1.33.0\rCopy\rStep 3: Install KServe and Dependencies 3.1 Install Cert Manager Follow the KServe 0.15 setup guide\r:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml kubectl get deployments -n cert-manager\rCopy\r3.2 Install Istio via Helm Referencing Istio\u0026rsquo;s Helm installation guide\r:\nhelm repo add istio https://istio-release.storage.googleapis.com/charts helm repo update helm install istio-base istio/base -n istio-system --set defaultRevision=default --create-namespace helm install istiod istio/istiod -n istio-system --wait helm ls -n istio-system helm install istio-ingressgateway istio/gateway -n istio-system --wait helm status istio-ingressgateway -n istio-system\rCopy\rApply the IngressClass:\n# istio-ingressclass.yaml apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: istio spec: controller: istio.io/ingress-controller\rCopy\rkubectl apply -f istio-ingressclass.yaml\rCopy\r3.3 Install KServe helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd --version v0.15.0 -n kserve --create-namespace helm install kserve oci://ghcr.io/kserve/charts/kserve --version v0.15.0 --set kserve.controller.deploymentMode=RawDeployment --set kserve.controller.gateway.ingressGateway.className=istio -n kserve --create-namespace\rCopy\rStep 4: Deploy Your First InferenceService We\u0026rsquo;ll use the KServe getting started example\r:\n# sklearn-iris.yaml apiVersion: \u0026#34;serving.kserve.io/v1beta1\u0026#34; kind: \u0026#34;InferenceService\u0026#34; metadata: name: \u0026#34;sklearn-iris\u0026#34; namespace: kserve-test spec: predictor: model: modelFormat: name: sklearn storageUri: \u0026#34;gs://kfserving-examples/models/sklearn/1.0/model\u0026#34; Copy\rkubectl create namespace kserve-test kubectl apply -n kserve-test -f sklearn-iris.yaml kubectl get inferenceservices sklearn-iris -n kserve-test\rCopy\rCheck the external IP of the Istio ingress gateway:\nkubectl get svc istio-ingressgateway -n istio-system\rCopy\rThen run a prediction request:\ncurl -v -H \u0026#34;Host: sklearn-iris-kserve-test.example.com\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; \u0026#34;http://217.142.185.27:80/v1/models/sklearn-iris:predict\u0026#34; -d @iris-input.json\rCopy\rTo automate, extract necessary variables:\n# INGRESS_HOST kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; # INGRESS_PORT kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].port}\u0026#39; # SERVICE_HOSTNAME kubectl get inferenceservice sklearn-iris -n kserve-test -o jsonpath=\u0026#39;{.status.url}\u0026#39; | cut -d \u0026#34;/\u0026#34; -f 3\rCopy\rStep 5: Deploy HDB Price Predictor With the model files downloaded from my homelab MinIO, as shared in my previous post\r, we now upload the price-predictor-model.bst to an Oracle Cloud Object Storage bucket. Once uploaded, define an InferenceService to deploy the XGBoost model:\n# price-predictor-xgboost.yaml apiVersion: \u0026#34;serving.kserve.io/v1beta1\u0026#34; kind: \u0026#34;InferenceService\u0026#34; metadata: name: \u0026#34;hdb-resale-xgb\u0026#34; namespace: kserve-test spec: predictor: model: modelFormat: name: xgboost runtime: \u0026#34;kserve-xgbserver\u0026#34; protocolVersion: v2 storageUri: \u0026#34;https://objectstorage...oraclecloud.com/.../price-predictor-model.bst\u0026#34; Copy\rApply the manifest and check the deployment status:\nkubectl apply -f price-predictor-xgboost.yaml kubectl get inferenceservices hdb-resale-xgb -n kserve-test\rCopy\rUse the ‚Ä¶","date":"2025-05-12","permalink":"https://seehiong.github.io/posts/2025/05/deploying-kserve-on-oke/","summary":"This post demonstrated how to deploy an XGBoost model using KServe on Oracle Kubernetes Engine (OKE). Starting from model upload to Object Storage, we ‚Ä¶","tags":["OKE","KServe","Helm","Istio","Streamlit"],"title":"Deploying KServe on OKE"},{"content":"As I continue the Unsupervised Learning, Recommenders, and Reinforcement Learning course\r‚Äî which combines theory with hands-on labs ‚Äî I‚Äôm applying what I learn to solidify my understanding. This post walks through building a movie recommender system using the MovieLens dataset. From data loading and feature engineering to generating embeddings and performing similarity search with pgvector in PostgreSQL, the goal is to bring the course material to life through a practical, end-to-end example. The dataset is based on The MovieLens Datasets: History and Context\r.\nEnvironment Setup PostgreSQL Installation I installed the Bitnami PostgreSQL Helm chart in my homelab environment:\nhelm install my-release oci://registry-1.docker.io/bitnamicharts/postgresql # To get password export POSTGRES_PASSWORD=$(kubectl get secret --namespace postgres postgres-postgresql -o jsonpath=\u0026#34;{.data.postgres-password}\u0026#34; | base64 -d) echo $POSTGRES_PASSWORD\rCopy\rDatabase Schema Preparation Here‚Äôs the schema I created for storing movie-related data and embeddings. I also enabled the vector extension for future use in vector-based operations.\n-- Create schema and enable vector extension CREATE SCHEMA IF NOT EXISTS movie_recommender; CREATE EXTENSION IF NOT EXISTS vector; -- Drop tables if they exist (clean start) DROP TABLE IF EXISTS movie_recommender.movie_embeddings; DROP TABLE IF EXISTS movie_recommender.ratings; DROP TABLE IF EXISTS movie_recommender.links; DROP TABLE IF EXISTS movie_recommender.tags; DROP TABLE IF EXISTS movie_recommender.movies; -- Define core tables CREATE TABLE movie_recommender.movies ( movieId INT PRIMARY KEY, title VARCHAR(255), genres VARCHAR(255) ); CREATE TABLE movie_recommender.ratings ( userId INT, movieId INT, rating FLOAT, timestamp BIGINT, PRIMARY KEY (userId, movieId), FOREIGN KEY (movieId) REFERENCES movie_recommender.movies(movieId) ); CREATE TABLE movie_recommender.links ( movieId INT PRIMARY KEY, imdbId VARCHAR(20), tmdbId INT, FOREIGN KEY (movieId) REFERENCES movie_recommender.movies(movieId) ); CREATE TABLE movie_recommender.tags ( userId INT, movieId INT, tag VARCHAR(255), timestamp BIGINT, FOREIGN KEY (movieId) REFERENCES movie_recommender.movies(movieId) ); -- Embeddings table CREATE TABLE movie_recommender.movie_embeddings ( movieid INT NOT NULL, embedding public.vector NULL, CONSTRAINT movie_embeddings_pkey PRIMARY KEY (movieid), FOREIGN KEY (movieid) REFERENCES movie_recommender.movies(movieId) );\rCopy\rLoad Data from CSV 1. Database Configuration in Jupyter Note: For production, consider using environment variables or a secrets manager for credentials.\n# File paths ratings_file = \u0026#39;ratings.csv\u0026#39; movies_file = \u0026#39;movies.csv\u0026#39; links_file = \u0026#39;links.csv\u0026#39; tags_file = \u0026#39;tags.csv\u0026#39; # Database credentials db_params = { \u0026#39;database\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;user\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;W8oMlEyhq9\u0026#39;, \u0026#39;host\u0026#39;: \u0026#39;postgres-postgresql.postgres\u0026#39;, \u0026#39;port\u0026#39;: \u0026#39;5432\u0026#39; } target_schema = \u0026#34;movie_recommender\u0026#34;\rCopy\r2. Writing CSV to PostgreSQL import psycopg2 # Function to bulk copy data def copy_data_to_postgres(csv_path, schema_name, table_name, columns, connection): qualified_table_name = f\u0026#34;{schema_name}.{table_name}\u0026#34; try: with open(csv_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: next(f) cursor = connection.cursor() copy_sql = f\u0026#34;COPY {qualified_table_name} ({\u0026#39;,\u0026#39;.join(columns)}) FROM STDIN WITH (FORMAT CSV, DELIMITER \u0026#39;,\u0026#39;, HEADER FALSE)\u0026#34; print(f\u0026#34;Attempting to copy data to {qualified_table_name}...\u0026#34;) cursor.copy_expert(sql=copy_sql, file=f) connection.commit() print(f\u0026#34;Successfully copied data from {csv_path} to {qualified_table_name}\u0026#34;) except (Exception, psycopg2.DatabaseError) as error: print(f\u0026#34;Error copying data to {qualified_table_name}: {error}\u0026#34;) connection.rollback() finally: if cursor: cursor.close() # Establish DB connection conn = None try: print(f\u0026#34;Attempting to connect to PostgreSQL database \u0026#39;{db_params[\u0026#39;database\u0026#39;]}\u0026#39; on {db_params[\u0026#39;host\u0026#39;]}...\u0026#34;) conn = psycopg2.connect(**db_params) print(\u0026#34;PostgreSQL connection successful.\u0026#34;) # Copy datasets print(f\u0026#34;\\n-- Copying data into tables in schema \u0026#39;{target_schema}\u0026#39; --\u0026#34;) movies_cols = [\u0026#39;movieId\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;genres\u0026#39;] links_cols = [\u0026#39;movieId\u0026#39;, \u0026#39;imdbId\u0026#39;, \u0026#39;tmdbId\u0026#39;] ratings_cols = [\u0026#39;userId\u0026#39;, \u0026#39;movieId\u0026#39;, \u0026#39;rating\u0026#39;, \u0026#39;timestamp\u0026#39;] tags_cols = [\u0026#39;userId\u0026#39;, \u0026#39;movieId\u0026#39;, \u0026#39;tag\u0026#39;, \u0026#39;timestamp\u0026#39;] copy_data_to_postgres(movies_file, target_schema, \u0026#39;movies\u0026#39;, movies_cols, conn) copy_data_to_postgres(links_file, target_schema, \u0026#39;links\u0026#39;, links_cols, conn) copy_data_to_postgres(ratings_file, target_schema, \u0026#39;ratings\u0026#39;, ratings_cols, conn) copy_data_to_postgres(tags_file, target_schema, \u0026#39;tags\u0026#39;, tags_cols, conn) except Exception as e: ‚Ä¶","date":"2025-05-02","permalink":"https://seehiong.github.io/posts/2025/05/building-a-recommender-system/","summary":"In this post, we walk through the process of building a movie recommender system using deep learning embeddings and PostgreSQL with pgvector. ‚Ä¶","tags":["Recommenders","MovieLens","Applied Machine Learning","Artificial Neural Networks","PostgreSQL"],"title":"Building a Recommender System"},{"content":"Building on my previous post\r, this article outlines how to move the selected features into a Kubeflow Pipeline (KFP) for a full model training and deployment workflow. The pipeline fetches input data from S3-compatible MinIO, trains a model using XGBoost with optimized feature selection, and deploys it via KServe. At the end of this journey, a Streamlit app enables users to make resale price predictions by entering flat attributes.\nDeploying the InferenceService Following the official KServe documentation\r, let‚Äôs begin by setting up the necessary components to enable model inference from an S3 backend.\n1. Create Service Account Define a service account that references the secret used to access MinIO.\nkubectl apply -f s3-secret.yaml\rCopy\r# s3-secret.yaml apiVersion: v1 kind: ServiceAccount metadata: name: sa namespace: kubeflow-user-example-com secrets: - name: minio-creds\rCopy\r2. Create S3 Secret Configure your secret to include the MinIO credentials and required annotations for KServe to read from the S3 bucket.\nkubectl apply -f secret-minio.yaml\rCopy\r# secret-minio.yaml apiVersion: v1 kind: Secret metadata: name: minio-creds namespace: kubeflow-user-example-com annotations: serving.kserve.io/s3-endpoint: \u0026#34;minio-service.kubeflow:9000\u0026#34; serving.kserve.io/s3-usehttps: \u0026#34;0\u0026#34; serving.kserve.io/s3-useanoncredential: \u0026#34;false\u0026#34; type: Opaque stringData: AWS_ACCESS_KEY_ID: \u0026#34;minio\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;minio123\u0026#34;\rCopy\r3. Deploy the InferenceService Using the XGBoost serving guide\r, deploy the model. This step will also be handled automatically within the pipeline‚Äôs Step 6.\nNote: Ensure the storageUri points to the correct S3 path that contains the XGBoost .bst model file.\nkubectl apply -f kserve-inference.yaml --server-side=true # Sample S3 minio url # storageUri: \u0026#34;s3://mlpipeline/v2/artifacts/hdb-xgboost-retraining-pipeline-multi-step/053453f4-8e52-4e4a-b2b1-1f8658b92b54/train-model/15ee1507-42e0-4a05-b06f-bd0fa12a7a42/output_model\u0026#34;\rCopy\r# kserve-inference.yaml apiVersion: \u0026#34;serving.kserve.io/v1beta1\u0026#34; kind: \u0026#34;InferenceService\u0026#34; metadata: name: hdb-resale-xgb namespace: kubeflow-user-example-com annotations: autoscaling.knative.dev/maxScale: \u0026#39;1\u0026#39; autoscaling.knative.dev/minScale: \u0026#39;0\u0026#39; spec: predictor: serviceAccountName: sa model: modelFormat: name: xgboost protocolVersion: v2 runtime: kserve-xgbserver storageUri: \u0026#34;s3://mlpipeline/hdb-resale-data/xgb-model-test/model.bst\u0026#34;\rCopy\r4. Grant Istio Access To allow prediction requests, create an Istio AuthorizationPolicy that explicitly allows POST and GET operations on the deployed model endpoint.\nkubectl apply -f allow-hdb-xgb-policy.yaml\rCopy\r# allow-hdb-xgb-policy.yaml apiVersion: security.istio.io/v1 kind: AuthorizationPolicy metadata: name: allow-hdb-resale-xgb-access namespace: kubeflow-user-example-com spec: selector: matchLabels: serving.kserve.io/inferenceservice: hdb-resale-xgb action: ALLOW rules: - to: - operation: methods: [\u0026#34;POST\u0026#34;,\u0026#34;GET\u0026#34;]\rCopy\r5. Run a Prediction Once the pipeline completes successfully, prepare an inference input JSON file and use curl to invoke the model endpoint. The output will contain the predicted resale prices.\nimport json payload = { \u0026#34;inputs\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;input-0\u0026#34;, \u0026#34;shape\u0026#34;: [5, 1], \u0026#34;datatype\u0026#34;: \u0026#34;FP32\u0026#34;, \u0026#34;data\u0026#34;: [ 0.4311926605504587, 0.2844036697247706, 0.27522935779816515, 0.15137614678899083, 0.4357798165137615 ] }] } with open(\u0026#34;inference_input.json\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(payload, f, indent=2)\rCopy\rSERVICE_URL=\u0026#34;http://hdb-resale-xgb.kubeflow-user-example-com.svc.cluster.local\u0026#34; MODEL_NAME=\u0026#34;hdb-resale-xgb\u0026#34; # Construct the V2 inference endpoint path INFERENCE_ENDPOINT=\u0026#34;${SERVICE_URL}/v2/models/${MODEL_NAME}/infer\u0026#34; echo \u0026#34;Sending request to: ${INFERENCE_ENDPOINT}\u0026#34; curl -v -H \u0026#34;Content-Type: application/json\u0026#34; \u0026#34;${INFERENCE_ENDPOINT}\u0026#34; -d @inference_input.json # * Trying 10.98.223.247:80... # * Connected to hdb-resale-xgb.kubeflow-user-example-com.svc.cluster.local (10.98.223.247) port 80 (#0) # \u0026gt; POST /v2/models/hdb-resale-xgb/infer HTTP/1.1 # \u0026gt; Host: hdb-resale-xgb.kubeflow-user-example-com.svc.cluster.local # ... # \u0026lt; # * Connection #0 to host hdb-resale-xgb.kubeflow-user-example-com.svc.cluster.local left intact # {\u0026#34;model_name\u0026#34;:\u0026#34;hdb-resale-xgb\u0026#34;,\u0026#34;model_version\u0026#34;:null,\u0026#34;id\u0026#34;:\u0026#34;6d902b7b-a172-4d14-84dd-cc8a004f1200\u0026#34;,\u0026#34;parameters\u0026#34;:null,\u0026#34;outputs\u0026#34;:[{ \u0026#34;name\u0026#34;:\u0026#34;output-0\u0026#34;,\u0026#34;shape\u0026#34;:[5],\u0026#34;datatype\u0026#34;:\u0026#34;FP32\u0026#34;,\u0026#34;parameters\u0026#34;:null,\u0026#34;data\u0026#34;:[600095.875,579441.5625,479052.34375,336679.75,586596\rCopy\r6. Download Scaler File For accurate predictions, use the same scaler from the training process. Port-forward your MinIO service and download the scaler from the designated path after the pipeline run.\nMultistep pipeline This section outlines the ‚Ä¶","date":"2025-04-21","permalink":"https://seehiong.github.io/posts/2025/04/from-model-to-hdb-app/","summary":"In this post, I demonstrated how to deploy a Jupyter notebook using the jupyter-tensorflow-full image in Kubeflow, develop an HDB resale price ‚Ä¶","tags":["Streamlit","XGBoost","MLflow","Kubeflow","MinIO","KFP","KServe"],"title":"From Model to HDB App"},{"content":"Predicting HDB resale prices is a fascinating application of machine learning, blending feature engineering, neural networks, and advanced models like XGBoost. This post walks through the end-to-end process‚Äîfrom data loading and cleaning, to feature engineering, and preparing the dataset for predictive modeling. Let‚Äôs dive in using Singapore‚Äôs structured HDB Resale Flat Prices dataset from data.gov.sg\r.\nData Loading and Exploration To begin, we load the HDB resale flat prices dataset and explore the first few rows.\nimport pandas as pd file_path = \u0026#39;Resale flat prices based on registration date from Jan-2017 onwards.csv\u0026#39; try: df = pd.read_csv(file_path, skipinitialspace=True) print(\u0026#34;Successfully loaded DataFrame. First 5 rows:\u0026#34;) print(df.head()) except FileNotFoundError: print(f\u0026#34;Error: The file \u0026#39;{file_path}\u0026#39; was not found.\u0026#34;) exit() except Exception as e: print(f\u0026#34;An error occurred while reading the CSV file: {e}\u0026#34;) exit()\rCopy\rFeature Engineering \u0026amp; Preprocessing To get meaningful insights from the dataset, we‚Äôll process some of the raw columns and convert them into usable numerical features.\n1. Extract remaining_lease_years HDB leases are represented in strings like \u0026ldquo;92 years 3 months\u0026rdquo;. We‚Äôll convert that into a single numerical column.\ndef extract_years(lease_str): if pd.isna(lease_str): return np.nan parts = lease_str.split() years = 0 for i, part in enumerate(parts): if part.isdigit() and i+1 \u0026lt; len(parts) and parts[i+1].startswith(\u0026#39;year\u0026#39;): years += int(part) elif part.isdigit() and i+1 \u0026lt; len(parts) and parts[i+1].startswith(\u0026#39;month\u0026#39;): years += int(part)/12 return years print(f\u0026#34;\\nMin original remaining lease: {df[\u0026#39;remaining_lease\u0026#39;].min()}\u0026#34;) print(f\u0026#34;Max original remaining lease: {df[\u0026#39;remaining_lease\u0026#39;].max()}\u0026#34;) df[\u0026#39;remaining_lease_years\u0026#39;] = df[\u0026#39;remaining_lease\u0026#39;].apply(extract_years) print(f\u0026#34;\\nMin remaining lease years: {df[\u0026#39;remaining_lease_years\u0026#39;].min()}\u0026#34;) print(f\u0026#34;Max remaining lease years: {df[\u0026#39;remaining_lease_years\u0026#39;].max()}\u0026#34;) print(f\u0026#34;NaNs in remaining lease years: {df[\u0026#39;remaining_lease_years\u0026#39;].isna().sum()}\u0026#34;) if df[\u0026#39;remaining_lease_years\u0026#39;].isna().any(): median_lease = df[\u0026#39;remaining_lease_years\u0026#39;].median() print(f\u0026#34;Imputing {df[\u0026#39;remaining_lease_years\u0026#39;].isna().sum()} NaNs in remaining_lease_years with median value: {median_lease}\u0026#34;) df[\u0026#39;remaining_lease_years\u0026#39;].fillna(median_lease, inplace=True) # Min original remaining lease: 40 years 01 month # Max original remaining lease: 97 years 09 months # Min remaining lease years: 40.083333333333336 # Max remaining lease years: 97.75 # NaNs in remaining lease years: 0\rCopy\r2. Convert storey_range to Average Storey Ranges like \u0026quot;04 TO 06\u0026quot; are transformed into a single value‚Äîhere, the average storey number.\ndef convert_storey(storey_range): if pd.isna(storey_range): return np.nan try: low, high = map(int, storey_range.split(\u0026#39; TO \u0026#39;)) return (low + high) / 2.0 except: print(f\u0026#34;Warning: Could not parse storey_range: {storey_range}\u0026#34;) return np.nan print(f\u0026#34;\\nMin in original storey_range: {df[\u0026#39;storey_range\u0026#39;].min()}\u0026#34;) print(f\u0026#34;Max in original storey_range: {df[\u0026#39;storey_range\u0026#39;].max()}\u0026#34;) df[\u0026#39;storey_avg\u0026#39;] = df[\u0026#39;storey_range\u0026#39;].apply(convert_storey) print(f\u0026#34;\\nMin in storey_avg: {df[\u0026#39;storey_avg\u0026#39;].min()}\u0026#34;) print(f\u0026#34;Max in storey_avg: {df[\u0026#39;storey_avg\u0026#39;].max()}\u0026#34;) print(f\u0026#34;\\nNaNs in storey_avg: {df[\u0026#39;storey_avg\u0026#39;].isna().sum()}\u0026#34;) if df[\u0026#39;storey_avg\u0026#39;].isna().any(): median_storey = df[\u0026#39;storey_avg\u0026#39;].median() print(f\u0026#34;Imputing {df[\u0026#39;storey_avg\u0026#39;].isna().sum()} NaNs in storey_avg with median value: {median_storey}\u0026#34;) df[\u0026#39;storey_avg\u0026#39;].fillna(median_storey, inplace=True) # Min in original storey_range: 01 TO 03 # Max in original storey_range: 49 TO 51 # Min in storey_avg: 2.0 # Max in storey_avg: 50.0 # NaNs in storey_avg: 0\rCopy\r3. Extracting Year \u0026amp; Month Components From the month column (e.g., 2020-05), we extract:\nsale_year sale_month cyclical encodings (month_sin and month_cos) to capture seasonality. import numpy as np df[\u0026#39;month\u0026#39;] = pd.to_datetime(df[\u0026#39;month\u0026#39;], format=\u0026#39;%Y-%m\u0026#39;) df[\u0026#39;sale_year\u0026#39;] = df[\u0026#39;month\u0026#39;].dt.year df[\u0026#39;sale_month\u0026#39;] = df[\u0026#39;month\u0026#39;].dt.month df[\u0026#39;month_sin\u0026#39;] = np.sin(2 * np.pi * df[\u0026#39;sale_month\u0026#39;] / 12.0) df[\u0026#39;month_cos\u0026#39;] = np.cos(2 * np.pi * df[\u0026#39;sale_month\u0026#39;] / 12.0) print(f\u0026#34;\\nMin in original month: {df[\u0026#39;month\u0026#39;].min()}\u0026#34;) print(f\u0026#34;Max in original month: {df[\u0026#39;month\u0026#39;].max()}\u0026#34;) print(f\u0026#34;\\nMin in sale_year: {df[\u0026#39;sale_year\u0026#39;].min()}\u0026#34;) print(f\u0026#34;Max in sale_year: {df[\u0026#39;sale_year\u0026#39;].max()}\u0026#34;)\rCopy\r# Sample Output Min in original month: 2017-01-01 00:00:00 Max in original month: 2025-04-01 ‚Ä¶","date":"2025-04-13","permalink":"https://seehiong.github.io/posts/2025/04/feature-impact-on-hdb-predictions/","summary":"In this post, I explore the impact of different feature sets on XGBoost model performance for HDB resale price prediction. By combining numerical, ‚Ä¶","tags":["Network Model","Network Architecture","TensorFlow","XGBoost","MLflow","Kubeflow","MinIO"],"title":"Feature Impact on HDB predictions"},{"content":"I‚Äôm currently progressing through the Advanced Learning Algorithms\rcourse by Andrew Ng\r. To get some hands-on practice, I decided to work on the classic MNIST dataset, which contains 60,000 28√ó28 grayscale images of handwritten digits (0 to 9). The dataset can be downloaded directly from the MNIST homepage\r.\nJupyter Notebook To get started, I used a Jupyter Notebook running TensorFlow, installed via my Kubeflow on K8s\r.\nLoad Data TensorFlow provides a convenient way to load the MNIST dataset\r.\nimport tensorflow as tf from tensorflow import keras (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() assert X_train.shape == (60000, 28, 28) assert X_test.shape == (10000, 28, 28) assert y_train.shape == (60000,) assert y_test.shape == (10000,)\rCopy\rVisualise the Data To better understand the dataset, I visualized a random sample of 64 digits from the training set:\nimport numpy as np import matplotlib.pyplot as plt import warnings warnings.simplefilter(action=\u0026#39;ignore\u0026#39;, category=FutureWarning) m = X_train.shape[0] fig, axes = plt.subplots(8, 8, figsize=(5, 5)) fig.tight_layout(pad=0.13, rect=[0, 0.03, 1, 0.91]) for i, ax in enumerate(axes.flat): random_index = np.random.randint(m) image_to_display = X_train[random_index].reshape((28, 28)) ax.imshow(image_to_display, cmap=\u0026#39;gray\u0026#39;) ax.set_title(y_train[random_index]) ax.set_axis_off() fig.suptitle(\u0026#34;Label, Image\u0026#34;, fontsize=14) plt.show()\rCopy\rModel Representation Each image is a 28√ó28 pixel grid, which we \u0026ldquo;unroll\u0026rdquo; into a 784-dimensional input vector. The architecture of our neural network is:\nInput layer: 784 units Hidden layer 1: 128 units (ReLU) Hidden layer 2: 64 units (ReLU) Output layer: 10 units (for digits 0‚Äì9) We start by normalizing pixel values from 0‚Äì255 to 0‚Äì1 and flattening the image data:\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.activations import linear, relu X_train = X_train.astype(\u0026#39;float32\u0026#39;) / 255.0 X_test = X_test.astype(\u0026#39;float32\u0026#39;) / 255.0 X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) tf.random.set_seed(1234) model = Sequential([ Dense(128, activation=\u0026#39;relu\u0026#39;, input_shape=(784,)), Dense(64, activation=\u0026#39;relu\u0026#39;), Dense(10, activation=\u0026#39;linear\u0026#39;) ], name = \u0026#34;my_model\u0026#34;) model.compile( loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(0.001), metrics=[\u0026#39;accuracy\u0026#39;] ) model.summary()\rCopy\rTrain the Model print(\u0026#34;Starting model training...\u0026#34;) history = model.fit( X_train, y_train, epochs=60 ) print(\u0026#34;Model training finished.\u0026#34;)\rCopy\rEvaluate the Model print(\u0026#34;\\nEvaluating model on test data...\u0026#34;) loss, accuracy = model.evaluate(X_test, y_test, verbose=0) print(f\u0026#34;Test Loss: {loss:.4f}\u0026#34;) print(f\u0026#34;Test Accuracy: {accuracy:.4f}\u0026#34;)\rCopy\r# Sample output Evaluating model on test data... Test Loss: 0.1928 Test Accuracy: 0.9815\rCopy\rLoss (Cost) Plot def plot_loss_tf(history): try: fig, ax = plt.subplots(figsize=(8, 5)) ax.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;loss\u0026#39;, color=\u0026#39;deepskyblue\u0026#39;, linewidth=4) ax.set_title(\u0026#39;Model Loss During Training\u0026#39;) ax.set_ylabel(\u0026#39;Loss\u0026#39;) ax.set_xlabel(\u0026#39;Epoch\u0026#39;) ax.legend(loc=\u0026#39;upper right\u0026#39;) ax.grid(True, color=\u0026#39;lightgrey\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.tight_layout() plt.show() except Exception as e: print(f\u0026#34;Error plotting: {e}\u0026#34;) plot_loss_tf(history)\rCopy\rModel Prediction Let‚Äôs try predicting a digit:\nimport numpy as np def display_digit(image_vector): try: image_matrix = image_vector.reshape((28, 28)) except ValueError as e: print(f\u0026#34;Error reshaping vector of size {num_pixels} to {dimension}x{dimension}: {e}\u0026#34;) return plt.imshow(image_matrix, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() image_of_three = X_train[8888] display_digit(image_of_three) print(y_train[8888]) prediction = model.predict(image_of_three.reshape(1,784)) print(f\u0026#34;Prediction logits:\\n{prediction}\u0026#34;) print(f\u0026#34;Predicted digit: {np.argmax(prediction)}\u0026#34;)\rCopy\rConvert Logits to Probabilities prediction_p = tf.nn.softmax(prediction) print(f\u0026#34;Probability vector:\\n{prediction_p}\u0026#34;) print(f\u0026#34;Sum of probabilities: {np.sum(prediction_p):.3f}\u0026#34;)\rCopy\r# Sample output Probability vector: Probability vector: [[1.1407684e-31 3.7281105e-25 1.1554550e-28 1.0000000e+00 4.0587488e-31 4.0818696e-23 0.0000000e+00 1.4413385e-26 1.2729607e-21 7.6773376e-23]] Sum of probabilities: 1.000\rCopy\rIndex of the largest probability for predicted target yhat = np.argmax(prediction_p) print(f\u0026#34;np.argmax(prediction_p): {yhat}\u0026#34;)\rCopy\r# Sample output np.argmax(prediction_p): 3\rCopy\rPrediction on Sample Set fig, axes = plt.subplots(8,8, figsize=(5,5)) fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) for i,ax in enumerate(axes.flat): random_index = np.random.randint(m) image = ‚Ä¶","date":"2025-04-06","permalink":"https://seehiong.github.io/posts/2025/04/mnist-digit-classifier-in-tensorflow/","summary":"In this post, I walk through building a simple feedforward neural network using TensorFlow to classify handwritten digits from the MNIST dataset. From ‚Ä¶","tags":["Machine Learning","Algorithms","Network Model","Network Architecture","TensorFlow","Keras","MNIST"],"title":"MNIST Digit Classifier in TensorFlow"},{"content":"Adding chat capabilities can greatly enhance user experience, especially in interactive applications. In this post, I‚Äôll integrate a chat function into my Vue Application\rallowing it to interact with an LLM (OpenAI or local models) while visualizing data using Chart.js.\nSetting Up Environment Variables To configure the application, I introduced new environment variables in the .env.example file. Copy and rename it to .env, then add your OpenAI API key:\n# Options: \u0026#34;local\u0026#34; or \u0026#34;openai\u0026#34; VUE_APP_LLM_PROVIDER=openai VUE_APP_LOCAL_LLM_API_ENDPOINT=http://localhost:11434/api/generate VUE_APP_OPENAI_API_ENDPOINT=https://api.openai.com/v1/chat/completions VUE_APP_OPENAI_API_KEY=your_api_key_here\rCopy\rEnhancing the Micronaut-Optimizer Repository The Micronaut-Optimizer\rrepository has been updated with new workflow nodes to support chat interactions.\nNew Workflow Nodes I added two workflow nodes in LeftPanel.vue: a placeholder CONVERT_TO_PROMPT node and a functional CHAT_WITH_LLM node. These changes enable interaction with an LLM:\n\u0026lt;script\u0026gt; export default { data() { return { sections: [ { title: \u0026#34;Transforms\u0026#34;, nodes: [ new Node({ name: NodeTypes.CONVERT_TO_PROMPT, iconType: IconType.TRANSFORM, inputTypes: [\u0026#34;textInput\u0026#34;], outputTypes: [\u0026#34;prompt\u0026#34;], triggerAction: TriggerAction.AUTO, transformType: \u0026#34;convert-to-prompt\u0026#34;, // this transform does nothing at the moment }), ], }, { title: \u0026#34;Chat With LLM\u0026#34;, nodes: [ new Node({ name: NodeTypes.CHAT_WITH_LLM, iconType: IconType.CHAT, inputTypes: [\u0026#34;any\u0026#34;, \u0026#34;prompt\u0026#34;], outputTypes: [\u0026#34;response\u0026#34;], triggerAction: \u0026#34;C\u0026#34;, transformType: \u0026#34;invoke-api\u0026#34;, apiEndpoint: \u0026#34;/api/generate\u0026#34;, // dummy endpoint as its overridden by the .env file }), ], }, ], }; }, methods: { // Get icon for node type getIcon(iconType) { const icons = { input: \u0026#34;üì•\u0026#34;, output: \u0026#34;üì§\u0026#34;, constraint: \u0026#34;‚õìÔ∏è\u0026#34;, transform: \u0026#34;üîÑ\u0026#34;, problem: \u0026#34;üß©\u0026#34;, chat: \u0026#34;üí¨\u0026#34;, }; return icons[iconType] || \u0026#34;üîò\u0026#34;; }, }, }; \u0026lt;/script\u0026gt;\rCopy\rIn WorkflowNode.vue, I added a new Chat action that determines whether to send data to OpenAI or a local LLM:\n\u0026lt;script\u0026gt; import { sendChartDataToOpenAI, sendChartDataToLocalLLM } from \u0026#39;@/utils/nodeUtils\u0026#39;; export default { methods: { async onTrigger() { // User clicks on action button, data at outPort switch (this.node.triggerAction) { case \u0026#34;S\u0026#34;: processSubmitAction(this.node.id, this.node.outputData); break; case \u0026#34;O\u0026#34;: if (this.node.transformType === \u0026#34;invoke-api\u0026#34;) { processApiStreamResponse(this.node); } break; case \u0026#34;C\u0026#34;: if (this.node.transformType === \u0026#34;invoke-api\u0026#34;) { const llmProvider = process.env.VUE_APP_LLM_PROVIDER?.trim() || \u0026#34;local\u0026#34;; if (llmProvider === \u0026#34;openai\u0026#34;) { await sendChartDataToOpenAI(this.node); } else { await sendChartDataToLocalLLM(this.node); } } break; } }, }, }; \u0026lt;/script\u0026gt;\rCopy\rImplementing Chat Functions These are the additional functions in nodeUtils.js:\nSending Chart.js Data to OpenAI export async function sendChartDataToOpenAI(node) { try { const apiEndpoint = process.env.VUE_APP_OPENAI_API_ENDPOINT; const apiKey = process.env.VUE_APP_OPENAI_API_KEY; if (!apiEndpoint || !apiKey) { throw new Error(\u0026#34;Missing required environment variables: VUE_APP_OPENAI_API_ENDPOINT or VUE_APP_OPENAI_API_KEY\u0026#34;); } const data = node.inputData[0]; const prompt = node.inputData[1]; const fullPrompt = `${prompt}\\n\\nData:\\n\\n###${JSON.stringify(data, null, 2)}###`; const requestBody = { model: \u0026#34;gpt-4o-mini\u0026#34;, messages: [{ role: \u0026#34;user\u0026#34;, content: fullPrompt }], temperature: 0.7, max_tokens: 200, }; const response = await fetch(apiEndpoint, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, Authorization: `Bearer ${apiKey}` }, body: JSON.stringify(requestBody), }); if (!response.ok) { throw new Error(`OpenAI request failed: ${await response.text()}`); } const result = await response.json(); if (!result.choices || result.choices.length === 0) { throw new Error(\u0026#34;Invalid OpenAI response\u0026#34;); } node.outputData = result.choices[0].message.content; propagateDataToDownstreamNodes(node); toast.success(\u0026#34;LLM processed the data successfully!\u0026#34;); } catch (error) { toast.error(`LLM request failed: ${error.message}`); console.error(error); } }\rCopy\rSending Chart.js Data to Local LLM export async function sendChartDataToLocalLLM(node) { try { const apiEndpoint = process.env.VUE_APP_LOCAL_LLM_API_ENDPOINT; if (!apiEndpoint) { throw new Error(\u0026#34;Missing required environment variables: VUE_APP_LOCAL_LLM_API_ENDPOINT\u0026#34;); } const data = node.inputData[0]; const prompt = node.inputData[1]; const fullPrompt = `${prompt}\\n\\nData:\\n\\n###${JSON.stringify(data, null, 2)}###`; const requestBody = { model: \u0026#34;deepseek-r1:1.5b\u0026#34;, prompt: fullPrompt, stream: false, }; const response = await fetch(apiEndpoint, { method: \u0026#34;POST\u0026#34;, headers: { ‚Ä¶","date":"2025-02-09","permalink":"https://seehiong.github.io/posts/2025/02/chat-driven-insights-with-chart.js/","summary":"Enhance your Vue.js application by integrating chat capabilities with Chart.js and LLMs like OpenAI and Deepseek-R1. This post walks through adding a ‚Ä¶","tags":["Vue.js","Chat","Chart.js","LLM","OpenAI","Deepseek-R1","Ollama","FE","Frontend","Vue","Optimizer"],"title":"Chat-Driven Insights with Chart.js"},{"content":"In this blog post, I explore the capabilities of JDK Mission Control\r, a powerful tool for low-overhead performance analysis and diagnostics of Java applications.\nIf you‚Äôve read my previous post\r, you‚Äôll know that I have been using DeepSeek extensively. However, with the increasing popularity of DeepSeek, I have noticed a degradation in service performance, as reflected in the DeepSeek status\r. To mitigate this, I have switched to Perplexity Pro\ra complimentary service offered to Singtel customers. For those without access to this, an alternative is Google AI Studio\r. Having an AI pair programmer significantly enhances the troubleshooting process.\nSetting up JMC To begin, download and install JMC from the official JMC 9.0.0 downloads page\r.\nAs part of my ongoing work, I aimed to optimize the inference performance of Micronaut-Llama3\rto support Unsloth\u0026rsquo;s DeepSeek-R1\r. Since DeepSeek-R1 only supports Q4 and Q8 quantization for the Llama architecture, I opted for the Q8_0 model.\nTo integrate support for this model, I made the following modifications:\nChanges to micronaut/model/ChatFormat.java:\npublic ChatFormat(Tokenizer tokenizer) { this.tokenizer = tokenizer; Map\u0026lt;String, Integer\u0026gt; specialTokens = this.tokenizer.getSpecialTokens(); specialTokens.putIfAbsent(\u0026#34;\u0026lt;|begin_of_text|\u0026gt;\u0026#34;, 128000); // for DeepSeek-R1 specialTokens.putIfAbsent(\u0026#34;\u0026lt;|end_of_text|\u0026gt;\u0026#34;, 128001); // for DeepSeek-R1 this.beginOfText = getRequiredToken(specialTokens, \u0026#34;\u0026lt;|begin_of_text|\u0026gt;\u0026#34;); this.startHeader = getRequiredToken(specialTokens, \u0026#34;\u0026lt;|start_header_id|\u0026gt;\u0026#34;); this.endHeader = getRequiredToken(specialTokens, \u0026#34;\u0026lt;|end_header_id|\u0026gt;\u0026#34;); this.endOfTurn = getRequiredToken(specialTokens, \u0026#34;\u0026lt;|eot_id|\u0026gt;\u0026#34;); this.endOfText = getRequiredToken(specialTokens, \u0026#34;\u0026lt;|end_of_text|\u0026gt;\u0026#34;); this.endOfMessage = specialTokens.getOrDefault(\u0026#34;\u0026lt;|eom_id|\u0026gt;\u0026#34;, -1); // only in 3.1 this.stopTokens = Set.of(endOfText, endOfTurn); }\rCopy\rChanges to micronaut/model/Tokenizer.java:\npublic Tokenizer(Vocabulary vocabulary, List\u0026lt;Pair\u0026lt;Integer, Integer\u0026gt;\u0026gt; merges, String regexPattern, Map\u0026lt;String, Integer\u0026gt; specialTokens) { specialTokens.putIfAbsent(\u0026#34;\u0026lt;|begin_of_text|\u0026gt;\u0026#34;, 128000); // for DeepSeek-R1 specialTokens.putIfAbsent(\u0026#34;\u0026lt;|end_of_text|\u0026gt;\u0026#34;, 128001); // for DeepSeek-R1 this.vocabulary = vocabulary; this.compiledPattern = regexPattern != null ? Pattern.compile(regexPattern) : null; this.specialTokens = new HashMap\u0026lt;\u0026gt;(specialTokens); this.merges = new HashMap\u0026lt;\u0026gt;(); for (Pair\u0026lt;Integer, Integer\u0026gt; pair : merges) { int firstIndex = pair.first(); int secondIndex = pair.second(); int mergeIndex = vocabulary.getIndex(vocabulary.get(firstIndex) + vocabulary.get(secondIndex)) .orElseThrow(); this.merges.put(pair, mergeIndex); } } ... public String decode(List\u0026lt;Integer\u0026gt; tokens) { String decoded = decodeImpl(tokens); // Replace the original decodedBytesAsInts with the below int[] decodedBytesAsInts = decoded.codePoints() .map(cp -\u0026gt; { Integer decodedByte = BYTE_DECODER.get(cp); if (decodedByte == null) { return (int) \u0026#39;?\u0026#39;; } return decodedByte; }) .toArray(); byte[] rawBytes = new byte[decodedBytesAsInts.length]; for (int i = 0; i \u0026lt; decoded.length(); i++) { rawBytes[i] = (byte) decodedBytesAsInts[i]; } return new String(rawBytes, StandardCharsets.UTF_8); }\rCopy\rProfiling with JMC To start profiling, simply initiate the Flight Recorder, as shown below:\nReferencing the Llama3.java post\r, I ran the application using:\ngradlew run\rCopy\rConfiguration in application.properties:\nmicronaut.application.name=llama3 micronaut.server.port=8888 llama.BatchSize=32 llama.VectorBitSize=512 llama.PreloadGGUF=DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf options.model_path=DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf options.temperature=0.1f options.topp=0.95f options.seed=42 options.max_tokens=512 options.stream=true options.echo=true options.fullResponseStream=true\rCopy\rTest URL:\nhttp://localhost:8888/api/llama3/generate?prompt=Why%20is%20the%20sky%20blue?\rCopy\rThe profiling results are as follows:\nPerformance Optimization: ByteVector Operations The flame graph analysis highlighted ByteVector operations as an optimization opportunity:\nOptimized code snippet:\n// Instead of separate operations ByteVector loBytes = wBytes.and(MASK_LOW).sub(OFFSET_8); ByteVector hiBytes = wBytes.lanewise(VectorOperators.LSHR, 4).sub(OFFSET_8); // Combine operations ByteVector loBytes = wBytes.and(MASK_LOW); ByteVector hiBytes = wBytes.lanewise(VectorOperators.LSHR, 4).and(MASK_LOW); ByteVector combined = loBytes.blend(hiBytes.lanewise(VectorOperators.LSHL, 4), BLEND_MASK);\rCopy\rJMC vs. VisualVM: A Comparative Analysis JMC offers a more sophisticated and efficient profiling experience, making it a preferred tool for optimizing Java applications at scale.\nFeature VisualVM JDK Mission Control (JMC) Ease of Use Simple, user-friendly Advanced, steeper learning curve ‚Ä¶","date":"2025-02-02","permalink":"https://seehiong.github.io/posts/2025/02/jmc-java-performance-profiling-simplified/","summary":"JDK Mission Control (JMC) is a powerful tool for low-overhead Java application profiling and performance analysis. In this post, I explore JMC‚Äôs ‚Ä¶","tags":["Oracle","JDK","Java","Profiling","JMC","Unsolth","DeepSeek-R1","VisualVM"],"title":"JMC: Java Performance Profiling Simplified"},{"content":"This application was developed with the assistance of AI pair programming tools, including DeepSeek Chat\rand Claude Chat\r. These tools were invaluable in brainstorming solutions, debugging code, and refining the architecture of the application. Their contributions helped streamline the development process and ensured the implementation of best practices.\nFollowing my previous post, Building a Flexible Optimizer Framework with Micronaut\r, I embarked on creating a frontend application with Vue.js. This app serves as a visual interface for designing and managing optimization workflows, seamlessly connecting to the Micronaut backend service. The primary goal was to create an intuitive drag-and-drop UI where users could define optimization problems by connecting inputs, transformations, and outputs. This approach eliminates the need for manual API testing tools and provides a user-friendly way to visualize and compare the efficiency of backend optimization algorithms.\nIn this blog, I‚Äôll guide you through the setup, project structure, and key features of this Vue.js application.\nSetup To get started, ensure you have Node.js\rinstalled. Verify the installation by running:\nnode --version # v22.12.0 npm --version # 10.9.0\rCopy\rNext, install the Vue CLI globally and create the Vue project within the Micronaut-Optimizer\rrepository:\nnpm install -g @vue/cli vue create vue-app\rCopy\rProject Structure The project is organized into a modular structure to ensure scalability and maintainability. Below is an overview of the directory layout:\nvue-app/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ assets/ ‚îÇ ‚îú‚îÄ‚îÄ components/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ inputs/ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ KeyInput.vue ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SubkeyInput.vue ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ TextInput.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ managers/ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DragManager.vue ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ InputManager.vue ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ LinkManager.vue ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ NodeManager.vue ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ OutputManager.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ outputs/ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ TextOutput.vue ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ TSPChartOutput.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ConnectionLine.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Inport.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ LeftPanel.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ LocalPersistent.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ MainPanel.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ OutPort.vue ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ WorkflowNode.vue ‚îÇ ‚îú‚îÄ‚îÄ models/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Node.js ‚îÇ ‚îú‚îÄ‚îÄ store/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.js ‚îÇ ‚îú‚îÄ‚îÄ utils/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ lineConnectionUtils.vue ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ nodeUtils.vue ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ transformUtils.js ‚îÇ ‚îú‚îÄ‚îÄ App.vue ‚îÇ ‚îî‚îÄ‚îÄ main.js ‚îú‚îÄ‚îÄ babel.config.js ‚îú‚îÄ‚îÄ jsconfig.json ‚îú‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ vue.config.js\rCopy\rThis structure groups components, utilities, and managers logically, ensuring the project remains scalable as it evolves.\nKey Concepts and Features 1. Component-Based Architecture The Vue.js app follows a modular, component-based architecture. Components are grouped into subfolders such as inputs, managers and outputs to promote reusability and maintainability.\nBelow are the template sections for two essential components, TextInput.vue and TextOutput.vue:\nTextInput.vue:\n\u0026lt;template\u0026gt; \u0026lt;textarea v-model=\u0026#34;text\u0026#34; placeholder=\u0026#34;Text input...\u0026#34; spellcheck=\u0026#34;false\u0026#34; @mousedown.stop @mouseup.stop @mousemove.stop @input=\u0026#34;onInputChange\u0026#34; :style=\u0026#34;{ width: initialWidth + \u0026#39;px\u0026#39;, height: initialHeight + \u0026#39;px\u0026#39; }\u0026#34;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;/template\u0026gt;\rCopy\rTextOutput.vue:\n\u0026lt;template\u0026gt; \u0026lt;textarea :value=\u0026#34;formattedOutput\u0026#34; placeholder=\u0026#34;Output text...\u0026#34; spellcheck=\u0026#34;false\u0026#34; @mousedown.stop readonly :style=\u0026#34;{ width: initialWidth + \u0026#39;px\u0026#39;, height: initialHeight + \u0026#39;px\u0026#39; }\u0026#34;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;/template\u0026gt;\rCopy\rThese components allow users to drag Workflow Nodes from the Left Panel, link outputs to inputs, and visually design optimization workflows.\n2. Drag-and-Drop Functionality The drag-and-drop feature is managed by DragManager.vue. Below is a snippet illustrating how nodes are handled during the drag process:\n\u0026lt;script\u0026gt; export default { methods: { onStartDrag(nodeId, event) { this.draggingNodeId = nodeId; const node = this.nodes.find((n) =\u0026gt; n.id === nodeId); // Get the container\u0026#39;s position const container = this.$el.parentElement; const containerRect = container.getBoundingClientRect(); // Calculate offset relative to the container this.offset.x = event.clientX - containerRect.left - node.x; this.offset.y = event.clientY - containerRect.top - node.y; document.addEventListener(\u0026#34;mousemove\u0026#34;, this.onMouseMove); document.addEventListener(\u0026#34;mouseup\u0026#34;, this.onStopDrag); }, onStopDrag() { this.draggingNodeId = null; document.removeEventListener(\u0026#34;mousemove\u0026#34;, this.onMouseMove); document.removeEventListener(\u0026#34;mouseup\u0026#34;, this.onStopDrag); }, }, }; \u0026lt;/script\u0026gt;\rCopy\r3. Connection Validation The LinkManager.vue component handles connection validation between nodes, ensuring that connections adhere to predefined rules. These rules are defined in lineConnectionUtils.js.\nExample of Line Linking:\n\u0026lt;script\u0026gt; export default { mounted() { window.addEventListener(\u0026#34;keydown\u0026#34;, this.onKeydown); }, beforeUnmount() { window.removeEventListener(\u0026#34;keydown\u0026#34;, this.onKeydown); }, methods: { ‚Ä¶","date":"2025-01-26","permalink":"https://seehiong.github.io/posts/2025/01/building-a-vue.js-frontend-for-combinatorial-optimization-problems/","summary":"In this post, I‚Äôll walk you through the development of a Vue.js frontend application designed to complement my previous work on a flexible optimizer ‚Ä¶","tags":["Vue.js","TSP","FE","Frontend","Vue","Optimizer"],"title":"Building a Vue.js Frontend for Combinatorial Optimization Problems"},{"content":"Combinatorial optimization problems are fundamental challenges in modern computing, particularly in logistics, network design, and operations research. The Facility Location Problem (FLP) and Traveling Salesman Problem (TSP) represent prime examples of these challenges. In this post, we\u0026rsquo;ll explore the development of a sophisticated optimizer framework using Micronaut, featuring real-time progress tracking and a modular, extensible architecture.\nWith the exciting announcement of the free plan for Github Copilot\r, we\u0026rsquo;ll leverage AI-assisted development to create our Micronaut Optimizer framework. This powerful combination of tools will help us tackle complex combinatorial optimization problems efficiently.\nBackground and Motivation Building on our previous exploration of Optimizing TSP with Genetic Algorithms\r, this post delves deeper into creating a versatile optimization framework. Our focus will be on the architectural decisions and implementation details that make this framework both powerful and extensible.\nArchitecture Overview The architecture of our Micronaut Optimizer framework is designed with modularity and scalability in mind. Below is a high-level architectural diagram that illustrates the key components and their interactions:\nCore Components The framework\u0026rsquo;s architecture is built around several key components, each serving a specific purpose while maintaining loose coupling for maximum flexibility:\nController Layer The controller layer serves as the entry point for HTTP requests, implementing a clean REST API interface. Here\u0026rsquo;s an example of our FLPController:\n// filepath: src/main/java/io/github/seehiong/controller/FLPController.java package io.github.seehiong.controller; @Controller(\u0026#34;/flp\u0026#34;) @RequiredArgsConstructor public class FLPController { private final FLPService flpService; @Post(value = \u0026#34;/uploadSolve\u0026#34;, produces = MediaType.TEXT_EVENT_STREAM, consumes = MediaType.MULTIPART_FORM_DATA) public Flux\u0026lt;Object\u0026gt; uploadSolve(@Body FLPInput input, CompletedFileUpload file) throws IOException { return flpService.uploadSolve(input, file); } }\rCopy\rService Layer The service layer encapsulates our business logic and orchestrates the optimization process. The FLPService demonstrates this orchestration:\n// filepath: src/main/java/io/github/seehiong/service/FLPService.java package io.github.seehiong.service; @Singleton @RequiredArgsConstructor public class FLPService { public Flux\u0026lt;Object\u0026gt; uploadSolve(FLPInput input, CompletedFileUpload file) { Solver\u0026lt;FLPInput, FLPOutput\u0026gt; solver = (Solver\u0026lt;FLPInput, FLPOutput\u0026gt;) solverFactory.getSolver(input.getProblemType()); return solver.solve(input, (PublishSubject\u0026lt;FLPOutput\u0026gt;) progressSubject); } }\rCopy\rFactory Pattern Implementation Our SolverFactory implements a clean factory pattern to instantiate appropriate solvers based on the problem type:\n// filepath: src/main/java/io/github/seehiong/factory/SolverFactory.java package io.github.seehiong.factory; @Singleton public class SolverFactory { public Solver\u0026lt;?, ?\u0026gt; getSolver(ProblemType problemType) { return switch (problemType) { case TSP -\u0026gt; new TSPSolver(); case TSP_GA -\u0026gt; new TSPGaSolver(); case FLP -\u0026gt; new FLPSolver(); default -\u0026gt; throw new IllegalArgumentException(\u0026#34;Unknown problem type: \u0026#34; + problemType); }; } }\rCopy\rSolver Interface The Solver interface defines the contract that all optimization implementations must fulfill:\n// filepath: src/main/java/io/github/seehiong/solver/Solver.java package io.github.seehiong.solver; public interface Solver\u0026lt;I extends Input, O extends Output\u0026gt; { Flux\u0026lt;Object\u0026gt; solve(I input, PublishSubject\u0026lt;O\u0026gt; progressSubject); }\rCopy\rReactive Programming Integration Our framework leverages reactive programming through Project Reactor\u0026rsquo;s Flux and RxJava\u0026rsquo;s PublishSubject. This enables:\nReal-time progress updates to clients Non-blocking execution of optimization algorithms Efficient handling of long-running computations The FLPSolver implementation showcases this reactive approach:\n// filepath: src/main/java/io/github/seehiong/solver/FLPSolver.java package io.github.seehiong.solver; public class FLPSolver implements Solver\u0026lt;FLPInput, FLPOutput\u0026gt; { @Override public Flux\u0026lt;Object\u0026gt; solve(FLPInput input, PublishSubject\u0026lt;FLPOutput\u0026gt; progressSubject) { // Optimization logic with reactive progress updates } }\rCopy\rReal-Time Visualization A key feature of our framework is its ability to visualize optimization progress in real-time, providing valuable insights into the solver\u0026rsquo;s behavior.\nServer-Sent Events Implementation We implement Server-Sent Events (SSE) to stream optimization progress to clients:\n// filepath: src/main/java/io/github/seehiong/controller/FLPController.java package io.github.seehiong.controller; public class FLPController { @Get(value = \u0026#34;/progress/{solverId}\u0026#34;, produces = MediaType.TEXT_EVENT_STREAM) public Flowable\u0026lt;Event\u0026lt;FLPOutput\u0026gt;\u0026gt; ‚Ä¶","date":"2024-12-29","permalink":"https://seehiong.github.io/posts/2024/12/building-a-flexible-optimizer-framework-with-micronaut/","summary":"This post outlines the design and implementation of the Micronaut Optimizer framework, which solves combinatorial optimization problems like TSP and ‚Ä¶","tags":["Genetic Algorithm","Micronaut","TSP","BE","Backend","Java","Optimizer"],"title":"Building a Flexible Optimizer Framework with Micronaut"},{"content":"Building on my previous post on Micronaut-Llama2\r, I‚Äôve undertaken a similar project‚Äîporting llama3.java\rinto a Micronaut application. This initiative is designed to simplify the integration of any large language model (LLM) in the GGUF format into Micronaut-based Java microservices or applications, enabling seamless adoption of cutting-edge AI in production-ready environments.\nGetting Started Below is the environment setup I used for this project:\njava --version # java 23.0.1 2024-10-15 # Java(TM) SE Runtime Environment Oracle GraalVM 23.0.1+11.1 (build 23.0.1+11-jvmci-b01) # Java HotSpot(TM) 64-Bit Server VM Oracle GraalVM 23.0.1+11.1 (build 23.0.1+11-jvmci-b01, mixed mode, sharing) mn --version # Micronaut Version: 4.7.1\rCopy\rCreating a Micronaut Project with GraalVM Support To get started, I created a new Micronaut application with built-in support for GraalVM, Gradle as the build tool, and Java as the programming language:\nmn create-app example.micronaut.llama3 --features=graalvm --build=gradle --lang=java --test=junit\rCopy\rProject Structure Below is the resulting project structure, modularized for scalability and clarity:\nmicronuat-llama3/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îî‚îÄ‚îÄ main/ ‚îÇ ‚îú‚îÄ‚îÄ java/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ example/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ micronaut/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ controller/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ gguf/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ model/ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tensor/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ service/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ utils/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Application.java ‚îÇ ‚îî‚îÄ‚îÄ resources/ ‚îÇ ‚îú‚îÄ‚îÄ application.properties ‚îÇ ‚îî‚îÄ‚îÄ logback.xml ‚îî‚îÄ‚îÄ build.gradle\rCopy\rApplication Configuration Key configurations for the application are defined in application.properties:\nmicronaut.application.name=llama3 llama.BatchSize=16 llama.VectorBitSize=0 llama.PreloadGGUF=Llama-3.2-1B-Instruct-Q4_0.gguf options.model_path=Llama-3.2-1B-Instruct-Q4_0.gguf options.temperature=0.1f options.topp=0.95f options.seed=-1 options.max_tokens=512 options.stream=true options.echo=true\rCopy\rGradle Customization To enable GraalVM native-image builds and optimize runtime performance, the build.gradle file was enhanced as follows:\ndependencies { annotationProcessor(\u0026#34;org.projectlombok:lombok\u0026#34;) compileOnly(\u0026#34;org.projectlombok:lombok\u0026#34;) compileOnly(\u0026#34;io.projectreactor:reactor-core\u0026#34;) } application { mainClass = \u0026#34;example.micronaut.Application\u0026#34; applicationDefaultJvmArgs = [ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules\u0026#39;, \u0026#39;jdk.incubator.vector\u0026#39;, ] } java { sourceCompatibility = JavaVersion.toVersion(\u0026#34;23\u0026#34;) targetCompatibility = JavaVersion.toVersion(\u0026#34;23\u0026#34;) } tasks.withType(JavaCompile) { options.compilerArgs += [ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules\u0026#39;, \u0026#39;jdk.incubator.vector\u0026#39; ] } tasks.withType(JavaExec) { jvmArgs += [ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules\u0026#39;, \u0026#39;jdk.incubator.vector\u0026#39; ] } graalvmNative { toolchainDetection = true binaries { main { imageName = \u0026#34;application\u0026#34; mainClass = \u0026#34;example.micronaut.Application\u0026#34; buildArgs.addAll([ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules=jdk.incubator.vector\u0026#39;, \u0026#39;-O3\u0026#39;, \u0026#39;-march=x86-64\u0026#39;, \u0026#39;--initialize-at-build-time=com.example.Application\u0026#39;, \u0026#39;--enable-monitoring=heapdump,jfr\u0026#39;, \u0026#39;-H:+UnlockExperimentalVMOptions\u0026#39;, \u0026#39;-H:+ForeignAPISupport\u0026#39;, \u0026#39;-H:+ReportExceptionStackTraces\u0026#39;, ]) } } }\rCopy\rClass Diagram The class diagram illustrates the high-level structure of the Llama3 Micronaut application, showcasing the relationships between core components.\nActivity Diagram The activity diagram outlines the typical workflow of the Llama3 Micronaut application from user input to model inference:\nRefactoring llama3.java Building upon the original llama3.java\r, this project refactors and modularizes the codebase into well-defined, logical packages to enhance maintainability and integration within a Micronaut application. Below are the highlights of each package:\nGGUF Package GGUF\ris a binary format for efficient model storage and inference. The GGUF package encapsulates all related data structures, enhancing the modularity of the codebase.\nModel Package The model.tensor subpackage focuses on tensor operations, sampling techniques, and token processing, which are critical for efficient model inference. The main model package encompasses all core model definitions and associated records required for the project.\nUtils Package Utility classes centralize helper methods for model loading, token generation, and runtime performance tracking, streamlining development.\nRunning the Application To build and run the application:\n.\\gradlew clean build run\rCopy\rYou can test the API by sending a request to:\nhttp://localhost:8080/api/llama3/generate?prompt=once%20upon%20a%20time\rCopy\rSample Response:\nCode Repository The complete implementation is available on GitHub: Micronaut-Llama3\r. Feel free to explore, clone, and integrate it into your projects!\n","date":"2024-12-15","permalink":"https://seehiong.github.io/posts/2024/12/porting-llama3.java-to-micronaut/","summary":"This project ports the original single-file llama3.java by Alfonso¬≤ Peterssen into a modular Micronaut application, transforming it from a console app ‚Ä¶","tags":["Llama3","Micronaut","Java","AI","GGUF","LLM"],"title":"Porting Llama3.java to Micronaut"},{"content":"In this post, I‚Äôll demonstrate how to port llama2.java\rinto a Micronaut application. The goal is to expose APIs for text generation and chat functionality with continuous inference support. Along the way, we‚Äôll explore GraalVM, parallelism configuration, and other optimizations.\nGetting Started GraalVM\ris an advanced JDK with ahead-of-time Native Image compilation.\njava --version # java 23.0.1 2024-10-15 # Java(TM) SE Runtime Environment Oracle GraalVM 23.0.1+11.1 (build 23.0.1+11-jvmci-b01) # Java HotSpot(TM) 64-Bit Server VM Oracle GraalVM 23.0.1+11.1 (build 23.0.1+11-jvmci-b01, mixed mode, sharing)\rCopy\rFirst, create a new Micronaut project with GraalVM support:\nmn create-app example.micronaut.llama2 --features=graalvm --build=gradle --lang=java --test=junit\rCopy\rProject Structure The following structure organizes the core components of the Llama2 Micronaut application:\nllama2/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îî‚îÄ‚îÄ main/ ‚îÇ ‚îú‚îÄ‚îÄ java/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ example/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ micronaut/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Application.java ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ model/ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Config.java ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Weights.java ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ RunState.java ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Transformer.java ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Tokenizer.java ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Sampler.java ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ utils/ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ TransformerUtils.java ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ TokenUtils.java ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ SamplingUtils.java ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ service/ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Llama2Service.java ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ controller/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Llama2Controller.java ‚îÇ ‚îî‚îÄ‚îÄ resources/ ‚îÇ ‚îú‚îÄ‚îÄ application.properties ‚îÇ ‚îî‚îÄ‚îÄ logback.xml ‚îî‚îÄ‚îÄ build.gradle\rCopy\rConfiguring Parallelism The parallelism for the ForkJoinPool is set programmatically within Application.java to improve performance during model inference:\n@Singleton public class Application { private final String parallelism; public Application(@Value(\u0026#34;${java.util.concurrent.ForkJoinPool.common.parallelism:8}\u0026#34;) String parallelism) { this.parallelism = parallelism; } public void run(String[] args) { // Programmatically set the parallelism property System.setProperty(\u0026#34;java.util.concurrent.ForkJoinPool.common.parallelism\u0026#34;, parallelism); System.out.println(\u0026#34;ForkJoinPool parallelism set to: \u0026#34; + System.getProperty(\u0026#34;java.util.concurrent.ForkJoinPool.common.parallelism\u0026#34;)); } public static void main(String[] args) { ApplicationContext context = Micronaut.run(Application.class, args); Application app = context.getBean(Application.class); app.run(args); } }\rCopy\rApplication Configuration In application.properties, you can define key settings such as parallelism and file paths for the transformer checkpoint and tokenizer:\nmicronaut.application.name=llama2 java.util.concurrent.ForkJoinPool.common.parallelism=8 transformer.checkpoint_path=stories15M.bin transformer.tokenizer_path=tokenizer.bin\rCopy\rGradle Modifications The build.gradle file was updated to include dependencies and configurations required for GraalVM native image builds and other features:\ndependencies { annotationProcessor(\u0026#34;org.projectlombok:lombok\u0026#34;) compileOnly(\u0026#34;org.projectlombok:lombok\u0026#34;) } application { mainClass = \u0026#34;example.micronaut.Application\u0026#34; applicationDefaultJvmArgs = [ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules\u0026#39;, \u0026#39;jdk.incubator.vector\u0026#39;, ] } java { sourceCompatibility = JavaVersion.toVersion(\u0026#34;23\u0026#34;) targetCompatibility = JavaVersion.toVersion(\u0026#34;23\u0026#34;) } tasks.withType(JavaCompile) { options.compilerArgs += [ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules\u0026#39;, \u0026#39;jdk.incubator.vector\u0026#39; ] } tasks.withType(JavaExec) { jvmArgs += [ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules\u0026#39;, \u0026#39;jdk.incubator.vector\u0026#39; ] } graalvmNative { toolchainDetection = false binaries { main { imageName = \u0026#34;application\u0026#34; mainClass = \u0026#34;com.example.Application\u0026#34; buildArgs.addAll([ \u0026#39;--enable-preview\u0026#39;, \u0026#39;--add-modules=jdk.incubator.vector\u0026#39;, \u0026#39;-march=x86-64\u0026#39;, \u0026#39;--initialize-at-build-time=com.example.Application\u0026#39;, \u0026#39;--enable-monitoring=heapdump,jfr\u0026#39;, \u0026#39;-H:+UnlockExperimentalVMOptions\u0026#39;, \u0026#39;-H:+ForeignAPISupport\u0026#39;, \u0026#39;-H:+ReportExceptionStackTraces\u0026#39;, ]) } } } ...\rCopy\rClass Diagram These are the class diagrams:\nPorting Llama2.java to Micronaut Following Alfonso¬≤ Peterssen\r\u0026rsquo;s original llama2.java\r, the codebase was refactored and modularized into logical packages. Below are the highlights of each package:\nModel Package The model package defines data structures such as Config, Weights, and RunState. For example, here‚Äôs the Config.java class:\n@ToString public class Config { ... Config(ByteBuffer buffer) { this.dim = buffer.getInt(); this.hidden_dim = buffer.getInt(); this.n_layers = buffer.getInt(); this.n_heads = buffer.getInt(); this.n_kv_heads = buffer.getInt(); int vocab_size = buffer.getInt(); this.vocab_size = Math.abs(vocab_size); this.seq_len = buffer.getInt(); this.shared_weights = vocab_size \u0026gt; 0; this.head_size = dim / n_heads; } }\rCopy\rUtils Package The utils package contains helper classes for neural network operations and token management. For example, TransformerUtils.java handles RMS normalization ‚Ä¶","date":"2024-12-07","permalink":"https://seehiong.github.io/posts/2024/12/porting-llama2.java-to-micronaut/","summary":"This post explores porting the single-file llama2.java into a robust Micronaut application, demonstrating both JDK and GraalVM native mode ‚Ä¶","tags":["Llama2","Micronaut","Java","GraalVM","AI"],"title":"Porting Llama2.java to Micronaut"},{"content":"After completing the 5-Day Gen AI Intensive Course\rand diving into Introduction to LangGraph\r, I embarked on building a personal AI Knowledge Assistant. This post documents my exploration of multi-agent processing using LangGraph\r.\nUnlike my earlier posts on Coding with CrewAI\rand Multi-agent Conservation with Autogen\r, this effort focuses on creating a sophisticated multi-agent Retrieval-Augmented Generation (RAG) system using LangGraph.\nGetting Started The Windsurf Editor For this project, I utilized the innovative Windsurf Editor\r, an agent-first IDE. Using the initial prompt:\nI want to build a multi-agent RAG with LangGraph using local Llama3.\rCopy\rI began crafting my AI Knowledge Assistant. Below is a screenshot of the initial code:\nSetting Up the Environment Running Ollama in Docker To power this system, I deployed the Llama3 model using Ollama in a Docker container\r:\ndocker run -d --gpus=all -v $PWD/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\rCopy\rThen, I started the Llama3 model:\ndocker exec -it ollama ollama run llama3\rCopy\rAlternatively, Ollama can be installed natively on Windows, and the model can be run directly:\nollama run llama3\rCopy\rBuilding the AI Knowledge Assistant Initial Code Structure Below is the initial code for main.py. It leverages LangGraph to coordinate multiple agents for query processing.\nfrom typing import Dict, TypedDict, Sequence from langchain_core.messages import BaseMessage, HumanMessage, AIMessage from langgraph.graph import StateGraph from langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_ollama import OllamaLLM, OllamaEmbeddings from langchain_chroma import Chroma import os from dotenv import load_dotenv from IPython.display import Image, display load_dotenv() # Initialize LLM with Ollama OLLAMA_BASE_URL = os.getenv(\u0026#34;OLLAMA_BASE_URL\u0026#34;, \u0026#34;http://host.docker.internal:11434\u0026#34;) MODEL_NAME = os.getenv(\u0026#34;OLLAMA_MODEL\u0026#34;, \u0026#34;llama3\u0026#34;) llm = OllamaLLM( base_url=OLLAMA_BASE_URL, model=MODEL_NAME, temperature=0.75, ) # Initialize embeddings with Ollama embeddings = OllamaEmbeddings( base_url=OLLAMA_BASE_URL, model=MODEL_NAME, ) # Initialize vector store vectorstore = Chroma( persist_directory=\u0026#34;./chroma_db\u0026#34;, embedding_function=embeddings, collection_name=\u0026#34;rag_collection\u0026#34; ) class AgentState(TypedDict): messages: Sequence[BaseMessage] current_step: str context: str research_summary: str final_answer: str def retriever_agent(state: AgentState) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Agent responsible for retrieving relevant documents.\u0026#34;\u0026#34;\u0026#34; query = state[\u0026#34;messages\u0026#34;][-1].content # Search vector store docs = vectorstore.similarity_search(query, k=3) context = \u0026#34;\\n\u0026#34;.join([doc.page_content for doc in docs]) return { **state, \u0026#34;context\u0026#34;: context, \u0026#34;current_step\u0026#34;: \u0026#34;researcher\u0026#34; } def researcher_agent(state: AgentState) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Agent responsible for analyzing retrieved documents.\u0026#34;\u0026#34;\u0026#34; researcher_prompt = ChatPromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;Based on the following context and question, provide a detailed analysis: Context: {context} Question: {question} Analysis:\u0026#34;\u0026#34;\u0026#34; ) chain = researcher_prompt | llm | StrOutputParser() research_summary = chain.invoke({ \u0026#34;context\u0026#34;: state[\u0026#34;context\u0026#34;], \u0026#34;question\u0026#34;: state[\u0026#34;messages\u0026#34;][-1].content }) return { **state, \u0026#34;research_summary\u0026#34;: research_summary, \u0026#34;current_step\u0026#34;: \u0026#34;writer\u0026#34; } def writer_agent(state: AgentState) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Agent responsible for composing the final response.\u0026#34;\u0026#34;\u0026#34; writer_prompt = ChatPromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;Based on the research summary, compose a clear and concise response: Research Summary: {research_summary} Original Question: {question} Response:\u0026#34;\u0026#34;\u0026#34; ) chain = writer_prompt | llm | StrOutputParser() final_answer = chain.invoke({ \u0026#34;research_summary\u0026#34;: state[\u0026#34;research_summary\u0026#34;], \u0026#34;question\u0026#34;: state[\u0026#34;messages\u0026#34;][-1].content }) return { **state, \u0026#34;final_answer\u0026#34;: final_answer, \u0026#34;current_step\u0026#34;: \u0026#34;critic\u0026#34; } def critic_agent(state: AgentState) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Agent responsible for reviewing and refining the response.\u0026#34;\u0026#34;\u0026#34; critic_prompt = ChatPromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;Review and improve the following response if necessary: Context: {context} Original Question: {question} Current Response: {response} Improved Response:\u0026#34;\u0026#34;\u0026#34; ) chain = critic_prompt | llm | StrOutputParser() improved_answer = chain.invoke({ \u0026#34;context\u0026#34;: state[\u0026#34;context\u0026#34;], \u0026#34;question\u0026#34;: state[\u0026#34;messages\u0026#34;][-1].content, \u0026#34;response\u0026#34;: state[\u0026#34;final_answer\u0026#34;] }) new_messages = list(state[\u0026#34;messages\u0026#34;]) new_messages.append(AIMessage(content=improved_answer)) return { **state, \u0026#34;messages\u0026#34;: new_messages } # Create the graph ‚Ä¶","date":"2024-11-24","permalink":"https://seehiong.github.io/posts/2024/11/building-an-ai-knowledge-assistant/","summary":"Learn how to build an AI-powered knowledge assistant using Python. This guide covers data ingestion from sources like PDFs, deploying a FastAPI-based ‚Ä¶","tags":["AI","Ollama","llama3","Chroma","LangChain","LangGraph","Windsurf"],"title":"Building an AI Knowledge Assistant"},{"content":"So, with the recent release of the Raspberry Pi AI Kit\r, I just couldn‚Äôt resist jumping into the AI pool with my shiny new toy. The AI Kit packs a whopping 13 tera-operations per second (TOPS) neural network inference accelerator powered by the mighty Hailo-8L chip.\nGetting Started As always, I got straight to work, setting up my Pi by flashing the Raspberry Pi OS onto a 64GB SD card using the reliable Raspberry Pi Imager\r. I added the active cooler, connected the AI Kit, and hooked up the AI camera. And since I‚Äôm a big fan of building things (especially with Legos), I crafted a highly professional Lego case to house it all.\nSetting Up the Raspberry Pi 5 + Hailo Now, let\u0026rsquo;s get the brain of the operation fired up! Following the excellent Raspberry Pi 5 and Hailo setup guide\r, here‚Äôs how I got everything running smoothly:\nsudo apt update sudo apt full-upgrade # Enable PCIe Gen 3 mode for max speed (because speed is life) sudo raspi-config sudo reboot sudo apt install hailo-all hailortcli fw-control identify\rCopy\rNext, I cloned the repository and installed the necessary tools:\ngit clone https://github.com/hailo-ai/hailo-rpi5-examples.git cd hailo-rpi5-examples ./install.sh\rCopy\rInstalling the AI Camera For the AI Camera, I followed the steps in the official documentation\r. Here\u0026rsquo;s what I ran:\nsudo apt update \u0026amp;\u0026amp; sudo apt full-upgrade sudo apt install imx500-all sudo reboot\rCopy\rAI Camera Now, the fun part ‚Äî getting that AI camera working! The camera app comes with built-in stages for IMX500 object detection and pose estimation, which means my Pi can spot objects and even analyze human poses. Here‚Äôs a quick peek at the settings in the imx500_posenet.json file:\n{ \u0026#34;imx500_posenet\u0026#34;: { \u0026#34;max_detections\u0026#34; : 5, \u0026#34;threshold\u0026#34; : 0.4, \u0026#34;offset_refinement_steps\u0026#34;: 5, \u0026#34;nms_radius\u0026#34;: 10.0, \u0026#34;network_file\u0026#34;: \u0026#34;/usr/share/imx500-models/imx500_network_posenet.rpk\u0026#34;, \u0026#34;save_input_tensor\u0026#34;: { \u0026#34;filename\u0026#34;: \u0026#34;/home/pi/posenet_input_tensor.raw\u0026#34;, \u0026#34;num_tensors\u0026#34;: 10 }, \u0026#34;temporal_filter\u0026#34;: { \u0026#34;tolerance\u0026#34;: 0.3, \u0026#34;factor\u0026#34;: 0.3, \u0026#34;visible_frames\u0026#34;: 8, \u0026#34;hidden_frames\u0026#34;: 2 } }, \u0026#34;plot_pose_cv\u0026#34;: { \u0026#34;confidence_threshold\u0026#34; : 0.2 } }\rCopy\rSome other post-processing JSON files can be found in /usr/share/rpicam-assets/:\nTo take it a step further, I ran the Pose Estimation app, which can detect body joints, limbs, and even facial features, using this command:\nrpicam-hello -t 0s --post-process-file /usr/share/rpi-camera-assets/hailo_yolov8_pose.json --viewfinder-width 1920 --viewfinder-height 1080 --framerate 30\rCopy\rFor object detection, I tried out MobileNet SSD, which performs detection and labels objects in real-time. Check out the action:\nrpicam-hello -t 0s --post-process-file /usr/share/rpi-camera-assets/imx500_mobilenet_ssd.json --viewfinder-width 1920 --viewfinder-height 1080 --framerate 30\rCopy\rThough I love the AI Camera, it‚Äôs a bit of a lightweight compared to the full power of the Hailo AI Kit.\nHailo AI Kit Now, let‚Äôs talk about the Hailo AI Kit. This thing is a beast. Using the YOLOv8s model, it handled object detection like a pro, with a smooth 13 TOPS (for context, that‚Äôs a lot of operations per second). Check out the official demo:\npython basic_pipelines/detection.py\rCopy\rNext, I took it a step further with Retrained Models. With custom models loaded up, it was time to get specific with object detection:\n# Raspberry Pi camera input python basic_pipelines/detection.py --labels-json resources/barcode-labels.json --hef resources/yolov8s-hailo8l-barcode.hef --input rpi\rCopy\rThen, I tried out Pose Estimation with the YOLOv8s_pose model, which detected not just people but their poses:\npython basic_pipelines/pose_estimation.py\rCopy\rFinally, I explored Instance Segmentation, which allows the system to distinguish and separate individual objects within a scene.\npython basic_pipelines/instance_segmentation.py\rCopy\rWrap-Up In conclusion, this little Raspberry Pi 5 is an absolute champ when paired with the Hailo-8L AI Kit. From object detection to pose estimation and even instance segmentation, I‚Äôve only scratched the surface of its capabilities. So, stay tuned‚Äîthis is just the beginning of my AI adventures!\n","date":"2024-11-09","permalink":"https://seehiong.github.io/posts/2024/11/exploring-ai-with-raspberry-pi-5/","summary":"In this post, I dive into the powerful capabilities of the Raspberry Pi 5 paired with the Hailo-8L AI Kit, a neural network accelerator offering 13 ‚Ä¶","tags":["Raspberry Pi","AI Kit","AI Camera","AI","Hailo"],"title":"Exploring AI with Raspberry Pi 5"},{"content":"In this post, I‚Äôll build on my previous post\r, where we set up GPT-2. Following Andrej Karpathy‚Äôs instructional video, I‚Äôll walk through each step for training GPT-2 on a small dataset‚ÄîTiny Shakespeare. This post is a documentation of my learning journey with GPT-2, closely following Karpathy\u0026rsquo;s approach.\nTraining We\u0026rsquo;ll use the Tiny Shakespeare dataset to get started:\nwith open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: text = f.read() data = text[:1000] print(data[:100]) # Sample output: # First Citizen: # Before we proceed any further, hear me speak. # # All: # Speak, speak. # # First Citizen: # You\rCopy\rTo verify the data size, we can use a word count tool in WSL:\nwc input.txt # Output: # 40000 202651 1115394 input.txt\rCopy\rEncoding the Dataset with tiktoken Using tiktoken to encode the dataset (GPT-2\u0026rsquo;s tokenizer), we can observe that 198 represents the newline character:\nimport tiktoken enc = tiktoken.get_encoding(\u0026#39;gpt2\u0026#39;) tokens = enc.encode(data) print(tokens[:24]) # [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\rCopy\rTo break this into sequences, we convert the encoded data into B x T tensors for batching.\nimport torch buf = torch.tensor(tokens[:24 + 1]) x = buf[:-1].view(4,6) y = buf[1:].view(4,6) print(x) # input tensor print(y) # label tensor # tensor([[ 5962, 22307, 25, 198, 8421, 356], # [ 5120, 597, 2252, 11, 3285, 502], # [ 2740, 13, 198, 198, 3237, 25], # [ 198, 5248, 461, 11, 2740, 13]]) # tensor([[22307, 25, 198, 8421, 356, 5120], # [ 597, 2252, 11, 3285, 502, 2740], # [ 13, 198, 198, 3237, 25, 198], # [ 5248, 461, 11, 2740, 13, 198]])\rCopy\rAdding a Loss Function Let\u0026rsquo;s define a loss function in the custom GPT model:\nclass GPT(nn.Module): ... def forward(self, idx, targets=None): # idx is of shape (B, T) B, T = idx.size() assert T \u0026lt;= self.config.block_size, f\u0026#34;Cannot forward sequence of length {t}, block size is only {self.config.block_size}\u0026#34; # forward the token and position embeddings pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (t) pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd) tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd) x = tok_emb + pos_emb # forward the blocks of the transformer for block in self.transformer.h: x = block(x) # forward the final layernorm and the classifier x = self.transformer.ln_f(x) logits = self.lm_head(x) # (B, T, vocab_size) loss = None if targets is not None: loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) return logits, loss device = \u0026#34;cpu\u0026#34; if torch.cuda.is_available(): device = \u0026#34;cuda\u0026#34; elif hasattr(torch.backends, \u0026#34;mps\u0026#34;) and torch.backends.mps.is_available(): device = \u0026#34;mps\u0026#34; print(f\u0026#34;using device: {device}\u0026#34;) # get a data batch import tiktoken enc = tiktoken.get_encoding(\u0026#39;gpt2\u0026#39;) with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: text = f.read() text = text[:1000] tokens = enc.encode(text) B, T = 4, 32 buf = torch.tensor(tokens[:B*T + 1]) buf = buf.to(device) # move buf to same device x = buf[:-1].view(B, T) y = buf[1:].view(B, T) # get logits model = GPT(GPTConfig()) model.to(device) logits, loss = model(x, y) print(loss) # using device: cuda # tensor(11.0591, device=\u0026#39;cuda:0\u0026#39;, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) # expected loss at initialization input = -math.log(1/50257) print(input) # 10.82490511970208\rCopy\rOptimizing the Model We\u0026rsquo;ll use the AdamW optimizer, which is typically effective for initial GPT training:\n# optimize! optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4) # good learning rate for most at the beginning for i in range(50): optimizer.zero_grad() # to always start zero gradient logits, loss = model(x, y) loss.backward() # deposits or add the gradient optimizer.step() # update parameters, decrease loss print(f\u0026#34;step: {i}, loss: {loss.item()}\u0026#34;) # step: 0, loss: 11.059085845947266 # step: 1, loss: 6.672627925872803 # step: 2, loss: 4.326003074645996 # ... # step: 47, loss: 0.003014578018337488 # step: 48, loss: 0.002937569282948971 # step: 49, loss: 0.002866392722353339\rCopy\rAdding a Simple DataLoader A lightweight data loader simplifies the batching process by iterating over the encoded data:\nimport tiktoken class DataLoaderLite: def __init__(self, B, T): self.B = B self.T = T with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: text = f.read() enc = tiktoken.get_encoding(\u0026#39;gpt2\u0026#39;) tokens = enc.encode(text) self.tokens = torch.tensor(tokens) print(f\u0026#34;loaded {len(self.tokens)} tokens\u0026#34;) print(f\u0026#34;1 epoch = {len(self.tokens) // (B * T)} batches\u0026#34;) self.current_position = 0 def next_batch(self): B, T = self.B, self.T buf = self.tokens[self.current_position : self.current_position+B*T+1] x = buf[:-1].view(B, T) # inputs y = buf[1:].view(B, T) # targets # advance the position in the tensor self.current_position += B * T # if loading the next ‚Ä¶","date":"2024-10-31","permalink":"https://seehiong.github.io/posts/2024/10/gpt-2-training-guide/","summary":"This post documents my journey training GPT-2 on the Tiny Shakespeare dataset, inspired by Andrej Karpathy\u0026rsquo;s instructional video and nanoGPT ‚Ä¶","tags":["AI","GPT","PyTorch","LLM","nanoGPT","MLflow"],"title":"GPT-2 Training Guide"},{"content":"In this post, I‚Äôll document my journey in learning how to reproduce GPT-2 from scratch using my 6GB NVIDIA RTX A2000 GPU. This is my first attempt at training a model from scratch, and I‚Äôm excited to learn from the experts and share my experiences here.\nThe Basics I began my journey with the video Create a Large Language Model from Scratch with Python\rby Elliot Arledge\r. This video covers the fundamentals of large language models (LLMs) and demonstrates how to build one from the ground up. Here, I‚Äôve documented the foundational concepts I extracted from the initial stages of this video.\nPyTorch Basic Examples As part of this journey, I‚Äôm learning PyTorch\r, an optimized tensor library for deep learning on GPUs and CPUs. In PyTorch, tensors are specialized data structures similar to arrays and matrices, with additional capabilities that make them suitable for deep learning.\nimport torch device = ( \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;mps\u0026#34; if torch.backends.mps.is_available() else \u0026#34;cpu\u0026#34; ) print(f\u0026#34;Using {device} device\u0026#34;) # Output: Using cuda device\rCopy\rThe shape of a tensor in PyTorch refers to its dimensions ‚Äî the number of elements along each axis. For example, a tensor with shape (2, 3, 4) means:\n2 elements along the first axis (depth) 3 elements along the second axis (height) 4 elements along the third axis (width) Here are some of the torch functions:\nrandint = torch.randint(-100, 100, (6,)) print(randint) # Output: tensor([-21, 0, -39, -71, -64, -60]) tensor = torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]]) print(tensor) # tensor([[0.1000, 1.2000], # [2.2000, 3.1000], # [4.9000, 5.2000]]) zeros = torch.zeros(2, 3) print(zeros) # tensor([[0., 0., 0.], # [0., 0., 0.]]) ones = torch.ones(2, 3) print(ones) # tensor([[1., 1., 1.], # [1., 1., 1.]]) input = torch.empty(2,3) print(input) # tensor([[-1.1287e+28, 6.1223e-41, -1.1247e+28], # [ 6.1223e-41, 1.6678e+19, 7.0976e+22]]) arange = torch.arange(5) print(arange) # tensor([0, 1, 2, 3, 4])\rCopy\rMeasuring Time Taken By using the %%time magic command at the beginning of a cell, I can measure how long the entire cell takes to run, which helps track and optimize execution time.\n%%time import time start_time = time.time() zeros = torch.zeros(1, 1) end_time = time.time() elapsed_time = end_time - start_time print(f\u0026#34;{elapsed_time:.8f}\u0026#34;) # Output: 0.00000000 seconds # CPU times: total: 0 ns # Wall time: 0 ns\rCopy\rAdditional PyTorch Features Here are some additional PyTorch functions I explored, which I‚Äôll use later in the model-building process. I also explored tril, triu, and masked_fill for manipulating tensor data, and transpose for altering tensor dimensions. These will be helpful for matrix operations and attention mechanisms.\n# Returns a tensor where each row contains num_samples indices sampled from the multinomial distribution located in the corresponding row of tensor input input = torch.tensor([0.1, 0.9]) samples = torch.multinomial(input, num_samples=10, replacement=True) print(samples) # Output: tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1]) # Concatenates the given sequence of tensors in tensors in the given dimension tensor = torch.tensor([1, 2, 3, 4]) out = torch.cat((tensor, torch.tensor([5])), dim=0) print(out) # tensor([1, 2, 3, 4, 5]) # Returns the lower triangular part of the matrix (2-D tensor), the other elements of the result tensor out are set to 0 out = torch.tril(torch.ones(5, 5)) print(out) # tensor([[1., 0., 0., 0., 0.], # [1., 1., 0., 0., 0.], # [1., 1., 1., 0., 0.], # [1., 1., 1., 1., 0.], # [1., 1., 1., 1., 1.]]) # Returns the upper triangular part of a matrix (2-D tensor), the other elements of the result tensor out are set to 0 out = torch.triu(torch.ones(5, 5)) print(out) # tensor([[1., 1., 1., 1., 1.], # [0., 1., 1., 1., 1.], # [0., 0., 1., 1., 1.], # [0., 0., 0., 1., 1.], # [0., 0., 0., 0., 1.]]) # Fills elements of self tensor with value, -inf where mask is True out = torch.zeros(5, 5).masked_fill(torch.tril(torch.ones(5, 5)) == 0, float(\u0026#39;-inf\u0026#39;)) print(out) # tensor([[0., -inf, -inf, -inf, -inf], # [0., 0., -inf, -inf, -inf], # [0., 0., 0., -inf, -inf], # [0., 0., 0., 0., -inf], # [0., 0., 0., 0., 0.]]) # Returns a tensor that is a transposed version of input wheret the given dimensions dim0 and dim1 are swapped input = torch.zeros(2, 3, 4) out = input.transpose(0, 2) print(out.shape) print(out) # torch.Size([4, 3, 2]) # tensor([[[0., 0.], # [0., 0.], # [0., 0.]], # # [[0., 0.], # [0., 0.], # [0., 0.]], # # [[0., 0.], # [0., 0.], # [0., 0.]], # # [[0., 0.], # [0., 0.], # [0., 0.]]])\rCopy\rLinear Transformations The linear layer in PyTorch applies an affine transformation, represented as \\( y = xA^T + b \\), where \\( y \\) is the output, \\( x \\) is the input, \\( A \\) is the weight matrix and \\( b \\) is the bias vector.\nimport torch.nn as nn sample = torch.tensor([10., 10., 10.]) linear = nn.Linear(3, 3, bias=False) print(linear) print(linear(sample)) # Output: tensor([10., 10., 10.]) # ‚Ä¶","date":"2024-10-28","permalink":"https://seehiong.github.io/posts/2024/10/gpt-2-setup-and-pretraining-guide/","summary":"This guide explores reproducing GPT-2 (124M) using Andrej Karpathy‚Äôs video walkthrough. It begins with an overview of the GPT-2 architecture, a ‚Ä¶","tags":["AI","GPT","PyTorch","LLM","nanoGPT"],"title":"GPT-2 Setup and Pretraining Guide"},{"content":"In this post, I will detail the installation of MLflow and Kubeflow on my Talos Homelab cluster.\nPreparation I have decided to reinitialize my homelab. You can follow similar steps in your own environment.\nTalos Setup As outlined in my previous Talos Linux setup\r, here is my updated control.patch file:\nmachine: network: hostname: control install: disk: /dev/nvme0n1 image: ghcr.io/siderolabs/installer:v1.7.6 wipe: true kubelet: defaultRuntimeSeccompProfileEnabled: false cluster: apiServer: admissionControl: - name: PodSecurity configuration: apiVersion: pod-security.admission.config.k8s.io/v1alpha1 defaults: audit: privileged audit-version: latest enforce: privileged enforce-version: latest warn: privileged warn-version: latest exemptions: namespaces: [] # Apply to all namespaces runtimeClasses: [] usernames: [] kind: PodSecurityConfiguration\rCopy\rNote\rI encountered an issue (time query error with server \u0026ldquo;17.253.60.125\u0026rdquo;) while setting up the latest Talos v1.8.1, which was being resolved with:\n# Edit control node talosctl edit machineconfig -n 192.168.68.115\rCopy\rmachine: time: disabled: false servers: - time.cloudflare.com\rCopy\rFor my first worker node, here‚Äôs the worker-1.patch:\nmachine: network: hostname: worker-1 install: disk: /dev/nvme0n1 image: ghcr.io/siderolabs/installer:v1.7.6 wipe: true kubelet: extraMounts: - destination: /var/mnt type: bind source: /var/mnt options: - bind - rw\rCopy\rThe installation steps remain unchanged:\n# Single master node talosctl gen config homelab https://192.168.68.115:6443 talosctl disks --insecure -n 192.168.68.115 talosctl machineconfig patch controlplane.yaml --patch @control.patch --output control.yaml talosctl apply-config --insecure -n 192.168.68.115 --file control.yaml talosctl bootstrap --nodes 192.168.68.115 --endpoints 192.168.68.115 --talosconfig talosconfig # Worker nodes talosctl machineconfig patch worker.yaml --patch @worker-1.patch --output worker-1.yaml talosctl apply-config --insecure -n 192.168.68.117 --file worker-1.yaml\rCopy\rLocal Path Provisioner Local-path will serve as the default storageClass for ReadWriteOnce access modes. Follow these steps:\ncurl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.28/deploy/local-path-storage.yaml -O\rCopy\rEdit the local-path-storage.yaml file to set it as the default:\n--- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-path annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; # around line 120 --- kind: ConfigMap apiVersion: v1 metadata: name: local-path-config namespace: local-path-storage data: # below section around line 131 config.json: |- { \u0026#34;nodePathMap\u0026#34;:[ { \u0026#34;node\u0026#34;:\u0026#34;DEFAULT_PATH_FOR_NON_LISTED_NODES\u0026#34;, \u0026#34;paths\u0026#34;:[\u0026#34;/var/mnt\u0026#34;] } ] }\rCopy\rNFS To support ReadWriteMany access modes, follow these steps:\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.68.111 \\ --set nfs.path=/mnt/public\rCopy\rMetallb To install Metallb, execute the following:\ncurl https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml -O kubectl apply -f metallb-native.yaml kubectl apply -f metallb-ipaddresspool.yaml kubectl apply -f metallb-l2advertisement.yaml\rCopy\rmetallb-ipaddresspool.yaml example:\napiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: first-pool namespace: metallb-system spec: addresses: - 192.168.68.220-192.168.68.240\rCopy\rmetallb-l2advertisement.yaml example:\napiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: first-advert namespace: metallb-system spec: ipAddressPools: - first-pool\rCopy\rKubeflow To install Kubeflow, follow the steps from my previous Kubeflow setup\r:\ngit clone https://github.com/kubeflow/manifests.git cd manifests while ! kustomize build example | kubectl apply -f -; do echo \u0026#34;Retrying to apply resources\u0026#34;; sleep 20; done\rCopy\rMLflow MLflow\ris an open-source platform designed to streamline the machine learning lifecycle, ensuring that all phases are manageable and reproducible.\nTo install MLflow on my Talos HomeLab cluster:\nhelm install mlflow oci://registry-1.docker.io/bitnamicharts/mlflow --namespace mlflow --create-namespace # Sample output # CHART NAME: mlflow # CHART VERSION: 2.0.2 # APP VERSION: 2.17.0 # # ** Please be patient while the chart is being deployed ** # You didn\u0026#39;t specify any entrypoint to your code. # To run it, you can either deploy again using the `source.launchCommand` option to specify your entrypoint, or # # execute it manually by jumping into the pods: # # 1. Get the running pods # kubectl get pods --namespace mlflow -l \u0026#34;app.kubernetes.io/name=mlflow,app.kubernetes.io/instance=mlflow\u0026#34; # # 2. Get into a pod # kubectl exec -ti [POD_NAME] bash # # 3. Execute your script as you ‚Ä¶","date":"2024-10-20","permalink":"https://seehiong.github.io/posts/2024/10/integrating-mlflow-and-kubeflow-on-talos/","summary":"This post details the installation of MLflow and Kubeflow on a Talos HomeLab cluster. It covers the setup process, including Talos configuration, ‚Ä¶","tags":["MLOps","MLflow","Kubeflow","Talos","HomeLab","K9s","NFS","Metallb"],"title":"Integrating MLflow and Kubeflow on Talos"},{"content":"Replicating a SaaS environment locally for testing microservices is a frequent need when developing modern applications. In this post, I\u0026rsquo;ll guide you through the steps to replicate a Google Kubernetes Engine (GKE) setup using Talos Linux\rin a VirtualBox VM. This approach is ideal for ensuring seamless integration with external services and testing your microservices code before pushing to production.\nThis demo is based on the popular Microservices-demo\r, which I previously encountered while preparing for my Professional Cloud Architect\rcertification. Let‚Äôs explore how to deploy the same setup in a local Kubernetes cluster powered by Talos Linux.\nPreparation To begin, create a new virtual machine in Oracle VirtualBox named talosvm, using the Talos Linux v1.7.6 ISO\r. Given the workloads we‚Äôll be deploying, allocate 8GB memory and 4 CPUs to the VM.\nFor networking, use the Bridged Adapter, selecting your Wi-Fi or LAN adapter, and set the promiscuous mode to \u0026ldquo;Allow All.\u0026rdquo;\nInstallation Installing Talos in VirtualBox To install Talos, we‚Äôll reference my earlier post on setting up Talos Linux\r. Below are the key steps:\nGenerate the configuration for the control plane node:\ntalosctl gen config talosvm https://192.168.68.106:6443\rCopy\rHere‚Äôs the content of the control.patch file used to customize the installation:\n# control.patch machine: network: hostname: talos-control install: disk: /dev/sda image: ghcr.io/siderolabs/installer:v1.7.6 wipe: true cluster: clusterName: talosvm allowSchedulingOnControlPlanes: true\rCopy\rUsing WSL, apply this configuration with:\ntalosctl machineconfig patch controlplane.yaml --patch @control.patch --output control.yaml talosctl apply-config --insecure -n 192.168.68.106 --file control.yaml\rCopy\rOnce the VM reboots and etcd is running, bootstrap the control plane:\ntalosctl bootstrap --nodes 192.168.68.106 --endpoints 192.168.68.106 --talosconfig talosconfig\rCopy\rCopy the talosconfig contents to %USERPROFILE%/.talos/config, and update the IP address to 192.168.68.106. To access Kubernetes with kubectl or k9s, merge the configuration into your Kubeconfig:\ntalosctl kubeconfig -n 192.168.68.106\rCopy\rDeploying the Microservices Demo Clone the microservices-demo repository and apply it using Kustomize:\ngit clone https://github.com/GoogleCloudPlatform/microservices-demo cd microservices-demo/kustomize kubectl apply -k .\rCopy\rMonitor the progress using K9s:\nInstalling MetalLB To assign external IPs, we‚Äôll use MetalLB. First, download and apply the manifest:\nmkdir ~/metallb cd ~/metallb wget https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml -O metallb-native.yaml\rCopy\rDefine the IP address pool:\napiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: first-pool namespace: metallb-system spec: addresses: - 192.168.68.230-192.168.68.240\rCopy\rSet up L2 advertisement:\napiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: first-advert namespace: metallb-system spec: ipAddressPools: - first-pool\rCopy\rApply the configurations:\nkubectl apply -f metallb-native.yaml kubectl apply -f metallb-ip-address-pool.yaml kubectl apply -f metallb-l2-advertisement.yaml\rCopy\rWith MetalLB set up, the frontend service will receive an external IP:\nOptional - Provision Istio with istioctl To enhance observability and manage microservices traffic, we can provision Istio. Start by installing the Gateway API CRDs:\nkubectl get crd gateways.gateway.networking.k8s.io \u0026amp;\u0026gt; /dev/null || \\ { kubectl kustomize \u0026#34;github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.1.0\u0026#34; | kubectl apply -f -; }\rCopy\rInstall Istio on your WSL system:\ncd ~ curl -L https://istio.io/downloadIstio | sh - # Setup the path to istio in bashrc export PATH=\u0026#34;$PATH:/home/pi/istio-1.23.2/bin\u0026#34; # Perform a precheck istioctl x precheck # Sample result: # ‚úî No issues found when checking the cluster. Istio is safe to install or upgrade! # To get started, check out https://istio.io/latest/docs/setup/getting-started/\rCopy\rInstall Istio on the Talos VM:\nistioctl install --set profile=minimal -y # Uninstall istioctl uninstall --purge\rCopy\rEnable the Istio component in Kustomize:\n# Delete the previously deployed workloads first, before executing the next command kubectl delete -k . cd kustomize/ kustomize edit add component components/service-mesh-istio kubectl apply -k .\rCopy\rOptional - Injecting the Istio Sidecar To inject Istio sidecars, label the default namespace:\nkubectl label namespace default istio-injection=enabled kubectl get namespace default --show-labels # Sample result # NAME STATUS AGE LABELS # default Active 116m istio-injection=enabled,kubernetes.io/metadata.name=default\rCopy\rIf Talos is using the baseline PodSecurity policy, you may need to adjust it to allow privileged pods:\nkubectl label namespace default pod-security.kubernetes.io/enforce=privileged kubectl get namespace default --show-labels # Sample result # NAME STATUS AGE LABELS # default ‚Ä¶","date":"2024-10-13","permalink":"https://seehiong.github.io/posts/2024/10/deploy-microservices-with-talos-locally/","summary":"In this guide, we walk through deploying the Google Cloud Microservices Demo locally using Talos Linux in a VirtualBox VM. This step-by-step tutorial ‚Ä¶","tags":["K8s","K9s","Talos","VirtualBox","Kustomize","Metallb","Isito","Kiali"],"title":"Deploy Microservices with Talos Locally"},{"content":"In this post, I explore the RoArm-M2-S with the Wave Rover\r. Due to the small base, there\u0026rsquo;s a tendency for instability, so I plan to upgrade to a UGV Rover\r. Meanwhile, let\u0026rsquo;s focus on setting up the RoArm.\nSetup Following the official Getting Started Tutorial\r, we begin by installing Oracle VirtualBox.\nInstalling Oracle VirtualBox Download the VirtualBox Platform Packages\rand its Extension Pack. Install both, then proceed to create a new Virtual Machine (VM) named RoArm.\nCreating the RoArm VM Download Ubuntu 22.04.5 LTS\rand complete the unattended installation.\nIf you encounter the user not in sudoers or blank screen issues, access the GRUB menu by pressing Esc during boot and select recovery mode.\nTo add your user to the sudoers file: usermod -aG sudo pi id pi\rCopy\rEnable auto-login: sudo nano /etc/gdm3/custom.conf\r# Uncomment the following lines\rAutomaticLoginEnable = true\rAutomaticLogin = pi\rCopy\rSet the VM Display Graphics Controller to VMSVGA, and configure options like Drag and Drop to Bidirectional. Installing ROS2 ROS2\ris essential for building robot applications. I attempted using the prebuilt ROS2 image, but encountered issues with mouse control, so I continued with a custom VM setup.\nROS2 + Moveit2 Installation Clone the RoArm-M2-S\rrepository and install necessary packages:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade sudo apt install git git clone https://github.com/DUDULRX/roarm_ws_em0.git\rCopy\rAdd ROS2 sources and install dependencies:\nsudo apt install software-properties-common sudo add-apt-repository universe sudo apt update \u0026amp;\u0026amp; sudo apt install curl -y sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release \u0026amp;\u0026amp; echo $UBUNTU_CODENAME) main\u0026#34; | sudo tee /etc/apt/sources.list.d/ros2.list \u0026gt; /dev/null sudo apt update sudo apt upgrade sudo apt install ros-humble-desktop sudo apt install ros-dev-tools sudo apt install net-tools sudo apt install ros-humble-moveit-* sudo apt install ros-humble-foxglove-bridge sudo apt autoremove ros-humble-moveit-servo-*\rCopy\rSource the environment:\necho \u0026#34;source /opt/ros/humble/setup.bash\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc\rCopy\rFinally, build the RoArm repository:\nsudo apt install python3-pip cd ~/roarm_ws_em0 python3 -m pip install -r requirements.txt cd ~/roarm_ws_em0 sudo chmod +x build_first.sh . build_first.sh cd ~/roarm_ws_em0 colcon build echo \u0026#34;source ~/roarm_ws_em0/install/setup.bash\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc Copy\rControlling a Physical RoArm Connect the RoArm-M2-S to your PC via a USB-C cable, then verify the connection:\nls /dev/tty* sudo chmod 666 /dev/ttyUSB0 # Adjust based on your setup\rCopy\rLaunch the driver node:\nros2 run roarm_driver roarm_driver\rCopy\rRViz Visualization Visualize joint movements with RViz\r:\nros2 launch roarm_description display.launch.py\rCopy\rMoveIt Integration Run the MoveIt\rdemo (press Ctrl + C to close the Rviz session):\nros2 launch roarm_moveit interact.launch.py\rCopy\rKeyboard Control Control the arm via keyboard (press Ctrl + C to close the MoveIt session):\nros2 launch moveit_servo demo.launch.py ros2 run roarm_moveit_cmd setgrippercmd ros2 run roarm_moveit_cmd keyboardcontrol\rCopy\rROS2Web_app for Web Control ROS2Web_app provides a web interface to control the RoArm. Follow these steps (ensure that the roarm_driver is still running):\nLaunch the MoveIt2 servo node: ros2 launch moveit_servo demo.launch.py\rCopy\rStart the web server: ros2 web server --no-auth\rCopy\rRun the web app: ros2 run roarm_web_app roarm_web_app\rCopy\rRun the web app control node ros2 run roarm_moveit_cmd webappcontrol\rCopy\rAccess the app via your browser at http://10.0.2.15:8080/roarm_web_app (adjust based on your VM setup): Optional - Installing VSCode If you prefer using VScode\rfor development, install it with the following commands:\nwget -O code_amd64.deb https://go.microsoft.com/fwlink/?LinkID=760868 sudo apt install ./code_amd64.deb\rCopy\r","date":"2024-10-06","permalink":"https://seehiong.github.io/posts/2024/10/controlling-roarm-m2-s-with-ros2/","summary":"Learn how to set up and control the RoArm-M2-S robotic arm using ROS2 in a virtual environment. This guide walks you through installing VirtualBox, ‚Ä¶","tags":["RoArm-M2-S","Robotics","RViz","VirtualBox","ROS2","Moveit"],"title":"Controlling RoArm-M2-S with ROS2"},{"content":"Continuing my previous journey with the Wave Rover\r, this post documents how I used a Raspberry Pi 4 as the main controller for the project.\nPreparation Using the Raspberry Pi Imager\r, I installed the 64-bit Raspberry Pi OS on a 64GB microSD card. I customized the installation by setting the hostname, username, password, and configuring the wireless LAN.\nInstallation Installing UGV_RPI Clone the Raspberry Pi example\rand install the necessary components: # Clone the repository git clone https://github.com/waveshareteam/ugv_rpi.git # Grant execution permissions cd ugv_rpi/ sudo chmod +x setup.sh sudo chmod +x autorun.sh # Install the app sudo ./setup.sh\rCopy\rAfter rebooting the Pi, autorun the setup: cd ugv_rpi/ ./autorun.sh # Reboot Pi sudo reboot\rCopy\rInstall AccessPopup by selecting option 1, then exit the installconfig.sh script by pressing 9: cd ugv_rpi/AccessPopup sudo chmod +x installconfig.sh sudo ./installconfig.sh # Reboot Pi to activate the cron job sudo reboot\rCopy\rInstalling VNC Server Enable VNC using the following command. Go to \u0026ldquo;Interface Options\u0026rdquo; and enable VNC:\nsudo raspi-config\rCopy\rInstalling RealVNC Viewer On your PC, download RealVNC Viewer\rand connect to your Raspberry Pi using the Pi\u0026rsquo;s credentials:\nInstalling Raspberry Pi Camera For the camera setup, use the The Picamera2 Library\r. After connecting the Pi Camera Rev 1.3, run the following command to set up video recording:\nrpicam-vid --frames 300 --qt-preview -o sink.h264\rCopy\rJupyterLab on Boot JupyterLab has been configured to start automatically via crontab:\ncrontab -e\rCopy\rNavigate to http://rasfi:8888/lab to explore the tutorial.\nPython Chassis Motion Control Here‚Äôs an example of Python code to control the Wave Rover‚Äôs movement:\nfrom base_ctrl import BaseController import time # GPIO Serial Device on Raspberry Pi 4 base = BaseController(\u0026#39;/dev/serial0\u0026#39;, 115200) # Move forward at 0.2 m/s for 2 seconds base.send_command({\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:0.2,\u0026#34;R\u0026#34;:0.2}) time.sleep(2) base.send_command({\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:0,\u0026#34;R\u0026#34;:0})\rCopy\rImage Transmission with Flask You can stream the Pi camera feed using Flask. Navigate to http://rasfi:5000 after running the following:\ncd ugv_rpi/tutorial_en/12 python flask_camera.py\rCopy\rPiCam and Movement Test For this test, I created a new virtual environment:\ncd ugv_rpi/tutorial_en/12 python -m venv --system-site-packages picam source picam/bin/activate pip install PyYAML\rCopy\rAfter that, copy the config.yaml and base_ctrl.py files into this folder and edit the config.yaml to disable unused sensors:\nbase_config: ... use_lidar: false extra_sensor: false\rCopy\rThe following simple index.html file provides an interface for controlling the rover:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Camera Stream\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { display: flex; justify-content: space-between; align-items: center; height: 100vh; margin: 0; padding: 0; font-family: Arial, sans-serif; } .controls { display: flex; flex-direction: column; align-items: center; gap: 10px; } .controls button { width: 60px; height: 60px; font-size: 16px; border: none; border-radius: 5px; background-color: #007BFF; color: white; cursor: pointer; } .controls button:hover { background-color: #0056b3; } .video-container { flex-grow: 1; display: flex; justify-content: center; align-items: center; } .middle-button { margin: 10px 0; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;controls\u0026#34;\u0026gt; \u0026lt;button onclick=\u0026#34;sendCommand(\u0026#39;up\u0026#39;)\u0026#34;\u0026gt;Up\u0026lt;/button\u0026gt; \u0026lt;div\u0026gt; \u0026lt;button onclick=\u0026#34;sendCommand(\u0026#39;left\u0026#39;)\u0026#34;\u0026gt;Left\u0026lt;/button\u0026gt; \u0026lt;button class=\u0026#34;middle-button\u0026#34; onclick=\u0026#34;sendCommand(\u0026#39;stop\u0026#39;)\u0026#34;\u0026gt;Stop\u0026lt;/button\u0026gt; \u0026lt;button onclick=\u0026#34;sendCommand(\u0026#39;right\u0026#39;)\u0026#34;\u0026gt;Right\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button onclick=\u0026#34;sendCommand(\u0026#39;down\u0026#39;)\u0026#34;\u0026gt;Down\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;video-container\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ url_for(\u0026#39;video_feed\u0026#39;) }}\u0026#34; alt=\u0026#34;Camera Stream\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; function sendCommand(direction) { let command; switch(direction) { case \u0026#39;up\u0026#39;: command = {\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:0.2,\u0026#34;R\u0026#34;:0.2}; break; case \u0026#39;down\u0026#39;: command = {\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:-0.2,\u0026#34;R\u0026#34;:-0.2}; break; case \u0026#39;left\u0026#39;: command = {\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:-0.3,\u0026#34;R\u0026#34;:0.3}; break; case \u0026#39;right\u0026#39;: command = {\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:0.3,\u0026#34;R\u0026#34;:-0.3}; break; case \u0026#39;stop\u0026#39;: command = {\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:0,\u0026#34;R\u0026#34;:0}; break; } // Send the command to the server fetch(\u0026#39;/send_command\u0026#39;, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify(command) }); } ‚Ä¶","date":"2024-09-28","permalink":"https://seehiong.github.io/posts/2024/09/wave-rover-with-raspberry-pi-4/","summary":"This post documents using a Raspberry Pi 4 as the main controller for the Wave Rover project. It covers installing Raspberry Pi OS, setting up the ‚Ä¶","tags":["Wave Rover","Robotics","Raspberry","UGV_RPI","JupyterLab","RealVNC","PiCam"],"title":"Wave Rover with Raspberry Pi 4"},{"content":"After a long wait, my Wave Rover\r, a robust 4WD full-metal body mobile robot chassis, and the RoArm-M2-S\r, a 4DOF smart robotic arm designed for innovative applications, have finally arrived. Along with these, I received three 18650 lithium batteries (which are not included in the base package) for powering the setup.\nInitial Preparation To begin, I carefully opened the Wave Rover chassis by loosening the four screws on the bottom with an allen key, ensuring not to pull any wires accidentally.\nNext, I installed the three 18650 batteries into the compartment. It‚Äôs essential to ensure that the battery polarity is correct to avoid damaging the electronics.\nAfter assembling the RoArm-M2-S onto the chassis, this was my initial setup:\nWarning\rHowever, due to the high center of gravity (CG), if the Wave Rover moves too fast or experiences sudden jerks, the robotic arm tends to topple over easily. As a solution, I removed the arm and decided to focus on testing the Wave Rover itself. In the future, I plan to either design a larger, more stable base with counterweights or 3D print a custom base. Another approach might be to limit the acceleration and deceleration of the rover to stabilize the arm during movement.\rGetting Started with Wave Rover Since the Wave Rover supports charging while in use, after powering it on, I connected to its Wi-Fi network named UGV (default password: 12345678). The control panel can be accessed by navigating to http://192.168.4.1 in a web browser.\nOnce connected, I could control the rover using the on-screen buttons or my keyboard\u0026rsquo;s arrow keys.\nIssue JSON Commands via the Web Interface The rover‚Äôs control system supports various commands in JSON format. For example, the left and right wheel speeds (CMD_SPEED_CTRL) can be controlled within a range from -0.5 to 0.5, where positive values move the rover forward and negative values move it backward. Since the motors don‚Äôt have encoders, a value of 0.5 represents 100% PWM.\n{\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:0.1,\u0026#34;R\u0026#34;:0.1}\rCopy\rThere are several other commands available in the web application. To connect the rover to another Wi-Fi network, use the following command:\n{\u0026#34;T\u0026#34;:404,\u0026#34;ap_ssid\u0026#34;:\u0026#34;UGV\u0026#34;,\u0026#34;ap_password\u0026#34;:\u0026#34;12345678\u0026#34;,\u0026#34;sta_ssid\u0026#34;:\u0026#34;your_ssid\u0026#34;,\u0026#34;sta_password\u0026#34;:\u0026#34;password\u0026#34;}\rCopy\rAfter this, the rover can be accessed via the new IP address, which is displayed on its OLED screen.\nControlling Wave Rover via Python For those who prefer scripting, the following Python script can issue HTTP-based JSON commands to the rover:\nimport requests import argparse def main(): parser = argparse.ArgumentParser(description=\u0026#39;Http JSON Communication\u0026#39;) parser.add_argument(\u0026#39;ip\u0026#39;, type=str, help=\u0026#39;IP address: 192.168.10.104\u0026#39;) args = parser.parse_args() ip_addr = args.ip try: while True: command = input(\u0026#34;input your json cmd: \u0026#34;) url = \u0026#34;http://\u0026#34; + ip_addr + \u0026#34;/js?json=\u0026#34; + command response = requests.get(url) content = response.text print(content) except KeyboardInterrupt: pass if __name__ == \u0026#34;__main__\u0026#34;: main()\rCopy\rTo run the script, simply execute the following command, replacing the IP with your rover\u0026rsquo;s IP:\n# Enter your IP address displayed in the OLED screen python http_simple_ctrl.py 192.168.68.108\rCopy\rHere are a few commands you can try:\n// To move backwards {\u0026#34;T\u0026#34;:1,\u0026#34;L\u0026#34;:-0.1,\u0026#34;R\u0026#34;:-0.1} // To show test in the OLED screen {\u0026#34;T\u0026#34;:3,\u0026#34;lineNum\u0026#34;:0,\u0026#34;Text\u0026#34;:\u0026#34;putYourTextHere\u0026#34;} // To reset back to the initial OLED display {\u0026#34;T\u0026#34;:-3}\rCopy\rProgramming the Driver Board for the Wave Rover For more advanced control, follow the official guide on How to install Arduino IDE\rand download the Arduino IDE\r.\nInstalling the ESP32 Plug-in Start the Arduino IDE and navigate to File \u0026gt; Preferences. In the \u0026ldquo;Additional Boards Manager URLs\u0026rdquo; field, enter the following URL: https://dl.espressif.com/dl/package_esp32_index.json\r: Download the ESP32 development package\rand extract it to C:\\Users\\username\\AppData\\Local\\Arduino15.\nNext, download the necessary libraries\rand place them in the libraries folder under C:\\Users\\username\\Documents\\Arduino.\nUploading the Wave Rover Demo Download the Wave Rover Open-source Demo\r, unzip it, and open the WAVE_ROVER_V0.9.ino file in the Arduino IDE.\nConnect the Wave Rover driver board to your computer via USB-C:\nSelect the appropriate COM port (e.g., COM6), which should appear once the board is connected: Under Tools \u0026gt; Board \u0026gt; esp, select the ESP32 Dev Module: Finally, upload the demo to the rover. Once uploaded, the setup() function is executed once for initialization, while the loop() function runs continuously to handle real-time interactions. However, after rebooting, I noticed that the rover did not respond to any web server commands, which I\u0026rsquo;ll address in the reset section.\nMotor Control Demo (Without Encoder) Below is the content of the ‚Ä¶","date":"2024-09-21","permalink":"https://seehiong.github.io/posts/2024/09/wave-rover-setup-guide-and-troubleshooting-tips/","summary":"This guide walks you through setting up the Wave Rover, including hardware connections and software configuration. It covers common issues like ‚Ä¶","tags":["Wave Rover","RoArm-M2-S","Robotics","Arduino","ESP32","PWM"],"title":"Wave Rover: Setup Guide and Troubleshooting Tips"},{"content":"In this post, I will walk you through the process of setting up a Kubernetes cluster using Talos Linux\r, an operating system specifically designed for Kubernetes that is secure, immutable, and minimal by design. Talos Linux is distinguished by its unique architecture: it is hardened by default, has no shell (bash), no SSH access, and no systemd. Instead, all management is conducted through an API.\nPreparation After downloading the ISO\rimage, I used balenaEtcher\rto create a bootable USB installation media. My setup consists of one control plane node and two worker nodes. The following IP addresses were assigned:\n192.168.68.115 - control-1 192.168.68.117 - worker-1 192.168.68.118 - worker-2\rCopy\rInstallation Installing Talos Linux CLI To install the Talos Linux CLI on my Windows machine, I used the following command:\nchoco install talosctl\rCopy\rSome commands need to be executed from within WSL, so I installed it there with:\ncurl -sL https://talos.dev/install | sh\rCopy\rSetting Up the Talos Linux Control Node To create a cluster named talosk8s, run the following command:\ntalosctl gen config talosk8s https://192.168.68.115:6443 # Sample output generating PKI and tokens created C:\\Users\\sh\\controlplane.yaml created C:\\Users\\sh\\worker.yaml created C:\\Users\\sh\\talosconfig\rCopy\rTo identify the installation disk, use:\ntalosctl disks --insecure -n 192.168.68.115\rCopy\rHere is my control-1.patch file:\n# control-1.patch machine: network: hostname: control-1 install: disk: /dev/nvme0n1 # The disk used for installations. image: ghcr.io/siderolabs/installer:v1.7.6 # Allows for supplying the image used to perform the installation. wipe: true # Indicates if the installation disk should be wiped at installation time.\rCopy\rFrom within WSL, I executed the following commands:\n# The CLI tool patches the original controlplane.yaml and outputs a specific control file for the node talosctl machineconfig patch controlplane.yaml --patch @control-1.patch --output control-1.yaml # Apply the configuration to the control node talosctl apply-config --insecure -n 192.168.68.115 --file control-1.yaml\rCopy\rOnce etcd is up and waiting to join the cluster, issue the bootstrap command:\ntalosctl bootstrap --nodes 192.168.68.115 --endpoints 192.168.68.115 --talosconfig talosconfig\rCopy\rManually copy the contents of talosconfig to ~/.talos/config or %USERPROFILE%/.talos/config and update the IP address from 127.0.0.1 to 192.168.68.115:\ncontext: talosk8s contexts: talosk8s: endpoints: - 192.168.68.115 ca: LS0tLS1xxx== crt: LS0tLS1xxx key: LS0tLS1xxx\rCopy\rTo use kubectl or k9s, merge the configuration into the default ~/.kube/config or %USERPROFILE%/.kube/config file with:\ntalosctl kubeconfig -n 192.168.68.115\rCopy\rYou can access the dashboard at any time with:\ntalosctl dashboard -n 192.168.68.115\rCopy\rAdding Talos Linux Worker Nodes Next, let\u0026rsquo;s add the worker nodes. Below is the worker-1.patch file:\n# worker-1.patch machine: network: hostname: worker-1 install: disk: /dev/sdb # The disk used for installations. image: ghcr.io/siderolabs/installer:v1.7.6 # Allows for supplying the image used to perform the installation. wipe: true # Indicates if the installation disk should be wiped at installation time. Copy\rNote\rThe installation disk can be identified using talosctl disks \u0026ndash;insecure -n 192.168.68.117. Repeat this for worker-2.patch by updating its IP address and installation disk.\rWithin WSL, I executed these commands:\n# The CLI tool patches the original worker.yaml and outputs the specific worker file for each node talosctl machineconfig patch worker.yaml --patch @worker-1.patch --output worker-1.yaml talosctl machineconfig patch worker.yaml --patch @worker-2.patch --output worker-2.yaml # Apply the configuration to the corresponding worker nodes talosctl apply-config --insecure -n 192.168.68.117 --file worker-1.yaml talosctl apply-config --insecure -n 192.168.68.118 --file worker-2.yaml\rCopy\rTo access the dashboard for each worker node:\ntalosctl dashboard -n 192.168.68.117 talosctl dashboard -n 192.168.68.118\rCopy\rTo check the version, use:\ntalosctl version -n 192.168.68.117 -e 192.168.68.117 # Sample output Client: Tag: v1.7.6 SHA: ae67123a Built: Go version: go1.22.5 OS/Arch: linux/amd64 Server: NODE: 192.168.68.117 Tag: v1.7.6 SHA: ae67123a Built: Go version: go1.22.5 OS/Arch: linux/amd64 Enabled: RBAC\rCopy\rConfiguring Local Storage Configuring hostPath Mounts Following the official Local Storage Guide\r, here is my local-storage-patch.yaml file:\nmachine: kubelet: extraMounts: - destination: /var/mnt type: bind source: /var/mnt options: - bind - rshared - rw\rCopy\rApply this machine configuration to all nodes:\n# Control node talosctl patch mc -p @local-storage-patch.yaml -e 192.168.68.115 -n 192.168.68.115 # All worker nodes talosctl patch mc -p @local-storage-patch.yaml -e 192.168.68.117 -n 192.168.68.117 talosctl patch mc -p @local-storage-patch.yaml -e 192.168.68.118 -n 192.168.68.118\rCopy\rLocal Path Provisioner The Local Path ‚Ä¶","date":"2024-09-01","permalink":"https://seehiong.github.io/posts/2024/09/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/","summary":"This post guides you through setting up a secure, immutable Kubernetes cluster using Talos Linux. It covers installing Talos on control and worker ‚Ä¶","tags":["K8s Dashboard","K8s","NFS","OpenEBS","Talos","HomeLab","WSL"],"title":"Talos Linux: Setting Up a Secure, Immutable Kubernetes Cluster"},{"content":"Building on my exploration of text generation with NVIDIA Jetson Orin NX\r, this post delves into the audio generation capabilities of the Jetson platform.\nTranscribing Audio with Whisper Following the Tutorial Whisper\r, after starting the container with the command below, you can access Jupyter Lab at https://192.168.68.100:8888 (password: nvidia):\njetson-containers run $(autotag whisper)\rCopy\rInstead of recording my own audio, I used the Free Transcription Example Files\r.\nAfter downloading the necessary models, here is the transcription output for the first 30 seconds:\nText LLM and ASR/TTS with Llamaspeak To start Llamaspeak with text LLM and ASR/TTS enabled, use the following command. Make sure your Hugging Face token is correctly set; I do this by adding HF_TOKEN=hf_xyz123abc456 to my .bashrc file.\nStart the nano_llm with:\njetson-containers run --env HUGGINGFACE_TOKEN=$HF_TOKEN \\ $(autotag nano_llm) \\ python3 -m nano_llm.agents.web_chat --api=mlc \\ --model meta-llama/Meta-Llama-3-8B-Instruct \\ --asr=riva --tts=piper\rCopy\rYour browser does not seem to support the HTML5 video tag. You can download the video instead.\rThat\u0026rsquo;s all for this post. I‚Äôll continue my journey with the Two Days to a demo\rseries, an introductory set of deep learning tutorials for deploying AI and computer vision in the field using NVIDIA Jetson Orin!\nOptional - Preparing RIVA Server Following the Speech AI Tutorial\r, you will need to sign up for NGC, generate an API key, and configure the NGC setup. Here are the commands to get started:\nsudo gedit /etc/docker/daemon.json # Add the line: ‚Äúdefault-runtime‚Äù: ‚Äúnvidia‚Äù sudo systemctl restart docker # Add your user to the Docker group sudo usermod -aG docker $USER newgrp docker # Download the NGC CLI wget --content-disposition https://ngc.nvidia.com/downloads/ngccli_arm64.zip \u0026amp;\u0026amp; unzip ngccli_arm64.zip \u0026amp;\u0026amp; chmod u+x ngc-cli/ngc find ngc-cli/ -type f -exec md5sum {} + | LC_ALL=C sort | md5sum -c ngc-cli.md5 # Add the NGC CLI to your PATH in .bashrc and source it export PATH=$PATH:/home/pi/ngc-cli # Configure NGC ngc config set # Download RIVA Quickstart ngc registry resource download-version nvidia/riva/riva_quickstart_arm64:2.12.0 cd riva_quickstart_arm64_v2.12.0 sudo bash riva_init.sh # Start the RIVA server in a Docker container bash riva_start.sh\rCopy\rThis section was intended for Llamaspeak, but due to a CUDA driver version error, I was unable to proceed with the speech testing. The error occurred when running the command docker logs riva-speech:\n","date":"2024-08-25","permalink":"https://seehiong.github.io/posts/2024/08/audio-generation-with-nvidia-jetson-orin-nx/","summary":"This post explores the audio generation capabilities of the NVIDIA Jetson Orin NX. It covers transcribing audio using Whisper, setting up ‚Ä¶","tags":["NVIDIA","Jetson","Orin NX","Whisper","Llamaspeak","RIVA","LLM"],"title":"Audio Generation with NVIDIA Jetson Orin NX"},{"content":"Navigating through the NVIDIA Jetson AI Lab\rhas been an exhilarating experience, showcasing the potential of generative AI powered by NVIDIA¬Æ Jetson‚Ñ¢. With a plethora of labs to explore, it‚Äôs challenging to cover everything in a limited time. In this post, I\u0026rsquo;ll focus on labs related to text generation.\nPreparation If you follow my Jetson Orin NX flashing guide\r, you might have noticed that a browser is not pre-installed. I recommend installing Brave\r, a browser that blocks ads and conserves data. To install it, simply run:\nsudo snap install brave\rCopy\rNote\rWhen running nvidia-smi, you might expect to see a GPU listed, but instead, you\u0026rsquo;ll find none. This is because the Jetson devices use an integrated GPU (iGPU) that connects directly to the memory controller. For monitoring GPU usage, I recommend using the built-in Jetson Power GUI.\nText Generation - WebUI Following the guide on text-generation-webui\r, clone the repository to utilize the utilities that will automatically pull and start the appropriate container:\ngit clone https://github.com/dusty-nv/jetson-containers bash jetson-containers/install.sh jetson-containers run $(autotag text-generation-webui)\rCopy\rLLama-2-7b-Chat-GGUF To start, I used TheBloke/Llama-2-7b-Chat-GGUF with the single-file model quantization llama-2-7b-chat.Q4_K_M.gguf:\nAfter downloading the model, I loaded it:\nOnce set up, I was able to start chatting locally. On my Orin NX 16G, the typical rate was 4.15 tokens per second:\nSheared-LLaMa-1-3b-ShareGPT Next, I experimented with princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT and set the Model Loader to Transformers using the same input prompt:\nOn average, this setup yielded a higher inference rate of around 8.48 tokens per second without compromising chat quality.\nInfo\rSheared-LLaMA-1.3B\ris a model pruned and further pre-trained from meta-llama/Llama-2-7b-hf.\rJetson Examples The jetson-examples\rrepository by Seed Studio offers a straightforward, one-line command deployment for running Vision AI and Generative AI models on the NVIDIA Jetson platform.\nTo install the package, run:\npip3 install jetson-examples\rCopy\rBefore proceeding to next section, add the required path to .bashrc file:\nexport PATH=/home/pi/.local/bin:$PATH\rCopy\rText (LLM) - Llama3 To run Llama3, use the one-line command:\nreComputer run llama3\rCopy\rHere is an example of the container in action:\nText (LLM) - Sheared-LLaMA-2.7B-ShareGPT For speed comparison, to run Sheared LLaMA, use the one-line command:\nreComputer run Sheared-LLaMA-2.7B-ShareGPT\rCopy\rHere is an another example of the container in action:\nInference Server - Ollama To run the Ollama inference server, use the command:\nreComputer run ollama\rCopy\rOnce the Ollama inference server is running, you can interact with it by executing commands such as ollama run llama3 to start a chat session:\nThere‚Äôs much more to explore, but I‚Äôll stop here for now. Enjoy your journey with Jetson!\n","date":"2024-08-17","permalink":"https://seehiong.github.io/posts/2024/08/unleashing-text-generation-with-nvidia-jetson-orin-nx/","summary":"This post explores the powerful capabilities of NVIDIA Jetson Orin NX for text generation tasks. It covers the setup and use of models like Llama 2 ‚Ä¶","tags":["NVIDIA","Jetson","Orin NX","Llama2","Llama3","Brave","Ollama"],"title":"Unleashing Text Generation with NVIDIA Jetson Orin NX"},{"content":"Building up my previous Kafka post\r, this article focuses on leveraging KEDA to scale Kafka consumer workloads dynamically. KEDA\ris a Kubernetes-based Event Driven Autoscaler that enables automatic scaling of pods based on the volume of events to be processed.\nKEDA Setup To begin, after accessing the mk8s-vm, simply install KEDA with the following commands:\nsudo microk8s enable community sudo microk8s enable keda # Disable KEDA if necessary sudo microk8s disable keda\rCopy\rPreparation Kafka Consumer Below is a sample kafka-consumer.yaml workload file (located in the newly created keda directory). This consumer will process messages from the serve-uuid Kafka topic as part of the kafka-consumer-group. Referring back to my Travel Portal Design\r, this could represent the Payment Service, where each UUID corresponds to a unique Order ID. In this example, each pod can handle only two unique order IDs and takes 20 seconds to complete processing.\napiVersion: apps/v1 kind: Deployment metadata: name: kafka-consumer namespace: kafka spec: replicas: 0 selector: matchLabels: app: kafka-consumer template: metadata: labels: app: kafka-consumer spec: containers: - name: kafka-consumer image: bitnami/kafka:latest command: - /bin/sh - -c - | kafka-console-consumer.sh \\ --bootstrap-server kafka.kafka.svc.cluster.local:9092 \\ --topic serve-uuid \\ --group kafka-consumer-group \\ --consumer-property security.protocol=SASL_PLAINTEXT \\ --consumer-property sasl.mechanism=PLAIN \\ --consumer-property sasl.jaas.config=\u0026#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026#39;user1\u0026#39; password=\u0026#39;password1\u0026#39;;\u0026#34; \\ --max-messages \u0026#39;2\u0026#39; \u0026amp;\u0026amp; \\ echo \u0026#34;Simulating order processing for 20 seconds...\u0026#34; \u0026amp;\u0026amp; \\ sleep 20 \u0026amp;\u0026amp; \\ echo \u0026#34;KEDA scaling down pod after cooldownPeriod...\u0026#34;\rCopy\rTo deploy the consumer:\nkubectl apply -f kafka-consumer.yaml\rCopy\rKafka Secret and Trigger Authentication Referring to the Apache Kafka Scaler\rdocumentation, I set up the Kafka authentication. Below is the keda-kafka-secrets.yaml file:\napiVersion: v1 kind: Secret metadata: name: keda-kafka-secrets namespace: kafka stringData: sasl: \u0026#34;plaintext\u0026#34; username: \u0026#34;user1\u0026#34; password: \u0026#34;password1\u0026#34;\rCopy\rAnd here is the keda-trigger-auth-kafka.yaml file:\napiVersion: keda.sh/v1alpha1 kind: TriggerAuthentication metadata: name: keda-trigger-auth-kafka-credential namespace: kafka spec: secretTargetRef: - parameter: sasl name: keda-kafka-secrets key: sasl - parameter: username name: keda-kafka-secrets key: username - parameter: password name: keda-kafka-secrets key: password\rCopy\rTo apply the secret and trigger authentication:\nkubectl apply -f keda-kafka-secrets.yaml kubectl apply -f keda-trigger-auth-kafka.yaml\rCopy\rKafka Scaled Object For this proof of concept, I started with zero replicas of the kafka-consumer pod. Referencing the official Scaling Deployments\rdocumentation, here is my kafka-scaledobject.yaml file:\napiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: kafka-scaledobject namespace: kafka spec: scaleTargetRef: name: kafka-consumer minReplicaCount: 0 maxReplicaCount: 10 pollingInterval: 1 # KEDA check queue length every polling interval cooldownPeriod: 120 # Seconds to wait before scaling resource back to 0 triggers: - type: kafka metadata: bootstrapServers: kafka.kafka.svc.cluster.local:9092 topic: serve-uuid consumerGroup: kafka-consumer-group lagThreshold: \u0026#34;1\u0026#34; authenticationRef: name: keda-trigger-auth-kafka-credential\rCopy\rTo deploy the scaled object:\nkubectl apply -f kafka-scaledobject.yaml\rCopy\rCreating a New Consumer Pod with KEDA First, create the topic named serve-uuid:\nNext, simulate the receipt of the first order from the frontend store:\nOnce the first order is received, KEDA will automatically scale up the kafka-consumer pod:\nHere is the log for the first order:\nAssuming each microservice pod can process two orders at a time:\nOnce the specified cooldownPeriod has elapsed, KEDA will scale the kafka-consumer pod back down to the defined minReplicaCount.\nScaling Under Heavy Load By sending multiple messages simultaneously (simulating heavy load), KEDA will check the Kafka serve-uuid topic every pollingInterval and scale horizontally to meet the load requirements.\nWith this setup, developers can continue creating their order or payment microservices without worrying about manual scaling. KEDA handles the horizontal scaling automatically, relieving developers of the burden of implementing scaling logic in their code!\nOptional - Kafka Scaled Job In addition to scaling Deployments, you can also scale your code as Kubernetes Jobs using a ScaledJob\r. Here\u0026rsquo;s a sample kafka-scaledjob.yaml file:\napiVersion: keda.sh/v1alpha1 kind: ScaledJob metadata: name: kafka-scaledjob namespace: kafka spec: jobTargetRef: parallelism: 5 # Max number of desired pods completions: 5 # Desired number of successfully finished pods template: spec: containers: - name: ‚Ä¶","date":"2024-08-10","permalink":"https://seehiong.github.io/posts/2024/08/scaling-kafka-workloads-with-keda-in-kubernetes/","summary":"This post demonstrates how to use KEDA, a Kubernetes-based Event Driven Autoscaler, to dynamically scale Kafka consumer workloads. Building on a ‚Ä¶","tags":["Multipass","MicroK8s","KEDA","Kafka","Kafka-UI","K8s"],"title":"Scaling Kafka Workloads with KEDA in Kubernetes"},{"content":"On this special Singapore National Day\r, I am delighted to extend my best wishes as we celebrate Singapore\u0026rsquo;s 59th birthday. Today, I\u0026rsquo;m particularly excited to share that I‚Äôve finally received the long-awaited NVIDIA¬Æ Jetson Orin‚Ñ¢ NX\rboard. I‚Äôll be documenting the flashing process for this powerful device. NVIDIA Jetson Orin\ris a game-changer, bringing next-generation products to life with the world‚Äôs most advanced embedded AI computers, perfect for generative AI, computer vision, and cutting-edge robotics.\nPreparation Since the reComputer J40 package does not include a power cord, I managed to find a Type G Power Cord\rfrom my storage. Although it comes equipped with a 128GB NVMe SSD, I opted to flash my own Kingston NVMe PCIe Gen 4.0 (2TB) SSD\r, even though it will run at a slightly reduced performance due to the reComputer J401 carrier board only supporting PCIe 3.0.\nTo prepare for the Jetson Orin flashing, I downloaded the Ubuntu 20.04.6 LTS ISO Image\rand used Rufus\rto burn the image onto a USB drive.\nInstallation Following the Install Jetson Software with SDK Manager\rguide, I downloaded the SDK Manager and installed it on the freshly set up Ubuntu 20.04 host:\nsudo apt install ./sdkmanager_2.1.0-11698_amd64.deb # Starts NVIDIA SDK Manager sdkmanager\rCopy\rAs this is a new installation, I followed the on-screen instructions to connect pins 9 and 10 (located on the backside of the board) to enter Force Recovery mode.\nNote\rPower off the device first, then connect its USB Type-C connector to the host computer. Short pins 9 (GND) and 10 (FC REC) together, power on the device, and then remove the connection to proceed with the flashing. For a smoother installation, I also connected the device to a monitor, network cable, keyboard, and mouse.\nUpon powering up the device, the Target Hardware section automatically recognized it as the Jetson Orin NX board.\nAfter logging in, I proceeded with downloading the Jetson Linux image:\nOnce the board was successfully flashed, I logged into the Jetson Linux desktop, opened a terminal, and checked the IP address with:\nip addr\rCopy\rFrom the Host computer, I changed the connection to Ethernet, entered the device\u0026rsquo;s IP address and proceeded to install the SDK components:\nFinally, the installation was completed successfully!\nTroubleshooting Jetson Orin NX Flashing Error In my case, I initially tried to use Ubuntu 22.04 as the host OS to install SDK Manager, but the flashing process failed.\nUltimately, I had to reformat my Host system and switch to Ubuntu 20.04, which resolved the issue.\n","date":"2024-08-09","permalink":"https://seehiong.github.io/posts/2024/08/exploring-nvidia-jetson-orin-nx-flashing-and-setup-guide/","summary":"In this guide, I document the process of setting up and flashing the NVIDIA Jetson Orin NX, a powerful embedded AI computer ideal for advanced ‚Ä¶","tags":["NVIDIA","Jetson","Orin NX"],"title":"Exploring NVIDIA Jetson Orin NX: Flashing and Setup Guide"},{"content":"My homelab is a playground for experimenting with various tools and setups. However, for Proof of Concept (POC) environments, a lightweight and portable setup is often more suitable. In this post, I will guide you through setting up a MicroK8s environment in a virtual machine using Multipass. This POC demonstrates how Kafka can be set up in this environment. Multipass\ris a CLI tool for launching and managing VMs on Windows, Mac, and Linux, simulating a cloud environment with support for cloud-init.\nPreparation After downloading Multipass for Windows, run the installer. Since I am using Windows Professional, I opted for the recommended Microsoft Hyper-V hypervisor.\nTo launch a virtual machine, use the following command:\nmultipass launch --name mk8s-vm --memory 16G --disk 40G # Open a shell to the VM multipass shell mk8s-vm # To delete the VM multipass stop mk8s-vm multipass delete mk8s-vm multipass purge\rCopy\rFollowing the instructions in Installing MicroK8s with multipass\r, use these commands:\n# Update VM instance sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y # Install MicroK8s \u0026amp; configure the network sudo snap install microk8s --classic --channel=1.30/stable sudo iptables -P FORWARD ACCEPT # Add the current user to the microk8s group \u0026amp; create a kubectl alias sudo usermod -a -G microk8s ubuntu sudo snap alias microk8s.kubectl kubectl # Reopen the shell exit multipass shell mk8s-vm # Start MicroK8s microk8s start\rCopy\rKafka Setup To install Kafka\r, an open-source distributed event streaming platform, I will set up a single-node Apache Kafka Raft (\rKRaft\r) which uses a new quorum controller service.\nFirst, install Helm\r: sudo snap install helm --classic helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update\rCopy\rPrepares MicroK8s for Kafka installation: # Enable hostpath storage for kafka microk8s enable hostpath-storage # Setup kubeconfig for Helm installation microk8s config \u0026gt; kubeconfig cp kubeconfig .kube/config chmod 700 .kube/config\rCopy\rReferring to the Bitnami package for Apache Kafka\rand Kafka charts values\r, here is my values.yaml file under a newly created kafka folder. Adjust it according to your requirements: replicaCount: 1 kafka: enabled: true replicaCount: 1 configurationOverrides: server: - \u0026#34;process.roles=controller,broker\u0026#34; - \u0026#34;node.id=0\u0026#34; - \u0026#34;controller.quorum.voters=0@localhost:9093\u0026#34; - \u0026#34;listeners=PLAINTEXT://:9092,CONTROLLER://:9093\u0026#34; - \u0026#34;inter.broker.listener.name=PLAINTEXT\u0026#34; - \u0026#34;advertised.listeners=PLAINTEXT://localhost:9092\u0026#34; - \u0026#34;listener.security.protocol.map=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT\u0026#34; - \u0026#34;controller.listener.names=CONTROLLER\u0026#34; - \u0026#34;log.dirs=/bitnami/kafka/data\u0026#34; - \u0026#34;num.network.threads=3\u0026#34; - \u0026#34;num.io.threads=8\u0026#34; - \u0026#34;socket.send.buffer.bytes=102400\u0026#34; - \u0026#34;socket.receive.buffer.bytes=102400\u0026#34; - \u0026#34;socket.request.max.bytes=104857600\u0026#34; - \u0026#34;num.partitions=3\u0026#34; - \u0026#34;num.recovery.threads.per.data.dir=1\u0026#34; - \u0026#34;offsets.topic.replication.factor=1\u0026#34; - \u0026#34;transaction.state.log.replication.factor=1\u0026#34; - \u0026#34;transaction.state.log.min.isr=1\u0026#34; - \u0026#34;log.retention.hours=168\u0026#34; - \u0026#34;log.segment.bytes=1073741824\u0026#34; - \u0026#34;log.retention.check.interval.ms=300000\u0026#34; - \u0026#34;zookeeper.connect=\u0026#34; - \u0026#34;group.initial.rebalance.delay.ms=0\u0026#34; extraConfig: | offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 sasl: enabledMechanisms: PLAIN client: users: - user1 passwords: \u0026#34;password1\u0026#34; controller: replicaCount: 1\rCopy\rNote\rKafka can only run in KRaft mode with SASL_PLAINTEXT. Additionally, the extraConfig is required to avoid errors (refer to the troubleshooting section for more details).\rCreate a single-node Kafka pod in KRaft mode with: kubectl create namespace kafka helm install kafka bitnami/kafka --namespace kafka --version 26.8.3 -f values.yaml # Uninstall Kafka helm uninstall kafka --namespace kafka\rCopy\rKafka-UI Next, I will install the UI for Apache Kafka using the Quick start\rguide:\ngit clone https://github.com/provectus/kafka-ui-charts.git cd kafka-ui-charts\rCopy\rBased on the output from the Kafka installation, here is my modified values.yaml:\nyamlApplicationConfig: kafka: clusters: - name: yaml bootstrapServers: kafka-controller-0.kafka-controller-headless.kafka.svc.cluster.local:9092 dynamicConfigEnabled: true properties: security.protocol: SASL_PLAINTEXT sasl.mechanism: PLAIN sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026#34;user1\u0026#34; password=\u0026#34;password1\u0026#34;; auth: type: disabled management: health: ldap: enabled: false service: type: NodePort nodePort: 30808\rCopy\rThis is the command to extract the password. For this post, I hard-coded it for convenience:\npassword=\u0026#34;$(kubectl get secret kafka-user-passwords --namespace kafka -o jsonpath=\u0026#39;{.data.client-passwords}\u0026#39; | base64 -d | cut -d , -f 1)\u0026#34; echo ‚Ä¶","date":"2024-08-03","permalink":"https://seehiong.github.io/posts/2024/08/setting-up-kafka-with-microk8s-and-multipass/","summary":"My homelab is a playground for experimenting with various tools and setups. However, for Proof of Concept (POC) environments, a lightweight and ‚Ä¶","tags":["Multipass","MicroK8s","Helm","Kafka","Kafka-UI","Homelab","KRaft"],"title":"Setting Up Kafka with MicroK8s and Multipass"},{"content":"\rKubeflow Pipelines (KFP)\ris a powerful platform for creating and deploying scalable machine learning (ML) workflows using Docker containers. It enables data scientists and ML engineers to author workflows in Python, manage and visualize pipeline runs, and efficiently utilize compute resources. KFP supports custom ML components, leverages existing ones, and ensures cross-platform portability with a platform-neutral IR YAML definition\r. In this post, I‚Äôll share my learnings about KFP v2.\npip list | grep kfp # Current versions kfp 2.8.0 kfp-pipeline-spec 0.3.0 kfp-server-api 2.0.5\rCopy\rCreating a Pipeline Referencing the Hello World\rsample code, here is the result of running the hello_world.py notebook:\nDownload the hello_world_pipeline.json to your local drive:\nRunning the Pipeline Following the official Run a Pipeline\rguide, I will run the pipeline by submitting it via the Upload pipeline button:\nNext, from the hello-world-pipeline pipelines, click on the Create experiment button.\nAfter a new run is triggered, select the pipeline and change the default pipeline-root according to your setup:\nminio://mlpipeline/v2/artifacts\rCopy\rHere is the result of the run:\nSequential Pipeline Referencing the Sequential\rsample code, here is the final run result:\nHere is a view of all the runs:\nShared Pipelines To run the shared pipleine, click on the Data passing in python components pipeline:\nSimilar to our own pipelines, after clicking on the Create experiment button, start a new run. Here is the sample run result:\nArtifacts As expected, the artifacts are stored in the default MinIO path:\nMinIO Browser To open the MinIO browser, you can do a port-forward and navigate to http://localhost:9000/. The default username is minio and password is minio123:\nkubectl port-forward -n kubeflow svc/minio-service 9000:9000\rCopy\rHere is the view from the MinIO Browser:\nK9s As managing the many Kubeflow pods can be overwhelming, I would like to introduce K9s\r, a terminal-based UI to interact with your K8s clusters.\nTo install K9s on Windows via Chocolatey, use the following command:\nchoco install k9s # To upgrade in elevated shell choco upgrade k9s\rCopy\rTo achieve the same MinIO browser port-forward, follow these steps:\nType : (colon) and enter svc as the resource to view all the services Type / (slash) and enter minio to perform a search for the service Press Enter to go to the pod view and type shift-f for the port-forward Note\rYou may have to copy the contents of .kube/config from your master node to your %USER%/.kube/config file\rSome other useful commands are:\nPress ctrl-a at any time to see all resources Press ? to see contextual help Press esc to return to the previous resource Press ctrl-c to quit K9s Optional - Run Pipeline from KFP SDK client To run a pipeline from the KFP SDK client, your notebook must have access to KFP. Referencing the XGBoost\rsample code, here is the result:\nHowever, there are some RBAC errors. When I click on the Experiment details link, here is the result:\nAnd here is the Run details result:\nTroubleshooting Failed to read a token from file If you encounter this error when running a pipeline from the KFP client (default location: /var/run/secrets/kubeflow/pipelines/token):\nPlease proceed to the next error for the fix.\nMissing Configurations - Allow access to Kubeflow Pipelines You might not see this configuration when creating a new notebook:\nReferencing the Connect API\r, you may need to use Kubeflow\u0026rsquo;s PodDefaults to inject the ServiceAccount token volume into your Pods.\nTo address this issue, pull the latest Kubeflow Manifest\rand apply the necessary configurations as follows:\ncd manifests kubectl apply -f tests/gh-actions/kf-objects/poddefaults.access-ml-pipeline.kubeflow-user-example-com.yaml\rCopy\rCould not find CSRF cookie XSRF-TOKEN in the request If you encounter this issue, it could mean that you are exposing Kubeflow over HTTP.\nWhile not recommended due to security risks, if this is for a home lab setup like mine, you may edit apps/jupyter/jupyter-web-app/upstream/base/params.env and set APP_SECURE_COOKIES to false as follows:\nJWA_APP_SECURE_COOKIES=false\rCopy\rYou may wish to change these as well:\nTWA_APP_SECURE_COOKIES=false # in apps/tensorboard/tensorboards-web-app/upstream/base/params.env VWA_APP_SECURE_COOKIES=false # in apps/volumes-web-app/upstream/base/params.env APP_SECURE_COOKIES=false # in contrib/kserve/models-web-app/overlays/kubeflow/kustomization.yaml\rCopy\r","date":"2024-07-20","permalink":"https://seehiong.github.io/posts/2024/07/building-your-first-kubeflow-pipeline-a-step-by-step-guide/","summary":"In this blog post, I guide you through creating and running your first Kubeflow pipeline. We\u0026rsquo;ll start with the \u0026ldquo;Hello World\u0026rdquo; ‚Ä¶","tags":["HomeLab","K3s","K8s","K9s","MinIO","Kubernetes","KFP","Kubeflow","MLOps","IR YAML","Chocolatey"],"title":"Building Your First Kubeflow Pipeline: A Step-by-Step Guide"},{"content":"As we are migrating away from Lucidchart\rto draw.io\r, a security-first diagramming for teams, I will be documenting the steps to integrate draw.io with GitLab.\nConfigure Diagrams.net Server Referencing the official Diagrams.net\rdocumentation, I run the diagrams.net container in Docker, using the following command:\ndocker run --rm --name=\u0026#34;draw\u0026#34; -p 8888:8080 -p 8443:8443 jgraph/drawio\rCopy\rTo make this setup permanent, place the above command into a script named launch.sh and set up cron job:\n# Makes the script exectable chmod +x launch.sh # Installs cron sudo apt install cron crontab -e\rCopy\rAdd the following to the cron file and save it:\n@reboot /home/pi/launch.sh\rCopy\rEnable Diagrams.net Integration Diagrams.net will be available via http://bee:8888 (where bee is the hostname of my GitLab server):\nAfter signing into GitLab as administrator, navigate to Admin Area. Go to Settings \u0026gt; General, scroll down to Diagrams.net, and enter the URL:\nCreate Diagrams with Diagrams.net From the project\u0026rsquo;s wiki page, create a new Wiki and click on the Add or Insert Diagram icon as shown below:\nAfter completing the design, click on the Save \u0026amp; Exit button in the top right corner.\nEdit Diagrams with Diagrams.net To edit a diagram from the Wiki page, click on the Edit button in the top right corner.\nSelect the SVG drawing, click on the Add or Insert Diagram icon to edit the diagram:\nConfigure PlantUML Following the PlantUML\rofficial guide, let\u0026rsquo;s run the PlantUML container in Docker using this command:\ndocker run -d --name plantuml -p 8005:8080 plantuml/plantuml-server:tomcat\rCopy\rTo check the status or remove the PlantUML container, you may run:\n# Check status docker inspect -f \u0026#39;{{.State.Status}}\u0026#39; plantuml # Stop container docker stop plantuml # Remove image docker rm plantuml\rCopy\rYou can create the Personas design by accessing the web interface at http://bee:8005 with the following:\n@startuml actor Traveler participant \u0026#34;Web/Mobile UI\u0026#34; as UI participant \u0026#34;Search Service\u0026#34; as Search participant \u0026#34;Order Service\u0026#34; as Order participant \u0026#34;Payment Service\u0026#34; as Payment Traveler -\u0026gt; UI: Search for flights/hotels UI -\u0026gt; Search: Search request Search -\u0026gt; Order: Check availability Order -\u0026gt; Search: Available options Search -\u0026gt; UI: Display search results Traveler -\u0026gt; UI: Proceed to payment UI -\u0026gt; Payment: Request payment details Payment -\u0026gt; UI: Provide payment information @enduml\rCopy\rTo integrate PlantUML to GitLab, navigate to Admin Area. Go to Settings \u0026gt; General, scroll down to PlantUML, and enter the URL:\nUpdate the design Wiki as shown:\nThat\u0026rsquo;s it! We have successfully integrated Diagrams.net and PlantUML with GitLab. Happy Diagramming!\nHigh Level Architecture Design This is the updated Microservice Design:\nPersonas Design For each of the different personas:\nTraveler @startuml\ractor Traveler\rparticipant \u0026#34;Web/Mobile UI\u0026#34; as UI\rparticipant \u0026#34;Search Service\u0026#34; as Search\rparticipant \u0026#34;Order Service\u0026#34; as Order\rparticipant \u0026#34;Payment Service\u0026#34; as Payment\rTraveler -\u0026gt; UI: Search for flights/hotels\rUI -\u0026gt; Search: Search request\rSearch -\u0026gt; Order: Check availability\rOrder -\u0026gt; Search: Available options\rSearch -\u0026gt; UI: Display search results\rTraveler -\u0026gt; UI: Proceed to payment\rUI -\u0026gt; Payment: Request payment details\rPayment -\u0026gt; UI: Provide payment information\r@enduml\rHotel Operator actor \u0026#34;Hotel Operator\u0026#34; as Operator\rparticipant \u0026#34;Web/Mobile UI\u0026#34; as UI\rparticipant \u0026#34;Inventory Service\u0026#34; as Inventory\rparticipant \u0026#34;Analytics Service\u0026#34; as Analytics\rOperator -\u0026gt; UI: Log in\rUI -\u0026gt; Operator: Display dashboard\rOperator -\u0026gt; UI: Manage inventory (CRUD)\rUI -\u0026gt; Inventory: Perform CRUD operations\rInventory -\u0026gt; UI: Confirmation/status\rOperator -\u0026gt; UI: Access analytics\rUI -\u0026gt; Analytics: Request insights\rAnalytics -\u0026gt; UI: Provide insights\rTravel Portal Manager actor \u0026#34;Travel Portal Manager\u0026#34; as Manager\rparticipant \u0026#34;Web/Mobile UI\u0026#34; as UI\rparticipant \u0026#34;Reporting Service\u0026#34; as Reporting\rparticipant \u0026#34;Analytics Service\u0026#34; as Analytics\rManager -\u0026gt; UI: Log in\rUI -\u0026gt; Manager: Display dashboard\rManager -\u0026gt; UI: Access sales data\rUI -\u0026gt; Reporting: Request performance reports\rReporting -\u0026gt; Analytics: Retrieve sales data\rAnalytics -\u0026gt; Reporting: Provide performance reports\rReporting -\u0026gt; UI: Display reports\rManager -\u0026gt; UI: Access analytics\rUI -\u0026gt; Analytics: Request sales predictions\rAnalytics -\u0026gt; Analytics: Analyze historical data\rAnalytics -\u0026gt; Analytics: Provide data insights\rAnalytics -\u0026gt; UI: Provide sales predictions\r","date":"2024-07-06","permalink":"https://seehiong.github.io/posts/2024/07/integrating-draw.io-and-plantuml-with-gitlab/","summary":"As we transition from Lucidchart to draw.io for team diagramming, this guide outlines the steps to integrate draw.io and PlantUML with GitLab. ‚Ä¶","tags":["HomeLab","Diagrams","Drawio","PlantUML","GitLab","Lucidchart"],"title":"Integrating Draw.io and PlantUML with GitLab"},{"content":"Expanding on my previous post on Kubeflow\r, I will explore KServe\r, a standard Model Inference Platform on Kubernetes built for highly scalable use cases.\nFirst KServe Endpoint Referencing KServe on Kubeflow with Istio-Dex\r, below is the sklearn.yaml configuration. Note the sidecar annotation, which instructs not to inject the istio sidecar. Without this annotation, you may encounter error (refer to the troubleshooting section):\napiVersion: \u0026#34;serving.kserve.io/v1beta1\u0026#34; kind: \u0026#34;InferenceService\u0026#34; metadata: name: \u0026#34;sklearn-iris\u0026#34; annotations: sidecar.istio.io/inject: \u0026#34;false\u0026#34; spec: predictor: model: modelFormat: name: sklearn storageUri: \u0026#34;gs://kfserving-examples/models/sklearn/1.0/model\u0026#34;\rCopy\rFrom the Kubeflow dashboard, navigate to KServe Endpoints, click on the New Endpoint button on the top right, and input the above configuration:\nHere is the overview of the sklearn-iris endpoint:\nAnd its corresponding Kubernetes deployment:\nFirst Prediction Using the dex_auth.py\r, input the following into your first notebook:\nimport re from urllib.parse import urlsplit import requests def get_istio_auth_session(url: str, username: str, password: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; Determine if the specified URL is secured by Dex and try to obtain a session cookie. WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported (we default to using `staticPasswords` if both are enabled) :param url: Kubeflow server URL, including protocol :param username: Dex `staticPasswords` or `LDAP` username :param password: Dex `staticPasswords` or `LDAP` password :return: auth session information \u0026#34;\u0026#34;\u0026#34; # define the default return object auth_session = { \u0026#34;endpoint_url\u0026#34;: url, # KF endpoint URL \u0026#34;redirect_url\u0026#34;: None, # KF redirect URL, if applicable \u0026#34;dex_login_url\u0026#34;: None, # Dex login URL (for POST of credentials) \u0026#34;is_secured\u0026#34;: None, # True if KF endpoint is secured \u0026#34;session_cookie\u0026#34;: None, # Resulting session cookies in the form \u0026#34;key1=value1; key2=value2\u0026#34; } # use a persistent session (for cookies) with requests.Session() as s: ################ # Determine if Endpoint is Secured ################ resp = s.get(url, allow_redirects=True) if resp.status_code != 200: raise RuntimeError( f\u0026#34;HTTP status code \u0026#39;{resp.status_code}\u0026#39; for GET against: {url}\u0026#34; ) auth_session[\u0026#34;redirect_url\u0026#34;] = resp.url # if we were NOT redirected, then the endpoint is UNSECURED if len(resp.history) == 0: auth_session[\u0026#34;is_secured\u0026#34;] = False return auth_session else: auth_session[\u0026#34;is_secured\u0026#34;] = True ################ # Get Dex Login URL ################ redirect_url_obj = urlsplit(auth_session[\u0026#34;redirect_url\u0026#34;]) # if we are at `/auth?=xxxx` path, we need to select an auth type if re.search(r\u0026#34;/auth$\u0026#34;, redirect_url_obj.path): ####### # TIP: choose the default auth type by including ONE of the following ####### # OPTION 1: set \u0026#34;staticPasswords\u0026#34; as default auth type redirect_url_obj = redirect_url_obj._replace( path=re.sub(r\u0026#34;/auth$\u0026#34;, \u0026#34;/auth/local\u0026#34;, redirect_url_obj.path) ) # OPTION 2: set \u0026#34;ldap\u0026#34; as default auth type # redirect_url_obj = redirect_url_obj._replace( # path=re.sub(r\u0026#34;/auth$\u0026#34;, \u0026#34;/auth/ldap\u0026#34;, redirect_url_obj.path) # ) # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST) if re.search(r\u0026#34;/auth/.*/login$\u0026#34;, redirect_url_obj.path): auth_session[\u0026#34;dex_login_url\u0026#34;] = redirect_url_obj.geturl() # else, we need to be redirected to the actual login page else: # this GET should redirect us to the `/auth/xxxx/login` path resp = s.get(redirect_url_obj.geturl(), allow_redirects=True) if resp.status_code != 200: raise RuntimeError( f\u0026#34;HTTP status code \u0026#39;{resp.status_code}\u0026#39; for GET against: {redirect_url_obj.geturl()}\u0026#34; ) # set the login url auth_session[\u0026#34;dex_login_url\u0026#34;] = resp.url ################ # Attempt Dex Login ################ resp = s.post( auth_session[\u0026#34;dex_login_url\u0026#34;], data={\u0026#34;login\u0026#34;: username, \u0026#34;password\u0026#34;: password}, allow_redirects=True, ) if len(resp.history) == 0: raise RuntimeError( f\u0026#34;Login credentials were probably invalid - \u0026#34; f\u0026#34;No redirect after POST to: {auth_session[\u0026#39;dex_login_url\u0026#39;]}\u0026#34; ) # store the session cookies in a \u0026#34;key1=value1; key2=value2\u0026#34; string auth_session[\u0026#34;session_cookie\u0026#34;] = \u0026#34;; \u0026#34;.join( [f\u0026#34;{c.name}={c.value}\u0026#34; for c in s.cookies] ) auth_session[\u0026#34;authservice_session\u0026#34;] = s.cookies.get(\u0026#34;authservice_session\u0026#34;) return auth_session\rCopy\rTo determine the cluster IP, use this command:\nCLUSTER_IP=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.clusterIP}\u0026#39;) echo $CLUSTER_IP\rCopy\rNext, in the notebook, enter the following:\nimport requests KUBEFLOW_ENDPOINT = \u0026#34;http://10.43.239.213\u0026#34; # Cluster or LoadBalancer IP and port KUBEFLOW_USERNAME = ‚Ä¶","date":"2024-06-30","permalink":"https://seehiong.github.io/posts/2024/06/setting-up-and-using-kserve-with-kubeflow/","summary":"In this post, we explore KServe, a model inference platform on Kubernetes designed for scalability. Building on our previous Kubeflow guide, we detail ‚Ä¶","tags":["Metallb","KServe","Kubeflow","K3s","K8s","HomeLab","MLOps","Istio","Gitlab","sklearn"],"title":"Setting Up and Using KServe with Kubeflow"},{"content":"The car inspection went well, and I will spend the rest of my half-day leave documenting the steps for setting up Kubeflow\r, the machine learning toolkit for kubernetes.\nPreparation Kustomize\rintroduces a template-free way to customize application configuration, simplifying the use of off-the-shelf application. The simplest way to get started is to download the precompiled binaries:\ncurl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash # Moves kustomize to a system-wide location sudo mv kustomize /usr/local/bin/\rCopy\rNext, pull the source code from the kubeflow manifests\r:\ngit clone https://github.com/kubeflow/manifests.git cd manifests\rCopy\rInstallation Install all official Kubeflow components and common services with the following command:\nwhile ! kustomize build example | kubectl apply -f -; do echo \u0026#34;Retrying to apply resources\u0026#34;; sleep 20; done\rCopy\rIt will take a while for everything to install. Here is the view from the Portainer dashboard after the installation is complete.\nOnce installed, you can access the Kubeflow Central Dashboard. On my Windows machine, I used this command:\nkubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\rCopy\rYou can visit http://localhost:8080, using the default email address user@example.com\rand default password 12341234.\nHere is the Kubeflow dashboard upon successful installation:\nNotebooks Let\u0026rsquo;s start by creating a new notebook named first-notebook. Select the image and leave the other settings as default:\nThe creation of the new notebook will take some time:\nTo execute the first Python command, enter the following in the first cell and then press shift + Enter:\nprint(\u0026#34;Hello World!\u0026#34;)\rCopy\rThat\u0026rsquo;s it! We have successfully created our first working notebook in Kubeflow:\nOptional - Add Test User To add a new test user to Kubeflow, you may restart by removing the current setup with these commands (or just proceed to the next):\n# Remain in this folder in subsequent steps cd manifest # Delete the entire installs kustomize build example | kubectl delete -f - # Delete the remaining namespace kubectl delete ns kubeflow-user-example-com # Run this in another terminal if deleting a particular namespace hangs kubectl get namespace \u0026#34;kubeflow-user-example-com\u0026#34; -o json \\ | tr -d \u0026#34;\\n\u0026#34; | sed \u0026#34;s/\\\u0026#34;finalizers\\\u0026#34;: \\[[^]]\\+\\]/\\\u0026#34;finalizers\\\u0026#34;: []/\u0026#34; \\ | kubectl replace --raw /api/v1/namespaces/kubeflow-user-example-com/finalize -f - while ! kustomize build example | kubectl apply -f -; do echo \u0026#34;Retrying to apply resources\u0026#34;; sleep 20; done\rCopy\rGenerate the password hash with this command:\n# Install passlib pip install passlib # Generate the password hash python3 -c \u0026#39;from passlib.hash import bcrypt; import getpass; print(bcrypt.using(rounds=12, ident=\u0026#34;2y\u0026#34;).hash(getpass.getpass()))\u0026#39;\rCopy\rAdd a password for this test user by updating the following file:\nvi common/dex/base/dex-passwords.yaml\rCopy\rNext, add the new user by updating the file common/dex/base/config-map.yaml with the following:\n- email: test@example.com hashFromEnv: DEX_TEST_PASSWORD username: test\rCopy\rFinally, update the default setting in apps/centraldashboard/upstream/base/params.env with the following:\nCD_REGISTRATION_FLOW=true\rCopy\rReinstall Kubeflow with the same command:\n# Execute these to replace the defaults (otherwise test@example.com will not be recognised): kubectl apply -f common/dex/base/dex-passwords.yaml -n auth kubectl apply -f common/dex/base/config-map.yaml -n auth kubectl rollout restart deployment dex -n auth\rCopy\rFrom Windows, use the same port forwarding command to log in as the newly created test user and grant access:\nWith the registration flow enabled, you will be greeted with:\nAnd this:\nThat\u0026rsquo;s all there is to it! Now you have your own new test user namespace.\nTroubleshooting Too many open files error If you see a too many open files error, such as:\nIn the admission-webhook-deployment pod: In the ml-pipeline pod: In the training-operator pod: You can temporarily increase the max_user_instances and max_user_watches settings and then redeploy the affected pods (\rreference\r):\n# default returns a value of 128 cat /proc/sys/fs/inotify/max_user_instances echo 2280 | sudo tee /proc/sys/fs/inotify/max_user_instances # default returns a value of around 121865, depending on the current ubuntu host cat /proc/sys/fs/inotify/max_user_watches echo 1255360 | sudo tee /proc/sys/fs/inotify/max_user_watches\rCopy\rFor a permanent fix, add these lines to /etc/sysctl.conf:\nfs.inotify.max_user_instances = 2280\rfs.inotify.max_user_watches = 1255360\rCopy\rNo Namespaces If you see no namespaces in your Kubeflow dashboard, it could be due to initial errors during installation. Here is my first Kubeflow dashboard without any namespaces:\nWithout namespaces, the LAUNCH button remains disabled when trying to create a new notebook.\nTo resolve this, I followed these ‚Ä¶","date":"2024-06-24","permalink":"https://seehiong.github.io/posts/2024/06/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/","summary":"In this post, I provide a comprehensive guide to setting up Kubeflow, a machine learning toolkit for Kubernetes. From initial preparation and ‚Ä¶","tags":["HomeLab","K3s","K83","Jupyter","Kubeflow","MLOps","Kustomize"],"title":"Setting Up Kubeflow on Kubernetes: A Step-by-Step Guide"},{"content":"In this post, I will explore MyScaleDB\r, an open-source, high-performance SQL vector database built on ClickHouse, and LlamaIndex\r, the leading data framework for building LLM applications.\nInstallation After installing VSCodium\ras my primary IDE, I proceeded with installing the Python extension via Marketplace Link\r. Next, I created the virtual environment\rusing venv:\n# Create the envrionment python -m venv myscaledb # Activate the environment myscaledb\\Scripts\\activate\rCopy\rThis is my requirements.txt:\nlangchain langchain_community pydantic clickhouse-connect llama-index llama-index-llms-ollama llama-index-vector-stores-myscale llama-index-embeddings-langchain\rCopy\rInstall all the requirement pakcages with:\npip install -r requirements.txt\rCopy\rPreparation After signing up an account with MyScale, click on the Clusters and from the Actions, select Connection Details:\nBuilding an RAG application By following closely to Build An Advanced RAG Application using MyScaleDB and LlamaIndex\r, this is my rag.py:\nfrom llama_index.llms.ollama import Ollama llm = Ollama(model=\u0026#34;llama3\u0026#34;) # NOTE: I added the send_receive_timeout and connect_timeout settings to fix the write operation timeout # SEE: https://clickhouse.com/docs/en/integrations/python#clickhouse-connect-driver-api import clickhouse_connect client = clickhouse_connect.get_client( host=\u0026#39;xxxxxx.myscale.com\u0026#39;, port=443, username=\u0026#39;xxxxxx\u0026#39;, password=\u0026#39;xxxxxx\u0026#39;, send_receive_timeout=600000, connect_timeout=600000 )\rCopy\rNext, download and load the data:\nimport requests url = \u0026#39;https://niketeam-asset-download.nike.net/catalogs/2024/2024_Nike%20Kids_02_09_24.pdf?cb=09302022\u0026#39; response = requests.get(url) with open(\u0026#39;Nike_Catalog.pdf\u0026#39;, \u0026#39;wb\u0026#39;) as f: f.write(response.content) from llama_index.core import SimpleDirectoryReader reader = SimpleDirectoryReader( input_files=[\u0026#34;Nike_Catalog.pdf\u0026#34;] ) documents = reader.load_data()\rCopy\rCategorize the data:\ndef analyze_and_assign_category(text): if \u0026#34;football\u0026#34; in text.lower(): return \u0026#34;Football\u0026#34; elif \u0026#34;basketball\u0026#34; in text.lower(): return \u0026#34;Basketball\u0026#34; elif \u0026#34;running\u0026#34; in text.lower(): return \u0026#34;Running\u0026#34; else: return \u0026#34;Uncategorized\u0026#34;\rCopy\rSince I am using Ollama, pull the embedding model with ollama pull mxbai-embed-large. This is how it is used:\nfrom langchain_community.embeddings import OllamaEmbeddings embeddings = OllamaEmbeddings(model=\u0026#34;mxbai-embed-large\u0026#34;) from llama_index.core import VectorStoreIndex, Settings Settings.embed_model = embeddings Settings.llm = llm\rCopy\rNext, creates an index:\nfrom llama_index.vector_stores.myscale import MyScaleVectorStore from llama_index.core import StorageContext for document in documents: category = analyze_and_assign_category(document.text) document.metadata = {\u0026#34;Category\u0026#34;: category} vector_store = MyScaleVectorStore(myscale_client=client) storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_documents( documents, storage_context=storage_context )\rCopy\rThis is a sample result with this command from SQL workspace:\nselect * from llama_index;\rCopy\rSimple Query To test it out with a simple query:\nquery_engine = index.as_query_engine() response = query_engine.query(\u0026#34;I want a few running shoes\u0026#34;) print(response)\rCopy\rThis is a sample response:\nFiltered Query The query engine is configured with the metadata filter:\n# Connect to the external vector store directly index = VectorStoreIndex.from_vector_store( vector_store=vector_store ) from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters query_engine = index.as_query_engine( filters=MetadataFilters( filters=[ ExactMatchFilter(key=\u0026#34;Category\u0026#34;, value=\u0026#34;Running\u0026#34;), ] ), similarity_top_k=2, vector_store_query_mode=\u0026#34;hybrid\u0026#34;, ) response = query_engine.query(\u0026#34;I want a few running shoes?\u0026#34;) print(response.source_nodes[0].text)\rCopy\rThis is a sample response:\nTroubleshooting Initially I wanted to self-host MyScaleDB\rwith this docker-compose.yaml:\nversion: \u0026#39;3.7\u0026#39; services: myscaledb: image: myscale/myscaledb:1.5 tty: true ports: - \u0026#39;8123:8123\u0026#39; - \u0026#39;9000:9000\u0026#39; - \u0026#39;8998:8998\u0026#39; - \u0026#39;9363:9363\u0026#39; - \u0026#39;9116:9116\u0026#39; networks: myscaledb_network: ipv4_address: 10.0.0.2 volumes: - C:\\myscaledb\\volumes\\data:/var/lib/clickhouse - C:\\myscaledb\\volumes\\log:/var/log/clickhouse-server - C:\\myscaledb\\volumes\\config\\users.d\\custom_users_config.xml:/etc/clickhouse-server/users.d/custom_users_config.xml deploy: resources: limits: cpus: \u0026#34;4.00\u0026#34; memory: 32Gb networks: myscaledb_network: driver: bridge ipam: driver: default config: - subnet: 10.0.0.0/24\rCopy\rand this custom_users_config.yaml:\n\u0026lt;clickhouse\u0026gt; \u0026lt;users\u0026gt; \u0026lt;default\u0026gt; \u0026lt;password\u0026gt;\u0026lt;/password\u0026gt; \u0026lt;networks\u0026gt; \u0026lt;ip\u0026gt;::1\u0026lt;/ip\u0026gt; \u0026lt;ip\u0026gt;127.0.0.1\u0026lt;/ip\u0026gt; \u0026lt;ip\u0026gt;10.0.0.0/24\u0026lt;/ip\u0026gt; \u0026lt;/networks\u0026gt; ‚Ä¶","date":"2024-06-15","permalink":"https://seehiong.github.io/posts/2024/06/building-advanced-rag-applications-with-myscaledb-and-llamaindex/","summary":"Explore how to build advanced Retrieval-Augmented Generation (RAG) applications using MyScaleDB and LlamaIndex. This guide covers the installation of ‚Ä¶","tags":["AI","Ollama","LlamaIndex","VSCodium","ClickHouse","MyScaleDB"],"title":"Building Advanced RAG Applications with MyScaleDB and LlamaIndex"},{"content":"In this post, I will document my journey into learning QGIS\r, a free and open-source geographic information system, with the goal of visualizing and calculating the total time required for me to deliver gifts to friends and relatives during special occasions.\nPreparation After installing QGIS 3.36 desktop version on my Windows PC, I proceeded to the QGIS Training Manual\r, to familiarize myself with the software.\nPlugins From the Plugins -\u0026gt; Manage and Install Plugins\u0026hellip; menu, I searched for and installed the QuickMapServices plugin.\nI then loaded the OpenStreetMap\rlayer within QGIS via Web -\u0026gt; QuickMapServices -\u0026gt; OSM -\u0026gt; OSM Standard. A new OSM Standard layer was automatically created.\nIn addition, I installed the ORS Tools plugin.\nPlaces Layer Next, I created a new shapefile layer from the menu, Layer -\u0026gt; Create Layer -\u0026gt; New Shapefile Layer\u0026hellip;, to mark the locations where my friends and relatives reside.\nAfter adding the points and changing their symbology, my map looked like this:\nORS Tools To use ORS Tools, you need to sign up for an account at openrouteservice\rand configure it. Alternatively, you can self-host Openrouteserivce\r, a highly customizable and performant routing service written in Java.\nFrom the Batch Jobs tab in the Directions section, I clicked on the Points (1 Layer) button and configured it as shown below:\nBy clicking the Run button, a new Directions layer was created. After adjusting the symbology and labels, I determined that 3 hours is needed to complete the entire trip!\nOptional : Export to Leaflet webmap If you wish to export this travel plan, you can install the qgis2web plugin. To export, navigate to Web -\u0026gt; qgis2map -\u0026gt; Create web map.\nAfter configuring the settings, click on the Export button. You may view my exported map here\r.\nConclusion In conclusion, while my primary goal was to use QGIS for planning gift deliveries, this powerful tool can be applied to various other use cases. For instance, businesses can utilize QGIS for optimizing delivery routes, urban planners can map and analyze infrastructure, and environmental scientists can track and manage natural resources. The flexibility and extensive plugin ecosystem of QGIS make it suitable for a wide range of applications beyond personal projects. Whether you\u0026rsquo;re managing logistics, conducting spatial analysis, or creating detailed maps, QGIS offers the tools you need to achieve your objectives efficiently.\n","date":"2024-06-08","permalink":"https://seehiong.github.io/posts/2024/06/planning-gift-deliveries-with-qgis/","summary":"In this post, I document my journey of using QGIS, a free and open-source geographic information system, to plan gift deliveries. I outline the steps ‚Ä¶","tags":["HomeLab","QGIS","Openrouteservice","OpenStreetMap","ORS Tools"],"title":"Planning Gift Deliveries With QGIS"},{"content":"Problem Statement In this post, I will follow the Vehicle Routing Problem\r(VRP) with a focus on the capacitated vehicle routing problem (CVRP) and utilizing an alternative Mixed Integer Programming (MIP) approach.\nRouting Model The main section of the program creates the index manager and the routing model.\n// Create Routing Index Manager RoutingIndexManager manager = new RoutingIndexManager(data.distanceMatrix.length, data.vehicleNumber, data.depot); // Create Routing Model RoutingModel routing = new RoutingModel(manager);\rCopy\rDistance Callback The code snippet initializes the transit callback to compute distances between nodes and sets up the arc costs for all vehicles in the VRP model to ensure that the solver considers the correct distances when optimizing the routes:\n// Create and register a transit callback final int transitCallbackIndex = routing.registerTransitCallback((long fromIndex, long toIndex) -\u0026gt; { // Convert from routing variable Index to user NodeIndex. int fromNode = manager.indexToNode(fromIndex); int toNode = manager.indexToNode(toIndex); return (long) data.distanceMatrix[fromNode][toNode]; }); // Define cost of each arc. routing.setArcCostEvaluatorOfAllVehicles(transitCallbackIndex);\rCopy\rDemand Callback and Capacity Constraints The solver requires a demand callback, which returns the demand at each location, and a dimension for the vehicle capacity constraints:\n// Add Capacity constraint. final int demandCallbackIndex = routing.registerUnaryTransitCallback((long fromIndex) -\u0026gt; { // Convert from routing variable Index to user NodeIndex. int fromNode = manager.indexToNode(fromIndex); return data.demands[fromNode]; }); routing.addDimensionWithVehicleCapacity(demandCallbackIndex, 0, // null capacity slack data.vehicleCapacities, // vehicle maximum capacities true, // start cumul to zero \u0026#34;Capacity\u0026#34;);\rCopy\rChanging the search strategy Since the routing solver does not always return the optimal solution, we can use a more advanced search strategy called guided local search, which enables escaping a local minimum. We can further tune our use case with other Routing Options\r.\n// Setting first solution heuristic. RoutingSearchParameters searchParameters = main.defaultRoutingSearchParameters() .toBuilder() .setFirstSolutionStrategy(FirstSolutionStrategy.Value.PATH_CHEAPEST_ARC) .setLocalSearchMetaheuristic(LocalSearchMetaheuristic.Value.GUIDED_LOCAL_SEARCH) .setTimeLimit(Duration.newBuilder().setSeconds(60).build()) .build();\rCopy\rData Model This is the data model:\n@Data @Builder @AllArgsConstructor @NoArgsConstructor @Serdeable.Serializable @Serdeable.Deserializable public class DataModel { double[][] distanceMatrix; long[] demands; long[] vehicleCapacities; int vehicleNumber; int depot; }\rCopy\rThe CVRP Service This is the Micronaut service for CVRP, with the printSolution method adapted from OR-Tools:\n@Singleton public class CVRPService { static { Loader.loadNativeLibraries(); } CVRPSolution solve(DataModel data) { // Create Routing Index Manager RoutingIndexManager manager = new RoutingIndexManager(data.distanceMatrix.length, data.vehicleNumber, data.depot); // Create Routing Model RoutingModel routing = new RoutingModel(manager); // Create and register a transit callback final int transitCallbackIndex = routing.registerTransitCallback((long fromIndex, long toIndex) -\u0026gt; { // Convert from routing variable Index to user NodeIndex. int fromNode = manager.indexToNode(fromIndex); int toNode = manager.indexToNode(toIndex); return (long) data.distanceMatrix[fromNode][toNode]; }); // Define cost of each arc. routing.setArcCostEvaluatorOfAllVehicles(transitCallbackIndex); // Add Capacity constraint. final int demandCallbackIndex = routing.registerUnaryTransitCallback((long fromIndex) -\u0026gt; { // Convert from routing variable Index to user NodeIndex. int fromNode = manager.indexToNode(fromIndex); return data.demands[fromNode]; }); routing.addDimensionWithVehicleCapacity(demandCallbackIndex, 0, // null capacity slack data.vehicleCapacities, // vehicle maximum capacities true, // start cumul to zero \u0026#34;Capacity\u0026#34;); // Setting first solution heuristic. RoutingSearchParameters searchParameters = main.defaultRoutingSearchParameters() .toBuilder() .setFirstSolutionStrategy(FirstSolutionStrategy.Value.PATH_CHEAPEST_ARC) .setLocalSearchMetaheuristic(LocalSearchMetaheuristic.Value.GUIDED_LOCAL_SEARCH) .setTimeLimit(Duration.newBuilder().setSeconds(60).build()) .build(); // Solve the problem. Assignment solution = routing.solveWithParameters(searchParameters); Map\u0026lt;Integer, String\u0026gt; vehicleMap = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; data.vehicleNumber; ++i) { long index = routing.start(i); long routeLoad = 0; String route = \u0026#34;\u0026#34;; while (!routing.isEnd(index)) { long nodeIndex = manager.indexToNode(index); routeLoad += data.demands[(int) nodeIndex]; route += nodeIndex + \u0026#34; Load(\u0026#34; + routeLoad + \u0026#34;) -\u0026gt; \u0026#34;; long previousIndex = index; index = ‚Ä¶","date":"2024-06-01","permalink":"https://seehiong.github.io/posts/2024/06/from-routing-models-to-mip-solving-capacitated-vehicle-routing-problem/","summary":"In this post, we delve into solving the Capacitated Vehicle Routing Problem (CVRP) by transitioning from traditional routing models to the advanced ‚Ä¶","tags":["Routing Model","MIP","Micronaut","VRP","CVRP","Backend","Java"],"title":"From Routing Models to MIP: Solving Capacitated Vehicle Routing Problem"},{"content":"Problem Statement In this post, I\u0026rsquo;ll tackle the Facility Location Problem\r, which involves deciding the optimal placement of facilities to minimize costs. Unlike my previous post on using a genetic algorithm for the TSP\r, I\u0026rsquo;ll utilize OR-Tools\rto solve this problem.\nDefining the solver Referencing the MIP example\r, I\u0026rsquo;ll solve the FLP using the MIP approach. Let\u0026rsquo;s place all solver codes in FLPService.java:\n// Create the linear solver with the SCIP backend. MPSolver solver = MPSolver.createSolver(\u0026#34;SCIP\u0026#34;); if (solver == null) { System.out.println(\u0026#34;Could not create solver SCIP\u0026#34;); return; }\rCopy\rDefining the variables Each facility \\( j \\) in the set of facilities \\( J \\quad j \\in J \\)\nA fixed setup cost \\( s_j \\quad s_j \\in \\mathbb{R}^+ \\) A capacity \\( c_j \\quad c_j \\in \\mathbb{Z}^+ \\) ‚Äã This is represented in Facility.java:\n@Data @Builder @AllArgsConstructor @NoArgsConstructor @Serdeable.Serializable public class Facility { double setupCost; int capacity; double x; double y; }\rCopy\rEach customer \\( i \\) in the set of customers \\( I \\quad i \\in I \\)\nA demand \\( d_i \\quad d_i \\in \\mathbb{Z}^+ \\) This is represented in Customer.java:\n@Data @Builder @AllArgsConstructor @NoArgsConstructor @Serdeable.Serializable public class Customer { int demand; double x; double y; }\rCopy\rThe delivery cost is proportional to the distance between the facility and the customer, \\( dist_{i,j} \\quad dist_{i,j} \\in \\mathbb{R}^+ \\).\ndouble[][] dist = new double[customerCount][facilityCount];\rCopy\rDefine decision variables:\n\\( f_j \\) for facility \\( j \\) \\( a_{i,j} \\) for assigning customer \\( i \\) to facility \\( j \\) // Decision Variables: f[j] is 1 if facility j is opened, otherwise 0 MPVariable[] f = solver.makeIntVarArray(numFacility,0.0, 1.0, \u0026#34;f_\u0026#34;); // Decision Variables: a[i][j] is 1 when customer i is assigned to facility j, otherwise 0 MPVariable[][] a = new MPVariable[numCustomer][numFacility]; for (int i = 0; i \u0026lt; numCustomer; i++) { a[i] = solver.makeIntVarArray(numFacility, 0.0, 1.0, \u0026#34;a_\u0026#34; + i); }\rCopy\rDefining the constraints Each customer must be served by exactly one facility: $$ \\sum_{j \\in J} a_{i,j} = 1 \\quad \\forall i \\in I $$\r// Customer constraint: Each customer is assigned to exactly one facility for (int i = 0; i \u0026lt; numCustomer; i++) { MPConstraint constraint = solver.makeConstraint(1.0, 1.0, \u0026#34;customer_\u0026#34; + i); for (int j = 0; j \u0026lt; numFacility; j++) { constraint.setCoefficient(a[i][j], 1.0); } }\rCopy\rEach facility must meet the demand of its assigned customers without exceeding its capacity: $$ \\sum_{i \\in I} (d_i \\cdot a_{i,j}) \\leq c_j \\cdot f_j \\quad \\forall j \\in J $$\r// Capacity constraint: Facility capacities for (int j = 0; j \u0026lt; numFacility; j++) { MPConstraint constraint = solver.makeConstraint(-MPSolver.infinity(), 0.0, \u0026#34;capacity_\u0026#34; + j); for (int i = 0; i \u0026lt; numCustomer; i++) { constraint.setCoefficient(a[i][j], customers[i].demand); } constraint.setCoefficient(f[j], -facilities[j].capacity); }\rCopy\rDefining the objective Minimize the total cost, including setup and delivery costs: $$ \\sum_{j \\in J} (f_j \\cdot s_j + \\sum_{i \\in I} a_{i,j} \\cdot dist_{i,j}) $$This is implemented as:\n// Objective function: minimize total cost (including setup costs and delivery cost) MPObjective objective = solver.objective(); for (int j = 0; j \u0026lt; numFacility; j++) { // Add setup cost for each facility objective.setCoefficient(f[j], facilities[j].setupCost); for (int i = 0; i \u0026lt; numCustomer; i++) { // Add delivery cost for assigning each customer to each facility objective.setCoefficient(a[i][j], dist[i][j]); } } objective.setMinimization();\rCopy\rDisplaying the solution // checks solver output progress solver.enableOutput(); // sets the max solving time in milliseconds solver.setTimeLimit(1200000); // calls the solver MPSolver.ResultStatus resultStatus = solver.solve(); // Check that the problem has a feasible solution if (resultStatus == MPSolver.ResultStatus.OPTIMAL || resultStatus == MPSolver.ResultStatus.FEASIBLE) { System.out.println(\u0026#34;Total cost: \u0026#34; + objective.value() + \u0026#34;\\n\u0026#34;); for (int j = 0; j \u0026lt; numFacility; ++j) { double toatlCost = facilities[j].setupCost; if (f[j].solutionValue() \u0026gt; 0.5) { double totalDemand = 0; for (int i = 0; i \u0026lt; numCustomer; ++i) { if (a[i][j].solutionValue() \u0026gt; 0.0) { totalDemand += customers[i].demand; toatlCost += dist[i][j]; } } System.out.println(\u0026#34;Facility: \u0026#34; + j + \u0026#34;, capacity: \u0026#34; + facilities[j].capacity + \u0026#34;, demand: \u0026#34; + totalDemand + \u0026#34;, cost: \u0026#34; + toatlCost); } } } else { System.out.println(\u0026#34;No solution found.\u0026#34;); }\rCopy\rThe input file Sample input file:\n3 8 100 100 1065.0 1065.0 100 100 1062.0 1062.0 100 500 0.0 0.0 50 1397.0 1397.0 50 1398.0 1398.0 75 1399.0 1399.0 75 586.0 586.0 80 900.0 900.0 80 910.0 910.0 90 1200.0 1200.0 90 1210.0 1210.0\rCopy\rWhere\nLines 2- 4 represent facilities (cost, capacity, coordinates) Lines 5 -12 represent ‚Ä¶","date":"2024-05-27","permalink":"https://seehiong.github.io/posts/2024/05/solving-facility-location-problem-with-or-tools-and-micronaut/","summary":"This post demonstrates solving the Facility Location Problem (FLP) using OR-Tools and Micronaut. It covers defining the solver, variables, ‚Ä¶","tags":["OR-Tools","MIP","Micronaut","FLP","Backend","BE","Java"],"title":"Solving Facility Location Problem with OR-Tools and Micronaut"},{"content":"Problem Statement Research suggests that the Optimal 85,900-Point Tour\ris the largest solved instance of the Traveling Salesman Problem (TSP). For this post, I will attempt to solve the TSP problem involving 200 cities.\nGenetic algorithms simulate the process of natural selection, mimicking \u0026ldquo;survival of the fittest\u0026rdquo; to solve problems. These algorithms evolve over generations, with each generation comprising individuals better adapted to solving the problem.\nReferencing Genetic Algorithm for TSP\r, I documented my solution steps.\nInitialisation I implemented the genome as an int[]. Here‚Äôs my Individual.java:\nimport io.micronaut.serde.annotation.Serdeable; import lombok.*; @Data @Builder @AllArgsConstructor @NoArgsConstructor @Serdeable.Serializable public class Individual { int[] genome; // sequence of cities to visit, starting with first city 0 double fitness; // total distance travelled String message; // additional information attached to the individual }\rCopy\rHere are the configuration parameters in TspGaService.java:\n// Configuration final int POPULATION_SIZE = 100; // Population size final int MAX_GENERATIONS = 1200; // Maximum number of generations final double INITIAL_TEMPERATURE = 1000; // Initial temperature to start with final double FINAL_TEMPERATURE = 1; // Terminate main loop when temperature reached final double COOLING_RATE = 0.9995; // Cooling rate final int FITNESS_MEMO_SIZE = 10000000; // Limit the size of the fitness memo (out of heap size, avoid OOM error) final int STAGNATION_THRESHOLD = 50; // Number of generations without improvement to trigger action final int LOCAL_SEARCH_ATTEMPTS = 50; // Number of attempts on local search before exiting local optima final int MUTATION_ATTEMPTS = 50; // Number of attempts on mutation int maxCities; double[][] graph; // Populate with actual distances double fitnessThreshold; // The desired fitness threshold int maxStagnationRetry = 10; ConcurrentHashMap\u0026lt;Integer, Double\u0026gt; fitnessMemo = new ConcurrentHashMap\u0026lt;\u0026gt;(); // Thread-safe memoization for fitness int generation = 1; DecimalFormat df = new DecimalFormat(\u0026#34;#\u0026#34;); int optimalCount = 0;\rCopy\rIn the same class, these helper methods were used:\nIndividual getIndividual(int[] genome) { return Individual.builder().genome(genome).fitness(calculateFitness(genome)).build(); } Individual getIndividual(int[] genome, double fitness) { return Individual.builder().genome(genome).fitness(fitness).build(); } // Random number generator function int randNumber(int start, int end) { return (int) (Math.random() * (end - start)) + start; } double calculateCityDistance(double x1, double y1, double x2, double y2) { double xDelta = (x1 - x2); double yDelta = (y1 - y2); return Math.sqrt((xDelta * xDelta) + (yDelta * yDelta)); } double calculateDistance(int[] genome) { double distance = 0; for (int i = 0; i \u0026lt; genome.length - 1; i++) { int from = genome[i]; int to = genome[i + 1]; distance += graph[from][to]; } //NOTE: This is distance needed to travel back to the original city distance += graph[genome[genome.length-1]][0]; return distance; } // Hash-based map for the fitness (total distance of the entire genome) double calculateFitness(int[] genome) { int genomeHash = Arrays.hashCode(genome); // Compute the hashcode of entire genome as key, to reduce memory usage if (fitnessMemo.containsKey(genomeHash)) { return fitnessMemo.get(genomeHash); } double totalDistance = calculateDistance(genome); if (fitnessMemo.size() \u0026lt; FITNESS_MEMO_SIZE) { fitnessMemo.put(genomeHash, totalDistance); } return totalDistance; } // Function to create a genome int[] createGenome() { int[] genome = new int[maxCities]; genome[0] = 0; Set\u0026lt;Integer\u0026gt; newGenome = new HashSet\u0026lt;\u0026gt;(); newGenome.add(0); int index = 1; while (newGenome.size() \u0026lt; maxCities) { int temp = randNumber(1, maxCities); if (!newGenome.contains(temp)) { genome[index++] = temp; newGenome.add(temp); } } return genome; } // Convenience method to reinit portion of population void reinitializePartOfPopulation(List\u0026lt;Individual\u0026gt; population) { int reinitializeCount = POPULATION_SIZE / 5; // Reinitialize 20% of the population for (int i = 0; i \u0026lt; reinitializeCount; i++) { int[] genome = createGenome(); population.set(randNumber(2, population.size()), getIndividual(genome)); // Replace random individuals (excluding the best two) } } List\u0026lt;Individual\u0026gt; initialPopulation() { List\u0026lt;Individual\u0026gt; population = Collections.synchronizedList(new ArrayList\u0026lt;\u0026gt;()); for (int i = 0; i \u0026lt; POPULATION_SIZE; i++) { population.add(getIndividual(createGenome())); } return population; }\rCopy\rLocal Search I implemented the 2-opt swap for local search to optimize the tour. My current approach emphasizes breaking the loop as soon as a better solution is found, rather than exhaustively checking all pairs, which would have a complexity of \\( O(n^2) \\).\ndouble calculateDelta(int[] genome, int i, int j) { // Calculate the difference in the tour length if the ‚Ä¶","date":"2024-05-23","permalink":"https://seehiong.github.io/posts/2024/05/optimizing-tsp-with-genetic-algorithms-in-micronaut/","summary":"In this post, I explore solving a Traveling Salesman Problem (TSP) involving 200 cities using genetic algorithms within a Micronaut framework. ‚Ä¶","tags":["Genetic Algorithm","Micronaut","TSP","BE","Backend","Java"],"title":"Optimizing TSP with Genetic Algorithms in Micronaut"},{"content":"Problem Statement I\u0026rsquo;m embarking on a journey to solve the Travelling Salesman Problem (TSP)\rutilizing Choco-solver\r, an effective Java library for constraint programming. Additionally, I\u0026rsquo;ll be converting this solution into an API using Micronaut\r, a cutting-edge, JVM-based framework for building modular, testable microservices and serverless applications.\nSetting up with Micronaut Following the official Micronaut guide\r, I downloaded the source and proceeded with the setup.\nDependencies Configuration I made adjustments to the dependencies in the build.gradle file to incorporate necessary libraries such as log4j, Lombok annotations, and Choco-solver.\ndependencies { annotationProcessor(\u0026#34;io.micronaut:micronaut-http-validation\u0026#34;) annotationProcessor(\u0026#34;io.micronaut.serde:micronaut-serde-processor\u0026#34;) annotationProcessor(\u0026#34;org.projectlombok:lombok\u0026#34;) implementation(\u0026#34;org.projectlombok:lombok\u0026#34;) implementation(\u0026#34;org.slf4j:slf4j-api\u0026#34;) implementation(\u0026#34;io.micronaut.serde:micronaut-serde-jackson\u0026#34;) implementation(\u0026#34;org.choco-solver:choco-solver:4.10.14\u0026#34;) compileOnly(\u0026#34;io.micronaut:micronaut-http-client\u0026#34;) runtimeOnly(\u0026#34;ch.qos.logback:logback-classic\u0026#34;) testImplementation(\u0026#34;io.micronaut:micronaut-http-client\u0026#34;) }\rCopy\rRunning micronaut By running the gradle task from Intellij or via command line, .\\gradlew run, this will be the expected result:\nWorking on the TSPSolver Input Definition In the TSPSolver.java, I began by defining the inputs required for the solver. This includes setting up variables to represent the tour, distances between cities, and the total distance traveled.\npublic static void main(String[] args) { TSPSolver solver = new TSPSolver(); int[][] distances = { {0, 10, 15, 20}, {10, 0, 35, 25}, {15, 35, 0, 30}, {20, 25, 30, 0}, }; solver.solveTSP(distances); // Solve the TSP instance }\rCopy\rNext the variables are being setup, where n is the total number of cities to be toured:\nIntVar[] tour = model.intVarArray(\u0026#34;tour\u0026#34;, n, 0, n - 1); // Tour representing the order of cities visited IntVar[] distance = model.intVarArray(\u0026#34;distance\u0026#34;, n, 0, 1000000); // Auxiliary variables for distances IntVar totalDistance = model.intVar(\u0026#34;totalDistance\u0026#34;, 0, 1000000); // Total distance traveled\rCopy\rConstraint Setup To ensure the tour adheres to the constraints of the TSP, I defined constraints using Choco-solver. This includes specifying the distances between cities and enforcing that the tour forms a single circuit, visiting each city exactly once.\nfor (int i = 0; i \u0026lt; n; i++) { Tuples tuples = new Tuples(true); // Create tuples to represent valid combinations of city and distance for (int j = 0; j \u0026lt; n; j++) { if (j != i) { tuples.add(j, distances[i][j]); // Add valid combinations of city and distance to tuples } } model.table(tour[i], distance[i], tuples).post(); // Apply table constraint for each city } // Ensure that the tour forms a single circuit, visiting each city exactly once model.subCircuit(tour, 0, model.intVar(n)).post(); }\rCopy\rSolver Configuration I configured the solver to search for the optimal solution using a FirstFail search strategy, prioritizing smaller values during the search process.\nSolver solver = model.getSolver(); solver.setSearch( Search.intVarSearch( new FirstFail(model), // Use FirstFail search strategy to select variables new IntDomainMin(), // Priorities smaller values from domain of integer variables during search distance));\rCopy\rModel Solving Upon solving the model, I iterated through the solution to extract the optimal tour and total distance traveled. This information was then logged for analysis.\nMap\u0026lt;Number, Integer[]\u0026gt; optimalSolution = new HashMap\u0026lt;\u0026gt;(); while (solver.solve()) { Integer[] optimalTour = new Integer[n+1]; int current = 0; // Start from the first city log.info(\u0026#34;City {}\u0026#34;, current); // Print the current city optimalTour[0] = current; for (int i = 1; i \u0026lt;= n; i++) { int next = tour[current].getValue(); // Get the next city in the tour optimalTour[i] = next; log.info(\u0026#34;City {}\u0026#34;, next); // Print the next city current = tour[current].getValue(); // Move to the next city } optimalSolution.put(totalDistance.getValue(), optimalTour); log.info(\u0026#34;\\nTotal tour distance: {}\u0026#34;, totalDistance.getValue()); // Print the total distance }\rCopy\rFinal TSPSolver This is my final TSPSolver.java:\npackage example.micronaut; import jakarta.inject.Singleton; import lombok.extern.slf4j.Slf4j; import org.chocosolver.solver.Model; import org.chocosolver.solver.Solver; import org.chocosolver.solver.constraints.extension.Tuples; import org.chocosolver.solver.search.strategy.Search; import org.chocosolver.solver.search.strategy.selectors.values.IntDomainMin; import org.chocosolver.solver.search.strategy.selectors.variables.FirstFail; import org.chocosolver.solver.variables.IntVar; import java.util.HashMap; import java.util.Map; @Slf4j @Singleton public class TSPSolver { ‚Ä¶","date":"2024-04-27","permalink":"https://seehiong.github.io/posts/2024/04/efficient-tsp-solver-api-with-micronaut/","summary":"Solve the Travelling Salesman Problem using Choco-solver and convert it into a powerful API with Micronaut. Explore the efficient solution and its ‚Ä¶","tags":["Choco-solver","Micronaut","TSP","BE","Backend","Java"],"title":"Efficient TSP Solver API with Micronaut"},{"content":"In this blog post, I\u0026rsquo;ll guide you through the setup of Graylog\r, an open-source log management platform, within a HomeLab environment, providing a comprehensive solution for log analysis and monitoring.\nSetting up Graylog with Docker To initiate our exploration of Graylog, we\u0026rsquo;ll opt for a Docker Installation\r, which ensures simplicity and ease of deployment. Follow the steps outlined in the official documentation to set up Graylog via Docker. Upon successful installation, access the Graylog interface by navigating to http://localhost:9000/, and use the default credentials: admin/admin.\nConfigure MongoDB MongoDB\rserves as the backend database for Graylog, facilitating efficient data storage and retrieval. Below is the configuration snippet for deploying MongoDB within our Kubernetes environment:\napiVersion: apps/v1 kind: Deployment metadata: name: mongo namespace: log spec: replicas: 1 selector: matchLabels: app: mongo template: metadata: labels: app: mongo spec: containers: - name: mongo image: mongo:6.0.14 ports: - containerPort: 27017 --- apiVersion: v1 kind: Service metadata: labels: app: mongo name: mongo-svc namespace: log spec: ports: - port: 27017 protocol: TCP targetPort: 27017 selector: app: mongo\rCopy\rConfigure Opensearch OpenSearch\r, a robust and scalable open-source search and analytics engine, complements Graylog by providing enhanced capabilities for data indexing and querying. Utilize the following configuration to deploy OpenSearch:\napiVersion: apps/v1 kind: Deployment metadata: name: opensearch namespace: log spec: replicas: 1 selector: matchLabels: app: opensearch template: metadata: labels: app: opensearch spec: containers: - name: opensearch image: opensearchproject/opensearch:2.2.0 env: - name: OPENSEARCH_JAVA_OPTS value: -Xms512m -Xmx512m - name: bootstrap.memory_lock value: \u0026#34;true\u0026#34; - name: discovery.type value: single-node - name: action.auto_create_index value: \u0026#34;false\u0026#34; - name: plugins.security.ssl.http.enabled value: \u0026#34;false\u0026#34; - name: plugins.security.disabled value: \u0026#34;true\u0026#34; # Can generate a password for `OPENSEARCH_INITIAL_ADMIN_PASSWORD` using a linux device via: # tr -dc A-Z-a-z-0-9_@#%^-_=+ \u0026lt; /dev/urandom | head -c${1:-32} - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD value: w6zp+GhiM2jvyT8MW#ANfI+Pz-IMppSs ports: - containerPort: 9200 --- apiVersion: v1 kind: Service metadata: name: opensearch-svc namespace: log spec: selector: app: opensearch type: NodePort ports: - name: \u0026#34;9200\u0026#34; port: 9200 targetPort: 9200 nodePort: 30200\rCopy\rConfigure Graylog The pivotal step involves configuring Graylog itself. Below, you\u0026rsquo;ll find the deployment configuration for Graylog, including necessary environment variables for seamless integration with MongoDB and OpenSearch:\napiVersion: apps/v1 kind: Deployment metadata: name: graylog namespace: log spec: replicas: 1 selector: matchLabels: app: graylog template: metadata: labels: app: graylog spec: containers: - name: graylog image: graylog/graylog:5.2 env: - name: GRAYLOG_SERVER_JAVA_OPTS value: \u0026#34;-Xms1g -Xmx1g -server -XX:+UseG1GC -XX:-OmitStackTraceInFastThrow\u0026#34; - name: GRAYLOG_NODE_ID_FILE value: /usr/share/graylog/data/data/node-id - name: GRAYLOG_PASSWORD_SECRET value: e2ssyyBDYfKMmBm7iAcQ3WjroGlcueh8mzC7nL6suirLw2lsSHTEQeVuDf1rJ7WTZaWA3GeDeqwW6tMwbnSd4lkBHgCSUzEp # echo -n \u0026#34;Enter Password: \u0026#34; \u0026amp;\u0026amp; head -1 \u0026lt;/dev/stdin | tr -d \u0026#39;\\n\u0026#39; | sha256sum | cut -d\u0026#34; \u0026#34; -f1 - name: GRAYLOG_ROOT_PASSWORD_SHA2 value: 8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918 - name: GRAYLOG_HTTP_BIND_ADDRESS value: 0.0.0.0:9000 - name: GRAYLOG_HTTP_EXTERNAL_URI value: http://192.168.68.239:9000/ - name: GRAYLOG_MONGODB_URI value: mongodb://mongo-svc.log.svc:27017/graylog - name: GRAYLOG_ELASTICSEARCH_HOSTS value: http://admin:admin@opensearch-svc.log.svc:9200 ports: - containerPort: 9000 - containerPort: 12201 --- apiVersion: v1 kind: Service metadata: name: graylog-svc namespace: log spec: type: LoadBalancer selector: app: graylog ports: - name: \u0026#34;9000\u0026#34; port: 9000 targetPort: 9000 - name: \u0026#34;12201\u0026#34; port: 12201 targetPort: 12201\rCopy\rOnce deployed, Graylog will be accessible via a LoadBalancer service, allowing for efficient log management and analysis. This is my GELF TCP input setup:\nDeploying Fluent Bit for Log Forwarding Fluent Bit\remerges as a lightweight and high-performance log processor and forwarder, enhancing the overall log management infrastructure. Follow the steps below to integrate Fluent Bit into your HomeLab environment:\nhelm repo add fluent https://fluent.github.io/helm-charts helm upgrade --install fluent-bit fluent/fluent-bit\rCopy\rUpon successful deployment, configure Fluent Bit to forward Kubernetes logs to Graylog using custom parsers and filters, ensuring seamless integration and enhanced log enrichment.\nCustomizing Log Parsing and Enrichment Referencing from the JSON\rparser, this is my config:\n[PARSER] Name docker Format json ‚Ä¶","date":"2024-04-19","permalink":"https://seehiong.github.io/posts/2024/04/log-management-with-graylog/","summary":"Explore setting up Graylog in your HomeLab for comprehensive log management. Configure MongoDB and OpenSearch, deploy Fluent Bit for log forwarding, ‚Ä¶","tags":["HomeLab","K3s","K8s","Mongo","OpenSearch","Graylog","Docker","MongoDB","Fluent Bit","Netshoot"],"title":"Log Management with Graylog"},{"content":"Welcome to an exploration of CrewAI\r, a state-of-the-art framework designed to orchestrate autonomous AI agents. In this post, we\u0026rsquo;ll dive into the practical aspects of CrewAI, discovering its functionalities and potential applications.\nGetting Started To dive into the world of AI-driven creativity, let\u0026rsquo;s start by setting up our environment. We\u0026rsquo;ll create a dedicated Conda environment to ensure seamless integration with CrewAI:\n# Create a new Conda environment conda create -n crewai python=3.10 # Active the environment conda activate crewai\rCopy\rNow, let\u0026rsquo;s install CrewAI along with its powerful tools:\npip install crewai pip install \u0026#39;crewai[tools]\u0026#39;\rCopy\rExploring Jan and LM Studio Before we jump into the coding adventure, let\u0026rsquo;s indulge in some AI magic with Jan\rand LM Studio\r. We\u0026rsquo;ll harness the power of the remarkable google/gemma-2b-it\rmodel:\nUnlocking the Potential with Serper Next, let\u0026rsquo;s unlock the vast possibilities with Serper\r, the Google Search API. By leveraging Serper\u0026rsquo;s capabilities, we can access an ocean of information for our AI agents:\nCrafting Brilliance with CrewAI and Jan Now, let\u0026rsquo;s delve into the heart of the matter ‚Äì coding with CrewAI\r. With a powerful combination of roles and tasks, CrewAI empowers us to unleash our creativity:\nimport os os.environ[\u0026#34;SERPER_API_KEY\u0026#34;] = \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; os.environ[\u0026#34;OPENAI_API_BASE\u0026#34;] = \u0026#34;http://192.168.68.117:1337/v1\u0026#34; os.environ[\u0026#34;OPENAI_MODEL_NAME\u0026#34;] = \u0026#34;NA\u0026#34; os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#34;NA\u0026#34; from crewai import Agent from crewai_tools import SerperDevTool search_tool = SerperDevTool() # Creating a senior researcher agent with memory and verbose mode researcher = Agent( role=\u0026#39;Senior Researcher\u0026#39;, goal=\u0026#39;Uncover groundbreaking technologies in {topic}\u0026#39;, verbose=True, memory=True, backstory=( \u0026#34;Driven by curiosity, you\u0026#39;re at the forefront of\u0026#34; \u0026#34;innovation, eager to explore and share knowledge that could change\u0026#34; \u0026#34;the world.\u0026#34; ), tools=[search_tool], allow_delegation=True ) # Creating a writer agent with custom tools and delegation capability writer = Agent( role=\u0026#39;Writer\u0026#39;, goal=\u0026#39;Narrate compelling tech stories about {topic}\u0026#39;, verbose=True, memory=True, backstory=( \u0026#34;With a flair for simplifying complex topics, you craft\u0026#34; \u0026#34;engaging narratives that captivate and educate, bringing new\u0026#34; \u0026#34;discoveries to light in an accessible manner.\u0026#34; ), tools=[search_tool], allow_delegation=False ) from crewai import Task # Research task research_task = Task( description=( \u0026#34;Identify the next big trend in {topic}.\u0026#34; \u0026#34;Focus on identifying pros and cons and the overall narrative.\u0026#34; \u0026#34;Your final report should clearly articulate the key points\u0026#34; \u0026#34;its market opportunities, and potential risks.\u0026#34; ), expected_output=\u0026#39;A comprehensive 3 paragraphs long report on the latest AI trends.\u0026#39;, tools=[search_tool], agent=researcher, ) # Writing task with language model configuration write_task = Task( description=( \u0026#34;Compose an insightful article on {topic}.\u0026#34; \u0026#34;Focus on the latest trends and how it\u0026#39;s impacting the industry.\u0026#34; \u0026#34;This article should be easy to understand, engaging, and positive.\u0026#34; ), expected_output=\u0026#39;A 4 paragraph article on {topic} advancements formatted as markdown.\u0026#39;, tools=[search_tool], agent=writer, async_execution=False, output_file=\u0026#39;new-blog-post.md\u0026#39; # Example of output customization ) from crewai import Crew, Process # Forming the tech-focused crew with enhanced configurations crew = Crew( agents=[researcher, writer], tasks=[research_task, write_task], process=Process.sequential # Optional: Sequential task execution is default ) # Starting the task execution process with enhanced feedback result = crew.kickoff(inputs={\u0026#39;topic\u0026#39;: \u0026#39;AI in healthcare\u0026#39;}) print(result)\rCopy\rWitness the AI prowess in action as we uncover the latest trends in AI healthcare. Brace yourself for groundbreaking insights!\nAnother sample topic for AI in proptech. This is the contents of new-blog-post.md:\n** ## The Future of PropTech: AI Trends and Challenges PropTech, the intersection of artificial intelligence and property market, is rapidly evolving. As AI capabilities expand, proptech companies leverage AI to drive growth and improve customer experiences. However, challenges such as data privacy and algorithmic bias need attention to ensure the responsible and inclusive implementation of AI in the proptech industry. One of the most significant AI trends in proptech is the development of AI-powered property search tools. These tools leverage machine learning algorithms to analyze data and provide users with more relevant and efficient search results. By incorporating AI-powered virtual assistants, property managers can automate tasks, manage maintenance, and respond to customer queries, reducing the ‚Ä¶","date":"2024-03-29","permalink":"https://seehiong.github.io/posts/2024/03/coding-with-crewai-ai-orchestration-simplified/","summary":"Explore CrewAI, a pioneering framework streamlining AI agent orchestration. Discover practical applications, from Jan and LM Studio integration to ‚Ä¶","tags":["AI","Jan","LM Studio","CrewAI","Multi-Agent","Serper","Gemma"],"title":"Coding with CrewAI: AI Orchestration Simplified"},{"content":"In this post, I\u0026rsquo;m going to demonstrate the setup of Continue\r, an open-source autopilot designed for VS Code.\nGetting Started Once you\u0026rsquo;ve installed the plugin from the market place, let\u0026rsquo;s proceed by adding Continue to the right sidebar of VS Code, as recommended.\nProvider - LM Studio First, on my Windows machine, I\u0026rsquo;ll execute LM Studio\rand download Google\u0026rsquo;s Gemma 2B Instruct\rmodel.\nProvider - OpenAI-compatible Next, let\u0026rsquo;s configure another LLM in our Home Lab. Here\u0026rsquo;s the Dockerfile from my previous post\r:\nFROM python:3-slim-bullseye ENV model sample_model # We need to set the host to 0.0.0.0 to allow outside access ENV HOST 0.0.0.0 # Install the package RUN apt update \u0026amp;\u0026amp; apt install -y libopenblas-dev ninja-build build-essential pkg-config RUN pip install --upgrade pip RUN python -m pip install --no-cache-dir --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context RUN CMAKE_ARGS=\u0026#34;-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\u0026#34; pip install --no-cache-dir --force-reinstall llama_cpp_python==0.2.27 --verbose # Run the server CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;python3 -m llama_cpp.server --model /models/\\\u0026#34;$model\\\u0026#34;\u0026#34;]\rCopy\rNow, let\u0026rsquo;s build it:\n# Build the image docker build . -t llama-cpp-python:0.2.27 # Tag it docker tag llama-cpp-python:0.2.27 registry.local:5000/llama-cpp-python:0.2.27 # Push to the registry docker push registry.local:5000/llama-cpp-python:0.2.27\rCopy\rIn this post, I\u0026rsquo;ll utilize the Deepseek-Coder\rmodel. Here\u0026rsquo;s the deploy.yaml file:\napiVersion: apps/v1 kind: Deployment metadata: name: deepseek-coder-instruct namespace: llm spec: replicas: 1 selector: matchLabels: app: deepseek-coder-instruct template: metadata: labels: app: deepseek-coder-instruct spec: containers: - name: deepseek-coder-instruct image: registry.local:5000/llama-cpp-python:0.2.27 imagePullPolicy: IfNotPresent ports: - containerPort: 8000 securityContext: capabilities: add: - IPC_LOCK volumeMounts: - name: models-store mountPath: /models subPath: ./llama-models env: - name: model value: deepseek-coder-6.7b-instruct.Q4_K_M.gguf resources: requests: memory: \u0026#34;6Gi\u0026#34; limits: memory: \u0026#34;6Gi\u0026#34; imagePullSecrets: - name: regcred volumes: - name: models-store persistentVolumeClaim: claimName: nfs\rCopy\rHere\u0026rsquo;s the svc.yaml file:\napiVersion: v1 kind: Service metadata: name: deepseek-coder-instruct-svc namespace: llm spec: selector: app: deepseek-coder-instruct type: ClusterIP ports: - name: http protocol: TCP port: 80 targetPort: 8000\rCopy\rDepending on your Home Lab configuration, I\u0026rsquo;m utilizing Kong Manager\r. Here\u0026rsquo;s my Gateway Service setup:\nAnd here\u0026rsquo;s the Route setup:\nAdditionally, here\u0026rsquo;s the Application Log for the deployed pod:\nConfiguration From the Continue sidebar, click on the Plus icon at the bottom, then scroll all the way down and click on the Open config.json button.\nSince I\u0026rsquo;ve set up 2 models locally, here are the corresponding config values:\n{ \u0026#34;models\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;LM Studio\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;lmstudio\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;gemma-2b-it\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;llama-cpp-python\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;deepseek-coder-6.7b-instruct.Q4_K_M.gguf\u0026#34;, \u0026#34;apiKey\u0026#34;: \u0026#34;EMPTY\u0026#34;, \u0026#34;apiBase\u0026#34;: \u0026#34;http://kong.local:8000/deepseek/v1\u0026#34; } ], ...\rCopy\rFor the tabAutocompleteModel, I am using:\n\u0026#34;tabAutocompleteModel\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;llama-cpp-python\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;deepseek-coder-6.7b-instruct.Q4_K_M.gguf\u0026#34;, \u0026#34;apiKey\u0026#34;: \u0026#34;EMPTY\u0026#34;, \u0026#34;apiBase\u0026#34;: \u0026#34;http://kong.local:8000/deepseek/v1\u0026#34; }, \u0026#34;allowAnonymousTelemetry\u0026#34;: false, ... }\rCopy\rPutting it into Action Let\u0026rsquo;s test it out by inserting the dockerfile block into this post. After selecting the block of code, press CTRL-L, and it will appear on the right.\nLM Studio Let\u0026rsquo;s experiment with the LM Studio model:\nOpenAI-compatible Now, let\u0026rsquo;s experiment with the llama-cpp-python model:\nThat\u0026rsquo;s all there is to it! We now have a locally enabled autopilot for code completion!\n","date":"2024-03-16","permalink":"https://seehiong.github.io/posts/2024/03/autopilot-setup-for-vs-code/","summary":"Discover the quick setup of AutoPilot for VS Code, enabling seamless code completion with LM Studio and OpenAI-compatible models. Learn the ‚Ä¶","tags":["Autopilot","LM Studio","OpenAI","Continue","Google","Gemma"],"title":"AutoPilot Setup for VS Code"},{"content":"Following up on my previous post about deploying Appwrite with K3s\r, I will now guide you through configuring K3s to support Appwrite Functions.\nPrepartion Install Ngrok Since I am running Appwrite in my HomeLab, I need to utilize ngrok\rto enable external network access (such as GitHub) to our internal network. After signing up, install ngrok via Chocolatey:\nchoco install ngrok ngrok config add-authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx ngrok http http://appwrite.local/ Copy\rTake note of the host; in my case, it is d7e9-42-60-49-2.ngrok-free.app.\nRegister GitHub App After logging in to your GitHub account, navigate to your avatar on the top right, and select Settings. Then, click on Developer settings and select New GitHub App at the top right corner.\nFollowing the Appwrite Functions Docs\r, fill in the details sequentially.\nFirst, enter the GitHub App name (e.g. sh-hello-world) and homepage URL (e.g. https://seehiong.github.io/\r).\nFor the callback URL, click Add Callback URL and enter the following two URLs:\nhttps://d7e9-42-60-49-2.ngrok-free.app/v1/vcs/github/callback\rhttps://d7e9-42-60-49-2.ngrok-free.app/v1/account/sessions/oauth2/callback/github/console\rCopy\rEnable the checkbox for requesting user authorization (OAuth) during installation, and also for redirect on update.\nIn the Webhook section, provide the Webhook URL and its secret (e.g. webhook-secret):\nhttps://d7e9-42-60-49-2.ngrok-free.app/v1/vcs/github/events\rCopy\rFollow the settings outlined in the official documentation for Repository permissions, Account permissions, and Subscribe to events sections.\nUnder Where can this GitHub App be installed?, select the Any account option to allow for multiple Appwrite projects.\nFinally, click on the generate a private key link and then the Generate a private key button, followed by generating a new client secret.\nAppwrite Setup After opening the private key file (e.g. sh-hello-world.xxx.pem), add a \u0026ldquo;\\n\u0026rdquo; to the end of each line and remove the \u0026ldquo;carriage return\u0026rdquo; so that it becomes a single-line string:\n\u0026#34;-----BEGIN RSA PRIVATE KEY-----\\nMIIEogIBAAKCAQEA5f58x/UROPmLo60dSmTqE3hDO4THacvj0nUy9gnGJ5tf6vS4\\n[...]-----END RSA PRIVATE KEY-----\u0026#34;\rCopy\rAppwrite-deployment.yaml Update the relevant values:\n- name: _APP_VCS_GITHUB_APP_NAME value: sh-hello-world - name: _APP_VCS_GITHUB_PRIVATE_KEY value: \u0026#34;-----BEGIN RSA PRIVATE KEY-----\\nMIIEogIBAAKCAQEA5f58x/UROPmLo60dSmTqE3hDO4THacvj0nUy9gnGJ5tf6vS4\\n[...]-----END RSA PRIVATE KEY-----\u0026#34; - name: _APP_VCS_GITHUB_APP_ID value: \u0026#34;850000\u0026#34; - name: _APP_VCS_GITHUB_WEBHOOK_SECRET value: webhook-secret - name: _APP_VCS_GITHUB_CLIENT_SECRET value: \u0026#34;aeb0000000000000000000000000000000000000\u0026#34; - name: _APP_VCS_GITHUB_CLIENT_ID value: \u0026#34;Iv1.0000000000000000\u0026#34;\rCopy\rAlso, update the ngrok-related values:\n- name: _APP_DOMAIN value: d7e9-42-60-49-2.ngrok-free.app - name: _APP_DOMAIN_TARGET value: d7e9-42-60-49-2.ngrok-free.app - name: _APP_DOMAIN_FUNCTIONS value: d7e9-42-60-49-2.ngrok-free.app\rCopy\rAs we migrated to using K3s, modify the _APP_EXECUTOR_HOST as follows:\n- name: _APP_EXECUTOR_HOST value: http://openruntimes-executor-svc.appwrite.svc/v1\rCopy\rAppwrite-worker-builds-deployment.yaml To support functions, update these values:\n- name: _APP_EXECUTOR_HOST value: http://openruntimes-executor-svc.appwrite.svc/v1 - name: _APP_VCS_GITHUB_APP_NAME value: sh-hello-world - name: _APP_VCS_GITHUB_PRIVATE_KEY value: \u0026#34;-----BEGIN RSA PRIVATE KEY-----\\nMIIEogIBAAKCAQEA5f58x/UROPmLo60dSmTqE3hDO4THacvj0nUy9gnGJ5tf6vS4\\n[...]-----END RSA PRIVATE KEY-----\u0026#34; - name: _APP_VCS_GITHUB_APP_ID value: \u0026#34;850000\u0026#34; - name: _APP_DOMAIN value: d7e9-42-60-49-2.ngrok-free.app\rCopy\rAppwrite-worker-certificates-deployment.yaml Update the ngrok-related values:\n- name: _APP_DOMAIN value: d7e9-42-60-49-2.ngrok-free.app - name: _APP_DOMAIN_TARGET value: d7e9-42-60-49-2.ngrok-free.app - name: _APP_DOMAIN_FUNCTIONS value: d7e9-42-60-49-2.ngrok-free.app\rCopy\rAppwrite-worker-functions-deployment.yaml To support functions, update these values:\n- name: _APP_EXECUTOR_HOST value: http://openruntimes-executor-svc.appwrite.svc/v1\rCopy\rAppwrite-worker-migrations-deployment.yaml Update the ngrok-related values:\n- name: _APP_DOMAIN value: d7e9-42-60-49-2.ngrok-free.app - name: _APP_DOMAIN_TARGET value: d7e9-42-60-49-2.ngrok-free.app\rCopy\rAppwrite-maintenance-deployment.yaml Update the ngrok-related values:\n- name: _APP_DOMAIN value: d7e9-42-60-49-2.ngrok-free.app - name: _APP_DOMAIN_TARGET value: d7e9-42-60-49-2.ngrok-free.app - name: _APP_DOMAIN_FUNCTIONS value: d7e9-42-60-49-2.ngrok-free.app\rCopy\rOpenruntimes-executor-deployment.yaml To allow executors to have access to Docker, set the pod to run as root (not advisable in production environments):\nspec: securityContext: runAsUser: 0 containers: - env: - name: OPR_EXECUTOR_NETWORK value: ...\rCopy\rAlternatively, identify the node that runs the pod, and issue the command to create the runtimes ‚Ä¶","date":"2024-03-10","permalink":"https://seehiong.github.io/posts/2024/03/configuring-appwrite-functions-with-k3s/","summary":"This guide outlines configuring Appwrite Functions within a K3s environment. It covers essential steps, including installing ngrok for external ‚Ä¶","tags":["HomeLab","K3s","K8s","Appwrite","Ngrok","Traefik"],"title":"Configuring Appwrite Functions with K3s"},{"content":"In this guide, we\u0026rsquo;ll delve into the process of installing Budibase\rwithin our HomeLab environment. Budibase offers the capability to craft robust applications and workflows from various data sources, enabling the secure deployment of professional-grade solutions across our teams.\nTesting Budibase with Docker Desktop Let\u0026rsquo;s start our exploration by testing Budibase using Docker compose\r. To begin, download both the docker-compose.yaml and .env files, then launch the platform with the following command:\ndocker-compose up\rCopy\rThe platform is now accessible at:\nhttp://localhost:10000\rCopy\rWith Budibase successfully running on Windows Docker Desktop, let\u0026rsquo;s proceed to installing it within our HomeLab.\nDeploying Budibase to Home Lab Preparing Helm As we\u0026rsquo;ll be utilizing Helm\r, let\u0026rsquo;s install it:\nsudo snap install helm --classic\rCopy\rNext, configure our kubeconfig to grant Helm access:\ncp /etc/rancher/k3s/k3s.yaml ~/.kube/config chmod 600 ~/.kube/config\rCopy\rDeploying Budibase Now, let\u0026rsquo;s install the Budibase Helm chart:\nhelm repo add budibase https://budibase.github.io/budibase/ helm repo update helm install --create-namespace --namespace budibase budibase budibase/budibase\rCopy\rAnd with that, Budibase should be installed and operational:\nBuilding Our First App After setting up the admin user, let\u0026rsquo;s proceed with crafting our job application app!\nWe\u0026rsquo;ll kickstart by creating the required table. Initially, I\u0026rsquo;ll utilize the built-in CouchDB:\nNext, we\u0026rsquo;ll design the application form and link it to the created table:\nThen, we\u0026rsquo;ll design the application form itself:\nFor the Submit button\u0026rsquo;s On Click action, let\u0026rsquo;s set it to Save Row:\nLastly, let\u0026rsquo;s publish the app by clicking on the Publish icon located in the top-right corner.\nSubmitting the Application Form The application form is accessible at the following URL, where the application URL is application-form and the route is form:\nhttp://192.168.68.115/app/application-form#/form\rCopy\rAnd there you have it! We\u0026rsquo;ve successfully built and published our first app using Budibase!\nOptional Let\u0026rsquo;s first export the current data and to uninstall the application. Next, let\u0026rsquo;s create the values.yaml file:\ncouchdb: persistentVolume: enabled: true storageClass: \u0026#34;budibase-couchdb\u0026#34; size: \u0026#34;1Gi\u0026#34; services: objectStore: storageClass: \u0026#34;budibase-services\u0026#34; redis: storageClass: \u0026#34;budibase-services\u0026#34;\rCopy\rCopy the budibase-nfs.yaml file to /var/lib/rancher/k3s/server/manifests:\napiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: budibase-couchdb namespace: budibase spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner set: nfs.server: tnas.local nfs.path: /mnt/public/budibase storageClass.name: budibase-couchdb storageClass.reclaimPolicy: Retain storageClass.accessModes: ReadWriteMany nfs.reclaimPolicy: Retain --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: budibase-services namespace: budibase spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner set: nfs.server: tnas.local nfs.path: /mnt/public/budibase storageClass.name: budibase-services storageClass.reclaimPolicy: Retain storageClass.accessModes: ReadWriteMany nfs.reclaimPolicy: Retain\rCopy\rReinstall Budibase with:\n# Install Helm Chart helm install --create-namespace --namespace budibase -f values.yaml budibase budibase/budibase\rCopy\rFinally import your app and from now onwards, all the Budibase files will be saved to our shared NAS!\n","date":"2024-02-25","permalink":"https://seehiong.github.io/posts/2024/02/deploying-budibase-in-homelab/","summary":"This post outlines the process of installing Budibase in a HomeLab environment, starting with testing it on Docker Desktop and then deploying it using ‚Ä¶","tags":["HomeLab","K3s","K8s","Helm","Budibase","NFS","CouchDB"],"title":"Deploying Budibase in HomeLab"},{"content":"In this post, we\u0026rsquo;ll embark on installing Appwrite\r, an open-source platform designed to facilitate the integration of authentication, databases, functions, and storage, enabling the development of scalable applications within our HomeLab setup.\nPrepartion Referencing my previous K3s setup post\r, let\u0026rsquo;s initiate the installation process by deploying K3s server, this time with Traefik disabled:\nsudo curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--disable traefik\u0026#34; K3S_KUBECONFIG_MODE=\u0026#34;644\u0026#34; sh -\rCopy\rAs usual, we verify the status with:\nkc get no\rCopy\rFollowing that, we deploy Portainer to the K3s cluster:\nmkdir portainer cd portainer # Download Portainer and deploy it wget https://raw.githubusercontent.com/portainer/k8s/master/deploy/manifests/portainer/portainer.yaml -O deploy.yaml kca deploy.yaml # Check the port to use kc get svc portainer -n portainer\rCopy\rNavigate to the specified port (e.g., http://bee:30777) and sets portainer up. and set up Portainer. Next, proceed by clicking on Get started.\nGenerating Appwrite K8s Files Referencing Appwrite\u0026rsquo;s Manual (Docker Compose)\r, let\u0026rsquo;s download the docker-compose.yml and .env files. We\u0026rsquo;ll utilize Kompose\rto convert them from Docker Compose to Kubernetes.\n# Download Appwrite files wget https://appwrite.io/install/compose -O docker-compose.yml wget https://appwrite.io/install/env -O env curl -L https://github.com/kubernetes/kompose/releases/download/v1.32.0/kompose-linux-amd64 -o kompose chmod +x kompose # Convert to K8s files ./kompose convert\rCopy\rPreparing Appwrite Volumes From my previous NFS post\r, let\u0026rsquo;s prepare the files to be placed in /var/lib/rancher/k3s/server/manifests. We require a total of 9 storage classes. Refer to my public repository, appwrite-k8s-deployment\rfor all necessary Kubernetes files mentioned in this post.\nBelow is an example for appwrite-builds.yaml:\n# Example: appwrite-builds.yaml apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: appwrite-builds namespace: appwrite spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner set: nfs.server: tnas.local nfs.path: /mnt/public/appwrite storageClass.name: appwrite-builds storageClass.reclaimPolicy: Retain storageClass.accessModes: ReadWriteMany nfs.reclaimPolicy: Retain\rCopy\rLet\u0026rsquo;s proceed to copying these 9 files to the auto-deployed folder:\nls appwrite-*.yaml sudo cp appwrite-*.yaml /var/lib/rancher/k3s/server/manifests # Verify files are copied correctly sudo ls /var/lib/rancher/k3s/server/manifests\rCopy\rSimilarly, create the corresponding persistent volume claims and apply them:\nkc create ns appwrite kca pvc-appwrite-builds.yaml kca pvc-appwrite-cache.yaml kca pvc-appwrite-certificates.yaml kca pvc-appwrite-config.yaml kca pvc-appwrite-functions.yaml kca pvc-appwrite-influxdb.yaml kca pvc-appwrite-mariadb.yaml kca pvc-appwrite-redis.yaml kca pvc-appwrite-uploads.yaml\rCopy\rPreparing Misc Services As Kompose do not include any required environment variables, we\u0026rsquo;ll manually map them against the original Docker Compose files and merge values from the .env file. These are the items to note of:\nTo add env var to all deployment files: # Example: mariadb-deployment.yaml spec: template: spec: containers: - args: - mysqld - --innodb-flush-method=fsync # To include the required env env: - name: MYSQL_DATABASE value: appwrite - name: MYSQL_PASSWORD value: password - name: MYSQL_ROOT_PASSWORD value: rootsecretpassword - name: MYSQL_USER value: user image: mariadb:10.7 name: appwrite-mariadb Copy\rTo use hostPath to access host\u0026rsquo;s /var/run/docker.sock: # Example: openruntimes-executor-deployment.yaml spec: template: metadata: spec: hostname: appwrite-executor restartPolicy: Always volumes: # To use hostPath to host\u0026#39;s /var/run/docker.sock and tmp folder - name: openruntimes-executor-claim0 hostPath: path: /var/run/docker.sock - name: appwrite-builds persistentVolumeClaim: claimName: appwrite-builds - name: appwrite-functions persistentVolumeClaim: claimName: appwrite-functions - name: openruntimes-executor-claim3 hostPath: path: /tmp\rCopy\rTo use the fully qualified name for the DB and other services. # Example: appwrite-deployment.yaml spec: template: spec: containers: - image: appwrite/appwrite:1.4.13 name: appwrite # To include the required env env: # Using the fully qualified name to DB host - name: _APP_DB_HOST value: mariadb-svc.appwrite.svc\rCopy\rApply all necessary DB and services:\n# Deployments kca influxdb-deployment.yaml kca mariadb-deployment.yaml kca openruntimes-executor-deployment.yaml kca redis-deployment.yaml kca telegraf-deployment.yaml # Services kca influxdb-svc.yaml kca mariadb-svc.yaml kca openruntimes-executor-svc.yaml kca redis-svc.yaml kca telegraf-svc.yaml\rCopy\rInstalling Appwrite Services Finally, apply the remaining items:\n# Deployments kca appwrite-worker-webhooks-deployment.yaml kca appwrite-worker-migrations-deployment.yaml kca ‚Ä¶","date":"2024-02-16","permalink":"https://seehiong.github.io/posts/2024/02/deploying-appwrite-in-homelab-with-k3s/","summary":"Learn how to seamlessly integrate Appwrite, an open-source platform, into your HomeLab setup using K3s. Follow step-by-step instructions to deploy K3s ‚Ä¶","tags":["HomeLab","K3s","K8s","Appwrite","Traefik","Portainer","Kompose"],"title":"Deploying Appwrite in HomeLab with K3s"},{"content":"In this post, I\u0026rsquo;ll delve into the capabilities of the StableDiffusionPipeline\rfor generating photorealistic images based on textual inputs.\nText-to-Image Continuing from the previous post\r, I initiated the environment setup:\ncd stable-diffusion conda activate ldm\rCopy\rSubsequently, I installed the necessary libraries, diffusers\rand transformers\r:\npip install --upgrade diffusers[torch] transformers\rCopy\rTo begin, let\u0026rsquo;s explore the diffusion pipeline:\nfrom diffusers import DiffusionPipeline import torch pipeline = DiffusionPipeline.from_pretrained(\u0026#34;runwayml/stable-diffusion-v1-5\u0026#34;, torch_dtype=torch.float16) pipeline.to(\u0026#34;cuda\u0026#34;) image = pipeline(\u0026#34;An image of a squirrel in Picasso style\u0026#34;).images[0] image.save(\u0026#34;squirrel-image.jpg\u0026#34;)\rCopy\rTextual Inversion Referencing from Textual inversion\r, the StableDiffusionPipeline supports a fascinating technique allowing models like Stable Diffusion to learn new concepts from a few images.\nTo employ Textual Inversion embedding vectors, as outlined in Image-to-image\r, let\u0026rsquo;s download the charturner v2 embeddings\r:\nwget https://huggingface.co/AmornthepKladmee/embeddings/resolve/main/charturnerv2.pt\rCopy\rNow, let\u0026rsquo;s apply textual inversion:\nfrom diffusers import StableDiffusionPipeline import torch model_id = \u0026#34;runwayml/stable-diffusion-v1-5\u0026#34; pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\u0026#34;cuda\u0026#34;) pipe.load_textual_inversion(\u0026#34;./charturnerv2.pt\u0026#34;, token=\u0026#34;charturnerv2\u0026#34;) prompt = \u0026#34;charturnerv2, multiple views of the same character in the same outfit, a fit character for a RPG game in best quality, intricate details.\u0026#34; image = pipe(prompt, num_inference_steps=50).images[0] image.save(\u0026#34;character.png\u0026#34;)\rCopy\rImage to Image Next, leveraging the previously generated carton-insect.png from the earlier post, let\u0026rsquo;s explore the image-to-image pipeline:\nfrom PIL import Image from diffusers import StableDiffusionImg2ImgPipeline import torch model_id = \u0026#34;runwayml/stable-diffusion-v1-5\u0026#34; pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16) pipe = pipe.to(\u0026#34;cuda\u0026#34;) init_image = Image.open(\u0026#34;cartoon-insect.png\u0026#34;).convert(\u0026#34;RGB\u0026#34;) pipe.load_textual_inversion(\u0026#34;./charturnerv2.pt\u0026#34;, token=\u0026#34;charturnerv2\u0026#34;) prompt = \u0026#34;charturnerv2, cartoon insect\u0026#34; images = pipe(prompt, image=init_image, strength=0.75, guidance_scale=7.5, num_inference_steps=50).images images[0].save(\u0026#34;cartoon-insect-1.png\u0026#34;)\rCopy\rAnimagine XL 2.0 Introducing Animagine XL 2.0\ran advanced latent text-to-image diffusion model tailored for creating high-resolution anime images. It is fine-tuned from Stable Diffusion XL 1.0 (SDXL) using a premium anime-style image dataset.\nLet\u0026rsquo;s try out the sample code with some tweaks:\nimport torch from diffusers import ( StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler, AutoencoderKL ) # Load VAE component vae = AutoencoderKL.from_pretrained( \u0026#34;madebyollin/sdxl-vae-fp16-fix\u0026#34;, torch_dtype=torch.float16 ) # Configure the pipeline pipe = StableDiffusionXLPipeline.from_pretrained( \u0026#34;Linaqruf/animagine-xl-2.0\u0026#34;, vae=vae, torch_dtype=torch.float16, use_safetensors=True, variant=\u0026#34;fp16\u0026#34; ) pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config) pipe.to(\u0026#39;cuda\u0026#39;) # Define prompts and generate image prompt = \u0026#34;face focus, cute, masterpiece, best quality, 1girl, red hair, sweater, looking at viewer, upper body, smiley, outdoors, daylight, blouse, earings\u0026#34; negative_prompt = \u0026#34;lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, hat\u0026#34; image = pipe( prompt, negative_prompt=negative_prompt, width=1024, height=1024, guidance_scale=12, num_inference_steps=50 ).images[0] image.save(\u0026#34;./animagine.png\u0026#34;)\rCopy\rStable Diffusion XL Stable Diffusion XL (SDXL)\ris a powerful text-to-image generation model that iterates on the previous Stable Diffusion Models.\nLet\u0026rsquo;s try with the text-to-image by passing the prompt. By default, SDXL generates a 1024x1024 image for the best results.\nfrom diffusers import AutoPipelineForText2Image import torch pipeline_text2image = AutoPipelineForText2Image.from_pretrained( \u0026#34;stabilityai/stable-diffusion-xl-base-1.0\u0026#34;, torch_dtype=torch.float16, variant=\u0026#34;fp16\u0026#34;, use_safetensors=True ).to(\u0026#34;cuda\u0026#34;) prompt = \u0026#34;Majestic dragon flying, huge fireworks in the form of Happy CNY 2024, detailed, 8k\u0026#34; image = pipeline_text2image(prompt=prompt).images[0] image.save(\u0026#34;majestic-dragon.png\u0026#34;)\rCopy\rWishing everyone a joyous and prosperous Chinese New Year 2024! Huat ah! üéâüêâ\n","date":"2024-02-10","permalink":"https://seehiong.github.io/posts/2024/02/text-to-image-with-stablediffusionpipeline/","summary":"In this post, we explore the capabilities of StableDiffusionPipeline for generating photorealistic images from textual inputs. We start with setting ‚Ä¶","tags":["AI","Stable Diffusion","Stable Diffusion XL","SDXL","Text to Image","Image to Image"],"title":"Text-to-Image with StableDiffusionPipeline"},{"content":"In this article, we will delve into Stable Diffusion\r, a latent text-to-image diffusion model. In simple terms, diffusion models in machine learning represent a type of sophisticated computer program designed to learn how patterns evolve over time. Comprising three essential components ‚Äì a forward process, a reverse process, and a sampling procedure ‚Äì these models aim to comprehend and generate intricate patterns within a given dataset.\nConsider having a blurry image that needs enhancement. Diffusion models act as intelligent tools that learn to eliminate blurriness by grasping how images blur and then effectively reversing that process.\nThese models find applications in various tasks, including image denoising, completing missing portions of an image, improving image clarity, and even generating entirely new images. For instance, OpenAI\u0026rsquo;s DALL-E 2, unveiled in April 2022, utilizes diffusion models to create images based on text descriptions.\nPreparation Upon installing Anaconda3 on my Windows Subsystem for Linux (WSL) machine, I cloned stable-diffusion\r.\ngit clone https://github.com/CompVis/stable-diffusion.git cd stable-diffusion conda env create -f environment.yaml conda activate ldm\rCopy\rNext, from Hugging Face\r, I obtained the provided checkpoint:\nwget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\rCopy\rLinked the model:\nmkdir -p models/ldm/stable-diffusion-v1/ # Actual path to the checkpoint file ln -s ~/stable-diffusion/sd-v1-4.ckpt models/ldm/stable-diffusion-v1/model.ckpt Copy\rImage generation Let\u0026rsquo;s experiment with generating a sample:\npython scripts/txt2img.py --prompt \u0026#34;a photograph of an astronaut riding a horse\u0026#34; --plms Copy\rThis image is from outputs/txt2img-samples/:\nTrying another sample:\npython scripts/txt2img.py --prompt \u0026#34;an astronaut riding a horse in photorealistic style\u0026#34; --plms Copy\rImage modification Resizing my original image, st-john-island-original.jpeg, to 512 x 512 with resize.py and executing with \u0026ldquo;python resize.py\u0026rdquo;:\nfrom PIL import Image def resize_image(input_path, output_path, new_size): try: with Image.open(input_path) as img: resized_img = img.resize(new_size) resized_img.save(output_path) print(f\u0026#34;Image resized and saved to {output_path}\u0026#34;) except Exception as e: print(f\u0026#34;Error: {e}\u0026#34;) input_image_path = \u0026#34;./st-john-island-original.jpeg\u0026#34; output_image_path = \u0026#34;./st-john-island-resized.jpg\u0026#34; new_size = (512, 512) resize_image(input_image_path, output_image_path, new_size)\rCopy\rModifying the image with this prompt:\npython scripts/img2img.py --prompt \u0026#34;monkey swinging from tree to tree on island\u0026#34; --init-img st-john-island-resized.jpg --strength 0.8\rCopy\rTroubleshooting The original image, st-john-island-original.jpeg, is 1600 x 1200 in size.\nHowever, encountered this error with the same prompt:\nRuntimeError: CUDA out of memory. Tried to allocate 52.22 GiB (GPU 0; 5.99 GiB total capacity; 56.84 GiB already allocated; 0 bytes free; 58.18 GiB reserved in total by PyTorch) If reserved memory is \u0026gt;\u0026gt; allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\rCopy\rEven after attempting to expand memory in the .wslconfig file and restarting WSL with wsl \u0026ndash;shutdown, the error persisted:\n# Settings apply across all Linux distros running on WSL 2 [wsl2] nestedVirtualization=true kernelCommandLine=ipv6.disable=1 memory=80GB processors=8\rCopy\rOn further research, realised that the model is trained on 512x512 images and hence I resized my image instead.\n","date":"2024-02-03","permalink":"https://seehiong.github.io/posts/2024/02/stable-diffusion-text-to-image-modeling-journey/","summary":"This post explores Stable Diffusion, a latent text-to-image diffusion model in machine learning. Diffusion models, with forward, reverse, and sampling ‚Ä¶","tags":["AI","Stable Diffusion","Text to Image","Ananconda","WSL","CUDA"],"title":"Stable Diffusion: Text-to-Image Modeling Journey"},{"content":"\rOpenVINO\rrepresents an open-source toolkit designed for the optimization and deployment of deep learning models. Acting as the interface between the Transformers and Diffusers libraries, Optimum-Intel\rseamlessly integrates with various Intel tools and libraries, facilitating the acceleration of end-to-end pipelines on Intel architectures. This post documents my journey as I set up and execute example code on my aging laptop, exploring the application of Quantization-aware Training (QAT) and the Token Merging method to optimize the UNet model within the Stable Diffusion pipeline.\nNote\rDue to limitations in my aging graphics card [GeForce GT 750M], the provided code is specifically tailored for CPU-based inference. The GPU resources are insufficient for handling GPU-intensive tasks.\rEnvironment Setup Commencing with the installation of conda\r, the following commands pave the way for the subsequent procedures:\nwget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh chmod +x Anaconda3-2022.05-Linux-x86_64.sh bash Anaconda3-2022.05-Linux-x86_64.sh # Identify your shell name using `echo $0`, e.g., shell.bash eval \u0026#34;$(/home/pi/anaconda3/bin/conda shell.YOUR_SHELL_NAME hook)\u0026#34; conda init conda create -n openvino python=3.10\rCopy\rSubsequently, the installation of prerequisites involves:\nsudo apt install vim git\rCopy\rReferencing the guidelines provided in Optimum-Intel\u0026rsquo;s GitHub repository\r, the repository is cloned using the following commands:\ngit clone https://github.com/huggingface/optimum-intel.git cd optimum-intel/examples/openvino/stable-diffusion # Install Optimum-Intel for OpenVINO pip install optimum-intel[openvino] pip install -r requirements.txt\rCopy\rNote\rA correction is necessary in the requirements.txt file:\ntomesd @ git+https://github.com/AlexKoff88/tomesd.git@openvino\rCopy\rExecution of Pre-Optimized Models To demonstrate the capabilities of the pre-optimized models, consider the following Python scripts:\nGeneral-purpose image generation model:\nfrom optimum.intel.openvino import OVStableDiffusionPipeline pipe = OVStableDiffusionPipeline.from_pretrained(\u0026#34;OpenVINO/stable-diffusion-2-1-quantized\u0026#34;, compile=False) pipe.reshape(batch_size=1, height=512, width=512, num_images_per_prompt=1) pipe.compile() prompt = \u0026#34;sailing ship in storm by Rembrandt\u0026#34; output = pipe(prompt, num_inference_steps=50, output_type=\u0026#34;pil\u0026#34;) output.images[0].save(\u0026#34;sailing-ship.png\u0026#34;)\rCopy\rPokemon generation:\nfrom optimum.intel.openvino import OVStableDiffusionPipeline pipe = OVStableDiffusionPipeline.from_pretrained(\u0026#34;OpenVINO/Stable-Diffusion-Pokemon-en-quantized\u0026#34;, compile=False) pipe.reshape(batch_size=1, height=512, width=512, num_images_per_prompt=1) pipe.compile() prompt = \u0026#34;cartoon bird\u0026#34; output = pipe(prompt, num_inference_steps=50, output_type=\u0026#34;pil\u0026#34;) output.images[0].save(\u0026#34;cartoon-bird.png\u0026#34;)\rCopy\rThe result appears to be a blend of both Pidgey\rand Swellow\r.\nprompt = \u0026#34;cartoon insert\u0026#34; output = pipe(prompt, num_inference_steps=50, output_type=\u0026#34;pil\u0026#34;) output.images[0].save(\u0026#34;cartoon-insect.png\u0026#34;)\rCopy\rThis iteration resembles Butterfree\r.\nHave great fun in your own Pokemon creation and evolution!\n","date":"2024-01-27","permalink":"https://seehiong.github.io/posts/2024/01/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/","summary":"Explore the convergence of OpenVINO and Optimum-Intel in this post, where I detail the setup and execution of example code on my aging laptop. Focused ‚Ä¶","tags":["AI","Stable Diffusion","Optimum-Intel","OpenVINO","Text to Image"],"title":"OpenVINO, Optimum-Intel, CPU: An Exploration in Model Optimization"},{"content":"In this post, I am delighted to share my journey of seamlessly integrating Java programming within Jupyter notebooks.\nSetup Commencing with the selection of a pertinent Jupyter Docker Stack image, as detailed in the Jupyter Docker Stacks documentation\r, the following Docker command initializes the setup:\ndocker pull quay.io/jupyter/minimal-notebook:notebook-7.0.6\rCopy\rSubsequently, the Docker image is run on a Windows WSL environment, with the host IP set to 192.168.68.114:\ndocker run -p 192.168.68.114:10000:8888 -v ~/work:/home/jovyan/work quay.io/jupyter/minimal-notebook:notebook-7.0.6\rCopy\rThe Jupyter environment becomes accessible by navigating to:\nhttp://192.168.68.114:10000/lab?token=52212ee09c057a6c8ef0c6db38eed07bf47037f76a2d8199\rCopy\rFurther, the Docker image is tagged and pushed to a local registry:\ndocker tag quay.io/jupyter/minimal-notebook:notebook-7.0.6 registry.local:5000/jupyter-minimal-notebook:notebook-7.0.6 docker push registry.local:5000/jupyter-minimal-notebook:notebook-7.0.6\rCopy\rDeployment in HomeLab The deployment in the HomeLab environment commences with the creation of a designated namespace:\nkc create ns jupyter\rCopy\rFollowing this, the deployment configuration is provided via a deploy.yaml file:\napiVersion: apps/v1 kind: Deployment metadata: name: jupyter-minimal-notebook namespace: jupyter spec: replicas: 1 selector: matchLabels: app: jupyter-minimal-notebook template: metadata: labels: app: jupyter-minimal-notebook spec: containers: - name: jupyter-minimal-notebook image: registry.local:5000/jupyter-minimal-notebook:notebook-7.0.6 ports: - containerPort: 8888 volumeMounts: - name: jupyter-store mountPath: /work subPath: ./jupyter-work imagePullSecrets: - name: regcred volumes: - name: jupyter-store persistentVolumeClaim: claimName: jupyter-nfs\rCopy\rAdditionally, a svc.yaml file specifies the service configuration:\napiVersion: v1 kind: Service metadata: name: jupyter-minimal-notebook-svc namespace: jupyter spec: selector: app: jupyter-minimal-notebook type: LoadBalancer ports: - name: http protocol: TCP port: 80 targetPort: 8888\rCopy\rUpon successful application of these changes in the HomeLab, the application logs yield the following output:\n# [C 2024-01-20 15:35:02.221 ServerApp] # To access the server, open this file in a browser: # file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html # Or copy and paste one of these URLs: # http://jupyter-minimal-notebook-b4fb98bbd-cxttl:8888/lab?token=2251c5c06cdfc487297057534dbc62de7589bcfa24f6be4a # http://127.0.0.1:8888/lab?token=2251c5c06cdfc487297057534dbc62de7589bcfa24f6be4a # [I 2024-01-20 15:35:03.813 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server\rCopy\rIn this instance, the external IP is recorded as 192.168.68.225. Access to Jupyter is achieved by navigating to:\nhttp://192.168.68.225/lab?token=2251c5c06cdfc487297057534dbc62de7589bcfa24f6be4a\rCopy\rJava Kernel Integration The integration of Java within the Jupyter notebook environment is facilitated by JBang\r, a tool simplifying the creation, editing, and execution of Java programs. Following the guidelines provided by Jupyter Java\r, let\u0026rsquo;s create a new notebook within the designated work folder.\n!pip install jbang import jbang jbang.exec(\u0026#34;trust add https://github.com/jupyter-java\u0026#34;) jbang.exec(\u0026#34;install-kernel@jupyter-java --java 17\u0026#34;)\rCopy\rThe Java Kernel can be selected seamlessly:\nTesting with Java Kernel A simple \u0026ldquo;Hello World\u0026rdquo; program is executed in the new IJava Notebook:\nSystem.out.println(\u0026#34;Hello World!\u0026#34;)\rCopy\rNext, let\u0026rsquo;s try with importing the commons-lang3 library:\n%%loadFromPOM \u0026lt;!-- https://mvnrepository.com/artifact/org.apache.commons/commons-lang3 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-lang3\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;\rCopy\rHere\u0026rsquo;s the sample Java code snippet which demonstrates the usage of the Apache Commons Lang library:\nimport org.apache.commons.lang3.StringUtils; String sampleString = \u0026#34;Hello, Apache Commons!\u0026#34;; // Check if the string is empty or null using Commons Lang if (StringUtils.isNotEmpty(sampleString)) { System.out.println(\u0026#34;The string is not empty or null.\u0026#34;); } else { System.out.println(\u0026#34;The string is empty or null.\u0026#34;); }\rCopy\rExploring Java in PyKernel Referencing to the JBang Jupyter tutorial\r, an alternate method to experiment with Java involves using JBang within the Python kernel:\nimport os\rif os.name == \u0026#39;nt\u0026#39;:\r!iex \u0026#34;\u0026amp; { $(iwr https://ps.jbang.dev) } ‚Ä¶","date":"2024-01-21","permalink":"https://seehiong.github.io/posts/2024/01/java-integration-with-jupyter-notebooks/","summary":"Embark on a seamless integration of Java into Jupyter notebooks with this comprehensive guide. Beginning with the selection of a relevant Jupyter ‚Ä¶","tags":["Java","K3s","K8s","Jupyter","HomeLab","PyKernel","JBang"],"title":"Java Integration with Jupyter Notebooks"},{"content":"In this comprehensive exploration, we delve into the realm of Autogen Studio\r, a powerful tool designed to streamline the rapid prototyping of multi-agent solutions for various tasks.\nGetting Started The journey begins with the initial setup. A new Python virtual environment is created using Conda, followed by the installation of Autogen Studio and the essential configuration of API keys.\nconda create -n autogenstudio python=3.10 conda activate autogenstudio pip install autogenstudio export OPENAI_API_KEY=sk-xxxx autogenstudio ui --port 8081\rCopy\rPlaytime with OpenAI API Navigate to http://localhost:8081, access the Playground and initiate a General Agent Workflow session. We experiment with sample prompts like Stock Price and Paint to observe Autogen Studio\u0026rsquo;s capabilities.\nHarnessing LM Studio API Diving deeper, we explore the behavior of different language models (LLMs) using the LM Studio API. Employing the Mistral Instruct 7B\rmodel, we compare responses to prompts like Stock Price and Paint.\nConfiguration Insights These are my key changes for the local LLM:\nSystem Message: Reply TERMINATE when you are satisfied with the response.\rCopy\rPrimary Assistant Setting: Model Configuration: Default Agent Workflow setting: Note\rEven if OpenAI API is not employed for inference, it remains imperative to set the OPENAI_API_KEY, ensuring seamless functionality. Set it to an arbitrary value: export OPENAI_API_KEY=123.\rConnectivity with HomeLab API If you followed my previous post\r, I have setup few LLM in my HomeLab. Let\u0026rsquo;s try to connect to http://api.local/dolphin-mistral/v1\r:\nNote\rWhen editing the Default Agent Workflow, we need to create a new Playground session for the new changes to take effect.\nFor the same reason where different LLM reacts to the prompts differently, there are some that takes very long to complete. To terminate, press CTRL-Z for Windows command prompt and from WSL:\nfuser 8081/tcp # Sample result: # 8081/tcp: 5929 kill -9 [5929]\rCopy\rDockerization for Seamless Deployment To ensure portability and ease of deployment, we encapsulate Autogen Studio in a Docker image. The Dockerfile configuration and steps for building, testing, and publishing the image are detailed.\nFROM python:3.11-slim-bookworm RUN : \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\ software-properties-common \\ \u0026amp;\u0026amp; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\ python3-venv \\ \u0026amp;\u0026amp; apt-get clean \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; : RUN python3 -m venv /venv ENV PATH=/venv/bin:$PATH EXPOSE 8081 ENV OPENAI_API_KEY=123 RUN cd /venv; pip install autogenstudio # Pre-load popular packages as per https://learnpython.com/blog/most-popular-python-packages/ RUN pip install numpy pandas matplotlib seaborn scikit-learn requests urllib3 nltk pillow pytest beautifulsoup4 CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;autogenstudio ui --host 0.0.0.0 --port 8081\u0026#34;]\rCopy\rBuild the image:\ndocker build . -t autogenstudio:latest\rCopy\rTest run the image:\ndocker run -p 8081:8081 -it autogenstudio:latest\rCopy\rPublish to HomeLab registry:\ndocker tag autogenstudio:latest registry.local:5000/autogenstudio:latest docker push registry.local:5000/autogenstudio:latest\rCopy\rDeployment in HomeLab Facilitating deployment in the HomeLab environment involves creating Kubernetes deployment and service YAML files. The final section guides through these steps, resulting in direct access to Autogen Studio from within the HomeLab network.\nLet\u0026rsquo;s prepare the deploy.yaml file:\napiVersion: apps/v1 kind: Deployment metadata: name: autogen-studio namespace: llm spec: replicas: 1 selector: matchLabels: app: autogen-studio template: metadata: labels: app: autogen-studio spec: containers: - name: autogen-studio image: registry.local:5000/autogenstudio:latest imagePullPolicy: IfNotPresent ports: - containerPort: 8081 imagePullSecrets: - name: regcred\rCopy\rThis is the svc.yaml file:\napiVersion: v1 kind: Service metadata: name: autogen-studio-svc namespace: llm spec: selector: app: autogen-studio type: LoadBalancer ports: - name: http protocol: TCP port: 80 targetPort: 8081\rCopy\rAfter applying these yaml files, check the external IP from portainer:\nAnd that\u0026rsquo;s it! We can now access Autogen Studio directly from our HomeLab!\nIn conclusion, this exploration not only showcases Autogen Studio\u0026rsquo;s capabilities in AI development but also provides practical insights into its integration with different APIs and deployment scenarios. Autogen Studio emerges as a versatile tool for AI enthusiasts and professionals alike.\n","date":"2024-01-14","permalink":"https://seehiong.github.io/posts/2024/01/exploring-autogen-studio/","summary":"In this exploration of Autogen Studio, we navigated through the AI landscape, harnessing the LM Studio API to compare responses from diverse language ‚Ä¶","tags":["AI","OpenAI","LM Studio","Autogen","AutogenStudio","HomeLab"],"title":"Exploring Autogen Studio"},{"content":"In this post, we delve into the deployment of Lightweight Language Models (LLMs) using WasmEdge\r, a lightweight, high-performance, and extensible WebAssembly runtime. This setup is tailored to run LLMs in our previously configured HomeLab environment.\nPreparation To establish an OpenAI-compatible API server\r, begin by downloading the API server application:\ncurl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm\rCopy\rFor Rust-based Llama 2 inference\r, we require the Wasi-NN\rplugin. The Dockerfile below reflects this configuration:\nFROM wasmedge/wasmedge:ubuntu-build-gcc ENV template sample_template ENV model sample_model RUN mkdir /models RUN apt update \u0026amp;\u0026amp; apt install -y curl git ninja-build RUN git clone https://github.com/WasmEdge/WasmEdge.git -b0.14.0-alpha.1 # Ubuntu/Debian with OpenBLAS RUN apt update RUN apt install -y software-properties-common lsb-release cmake unzip pkg-config RUN apt install -y libopenblas-dev RUN cmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\ -DWASMEDGE_PLUGIN_WASI_NN_BACKEND=\u0026#34;GGML\u0026#34; \\ -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_BLAS=ON \\ . RUN cmake --build build RUN cmake --install build COPY ./llama-api-server.wasm /WasmEdge RUN export WASMEDGE_PLUGIN_PATH=/WasmEdge/build/plugins/wasi_nn WORKDIR /WasmEdge CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;wasmedge --dir .:. --nn-preload default:GGML:AUTO:/models/\\\u0026#34;$model\\\u0026#34; llama-api-server.wasm --prompt-template ${template} --log-stat\u0026#34;]\rCopy\rThe template and model environment variables in the Dockerfile are parameters that can be customized later in the K3s YAML file.\nBuild and push the image to registry:\ndocker build . -t wasmedge-wasi-nn:0.14.0-alpha.1 docker tag wasmedge-wasi-nn:0.14.0-alpha.1 registry.local:5000/wasmedge-wasi-nn:0.14.0-alpha.1 docker push registry.local:5000/wasmedge-wasi-nn:0.14.0-alpha.1\rCopy\rDownload the LLM:\nwget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q4_K_M.gguf\rCopy\rRun the image:\ndocker run -p 8080:8080 -v $PWD:/models -e \u0026#34;model=llama-2-7b-chat.Q4_K_M.gguf\u0026#34; -e \u0026#34;template=llama-2-chat\u0026#34; wasmedge-wasi-nn:0.14.0-alpha.1\rCopy\rTest the API server:\ncurl -X POST http://localhost:8080/v1/models -H \u0026#39;accept:application/json\u0026#39; | jq\rCopy\rDeploying to HomeLab After moving the downloaded LLM to the NAS server, create the deploy.yaml file:\napiVersion: apps/v1 kind: Deployment metadata: name: wasmedge-llama2-7b-chat namespace: llm spec: replicas: 1 selector: matchLabels: app: wasmedge-llama2-7b-chat template: metadata: labels: app: wasmedge-llama2-7b-chat spec: containers: - name: wasmedge-llama2-7b-chat image: registry.local:5000/wasmedge-wasi-nn:0.14.0-alpha.1 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 env: - name: model value: llama-2-7b-chat.Q4_K_M.gguf - name: template value: llama-2-chat resources: requests: memory: \u0026#34;8Gi\u0026#34; limits: memory: \u0026#34;8Gi\u0026#34; volumeMounts: - name: models-store mountPath: /models imagePullSecrets: - name: regcred volumes: - name: models-store persistentVolumeClaim: claimName: nfs-models\rCopy\rCreate the corresponding svc.yaml file:\napiVersion: v1 kind: Service metadata: name: wasmedge-llama2-7b-chat-svc namespace: llm spec: selector: app: wasmedge-llama2-7b-chat type: ClusterIP ports: - name: http protocol: TCP port: 80 targetPort: 8080\rCopy\rApply the changes:\nkca deploy.yaml kca svc.yaml\rCopy\rNext, set up both the Kong service and route:\nIn Action Chnage to using the new path in the Langchain4j application:\nprivate static final String LOCAL_AI_URL = \u0026#34;http://api.local/wasmedge-llama2-7b-chat/v1\u0026#34;;\rCopy\rAfter running the Spring Boot application, observe the result from the retrieve API:\nAnd that concludes our setup! Feel free to drop any questions or comments below.\n","date":"2024-01-13","permalink":"https://seehiong.github.io/posts/2024/01/deploying-llms-with-wasmedge-in-homelab/","summary":"In this post, we explored deploying Lightweight Language Models (LLMs) using WasmEdge, a high-performance WebAssembly runtime, within a HomeLab ‚Ä¶","tags":["LLaMA","WasmEdge","AI","HomeLab","Java","LLM","Langchain4j"],"title":"Deploying LLMs with WasmEdge in HomeLab"},{"content":"If you\u0026rsquo;ve been following the previous post\r, you might have observed that deploying LLM may not be as scalable. In this post, we delve into the integration of NFS (Network File System) to externalize model environment variables. This approach eliminates the need to rebuild a new image each time a new LLM (Language Model) is introduced into your workflow.\nSetting up NFS Let\u0026rsquo;s start by setting up NFS to connect to my recently acquired TerraMaster NAS.\nsudo apt install nfs-common # Create an arbitrary folder for mounting sudo mkdir -p /mnt/shared # Mounting to the external NAS sudo mount -t nfs 192.168.68.111:/mnt/usb/usbshare1 /mnt/shared\rCopy\rFor a permanent mount, append the following to /etc/fstab:\n192.168.68.111:/mnt/usb/usbshare1 /mnt/shared nfs auto 0 0\rCopy\rVerify the settings:\nsudo umount /mnt/shared sudo mount -a df -h\rCopy\rIntegrating NFS with K3s Utilizing the K3s packaged components\r, any files dropped into /var/lib/rancher/k3s/server/manifests are automatically picked up. Referring to this reference\r, let\u0026rsquo;s create the /var/lib/rancher/k3s/server/manifests/nfs-models.yaml file with the HelmChart content:\napiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: nfs-models namespace: llm spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner set: nfs.server: 192.168.68.111 nfs.path: /mnt/usb/usbshare1 storageClass.name: nfs-models storageClass.reclaimPolicy: Retain storageClass.accessModes: ReadWriteMany nfs.reclaimPolicy: Retain\rCopy\rCheck the creation of the storageClass:\nkc get sc\rCopy\rThe goal is to share the PersistentVolumeClaim (PVC) with all LLM pods. This is the pvc.yaml:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-models namespace: llm spec: accessModes: - ReadWriteMany storageClassName: nfs-models resources: requests: storage: 200Gi\rCopy\rApply the change:\nkca pvc.yaml\rCopy\rCopy the downloaded LLM models to the shared folder, which should look something like:\n$ ls /mnt/shared/\rllm-nfs-models-pvc-e1dffafe-b3c0-469c-b988-cf46c57f666a\rCopy\rBuilding the llama-cpp-python image To make the model dynamic via environment variable, modify the Dockerfile:\nFROM python:3-slim-bullseye ENV model sample_model # We need to set the host to 0.0.0.0 to allow outside access ENV HOST 0.0.0.0 # Install the package RUN apt update \u0026amp;\u0026amp; apt install -y libopenblas-dev ninja-build build-essential pkg-config RUN pip install --upgrade pip RUN python -m pip install --no-cache-dir --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context RUN CMAKE_ARGS=\u0026#34;-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\u0026#34; pip install --no-cache-dir --force-reinstall llama_cpp_python==0.2.27 --verbose # Run the server CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;python3 -m llama_cpp.server --model /models/\\\u0026#34;$model\\\u0026#34;\u0026#34;]\rCopy\rBuild the image:\n# Build the image docker build . -t llama-cpp-python:0.2.27 # Tag it docker tag llama-cpp-python:0.2.27 192.168.68.115:30500/llama-cpp-python:0.2.27 # Push to the registry docker push 192.168.68.115:30500/llama-cpp-python:0.2.27\rCopy\rDeploying llama-cpp-python image Here is the deploy.yaml file for deploying the phi2 model:\napiVersion: apps/v1 kind: Deployment metadata: name: llama-phi2 namespace: llm spec: replicas: 1 selector: matchLabels: app: llama-phi2 template: metadata: labels: app: llama-phi2 name: llama-phi2 spec: containers: - name: llama-phi2 image: 192.168.68.115:30500/llama-cpp-python:0.2.27 imagePullPolicy: IfNotPresent ports: - containerPort: 8000 securityContext: capabilities: add: - IPC_LOCK volumeMounts: - name: models-store mountPath: /models env: - name: model value: phi-2.Q4_K_M.gguf resources: requests: memory: \u0026#34;6Gi\u0026#34; limits: memory: \u0026#34;6Gi\u0026#34; imagePullSecrets: - name: regcred volumes: - name: models-store persistentVolumeClaim: claimName: nfs-models\rCopy\rThe corresponding svc.yaml (service) file is as follows:\napiVersion: v1 kind: Service metadata: name: llama-phi2-svc namespace: llm spec: selector: app: llama-phi2 type: ClusterIP ports: - name: http protocol: TCP port: 80 targetPort: 8000\rCopy\rThese changes are applied with the following commands:\nkca deploy.yaml kca svc.yaml\rCopy\rUpdate both Kong service and route:\nNow, you can test the connection with:\ncurl http://api.local/phi2/v1/models | jq\rCopy\rDeploying a New LLM Let\u0026rsquo;s deploy stabelm-zephyr-3b-GGUF\rby:\ncd /mnt/shared/llm-nfs-models-pvc-e1dffafe-b3c0-469c-b988-cf46c57f666a # Downloads the stablelm wget https://huggingface.co/TheBloke/stablelm-zephyr-3b-GGUF/resolve/main/stablelm-zephyr-3b.Q4_K_M.gguf\rCopy\rIn a similar fashion, the deploy.yaml file for the stablelm-zephyr LLM is as follows:\napiVersion: apps/v1 kind: Deployment metadata: name: llama-stablelm-zephyr namespace: llm spec: replicas: 1 selector: matchLabels: app: llama-stablelm-zephyr template: metadata: labels: app: llama-stablelm-zephyr name: llama-stablelm-zephyr spec: ‚Ä¶","date":"2024-01-07","permalink":"https://seehiong.github.io/posts/2024/01/integrating-nfs-for-improved-scalability/","summary":"In this post, we explored the integration of NFS to enhance scalability in deploying LLM models within a home lab. Setting up NFS involved connecting ‚Ä¶","tags":["K3s","K8s","NFS","HomeLab","NAS","LLM","Phi2"],"title":"Integrating NFS for Improved Scalability"},{"content":"This post will guide you through the process of configuring Kong Gateway OSS and Kong Ingress Controller (KIC) separately and integrating Kong into our AI workflow.\nIntegrate via Kong Gateway OSS Configuration If you followed my earlier guide on setting up Kong Gateway\rsetup, you likely use api.local:8000 to access the API.\nLet\u0026rsquo;s revisit and update KONG_ADMIN_GUI_URL environment variable in the kong-deploy-svc.yaml file:\napiVersion: apps/v1 kind: Deployment metadata: name: kong-gateway namespace: llm spec: replicas: 1 selector: matchLabels: app: kong-gateway template: metadata: labels: app: kong-gateway spec: initContainers: - name: kong-init image: 192.168.68.115:30500/kong-image:3.5.0 command: [\u0026#34;kong\u0026#34;, \u0026#34;migrations\u0026#34;, \u0026#34;bootstrap\u0026#34;] env: - name: KONG_DATABASE value: \u0026#34;postgres\u0026#34; - name: KONG_PG_HOST value: \u0026#34;kong-db-svc\u0026#34; - name: KONG_PG_PASSWORD value: \u0026#34;kongpass\u0026#34; - name: KONG_PASSWORD value: \u0026#34;test\u0026#34; containers: - name: kong-gateway image: 192.168.68.115:30500/kong-image:3.5.0 ports: - containerPort: 8000 - containerPort: 8443 - containerPort: 8001 - containerPort: 8444 - containerPort: 8002 - containerPort: 8445 - containerPort: 8003 - containerPort: 8004 env: - name: KONG_DATABASE value: \u0026#34;postgres\u0026#34; - name: KONG_PG_HOST value: \u0026#34;kong-db-svc\u0026#34; - name: KONG_PG_USER value: \u0026#34;kong\u0026#34; - name: KONG_PG_PASSWORD value: \u0026#34;kongpass\u0026#34; - name: KONG_PROXY_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PROXY_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_ADMIN_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_ADMIN_LISTEN value: \u0026#34;0.0.0.0:8001\u0026#34; - name: KONG_ADMIN_GUI_URL value: \u0026#34;http://api.local:8002\u0026#34; - name: KONG_LICENSE_DATA value: \u0026#34;\u0026#34; nodeSelector: kubernetes.io/hostname: hp\rCopy\rThe next change involves port 80:\napiVersion: v1 kind: Service metadata: name: kong-gateway-svc namespace: llm spec: selector: app: kong-gateway type: LoadBalancer ports: - name: http protocol: TCP port: 80 targetPort: 8000 - name: admin protocol: TCP port: 8001 targetPort: 8001 - name: manager protocol: TCP port: 8002 targetPort: 8002\rCopy\rApply the changes:\nkca kong-deploy-svc.yaml\rCopy\rThat\u0026rsquo;s it! To access the Kong Manager, navigate to http://api.local:8002\rIntegration with Langchain4j Application Let\u0026rsquo;s modify our previous Langchain4j application\rcodebase. By updating the LOCAL_AI_URL, we can now access our local LLM via Kong path-based API.\n@Service public class ModelService { ... private static final String LOCAL_AI_URL = \u0026#34;http://api.local/localai/v1\u0026#34;; }\rCopy\rBy integrating Kong API to our AI workflow, we can easily add on new path-based API and create any number of local LLM that suits our application needs.\nRemember, since we are using the POST HTTP verb here, you need to add the POST method to the route too!\nNote\rThe default connection timeout is 60,000 milliseconds. If you encounter errors like \u0026ldquo;The upstream server is timing out\u0026rdquo;:\rIncrease the timeout setting to, say, 600,000 in Kong Manager by editing your Gateway Services\u0026rsquo;s advanced fields:\nIntegration via Kong Ingress Controller (KIC) Next, I will demonstrate the configuration via KIC. Since there is no GUI along this path, let\u0026rsquo;s explore how to set things up to achieve similar results.\nIf you followed my previous exploration of KIC\r, and attempted to execute the Langchain4j application with following URL, you might encounter the same \u0026ldquo;The upstream server is timing out\u0026rdquo; error:\n@Service public class ModelService { ... private static final String LOCAL_AI_URL = \u0026#34;http://kic.local/localai/v1\u0026#34;; }\rCopy\rChecking the Kong annotations\r, it states that konghq.com/connect-timeout should be set at the service resource level.\nHere is my updated localai-svc.yml for the localai LLM resource. (you may refer to my previous LLM setup\r):\napiVersion: v1 kind: Service metadata: name: localai-server-svc namespace: llm annotations: konghq.com/connect-timeout: \u0026#34;600000\u0026#34; konghq.com/read-timeout: \u0026#34;600000\u0026#34; konghq.com/write-timeout: \u0026#34;600000\u0026#34; spec: selector: app: localai-server type: NodePort ports: - name: http protocol: TCP port: 80 targetPort: 8080 nodePort: 30808\rCopy\rApply the changes:\nkca localai-svc.yml\rCopy\rThat\u0026rsquo;s it! You should now be able to access the endpoint. Choose the setting that better suits your application needs based on your use case!\n","date":"2024-01-06","permalink":"https://seehiong.github.io/posts/2024/01/integration-of-kong-into-ai-workflow/","summary":"This comprehensive guide navigates through configuring Kong OSS and Kong Ingress Controller (KIC), seamlessly integrating Kong into an AI workflow. ‚Ä¶","tags":["K3s","K8s","AI","Kong","KIC","Langchain4j","HomeLab"],"title":"Integration of Kong into AI Workflow"},{"content":"Wishing everyone a Happy New Year 2024! In this post, I shift focus from my previous discussion on Kong Gateway\rto delve into the setup of the Kong Ingress Controller (KIC). Keeping it concise and celebratory for the New Year!\nPreparation Helm\rserves as a Kubernetes package manager. To install it, execute the following command:\nsudo snap install helm --classic\rCopy\rDepending on your configuration, from my K3s master node, I set up my kubeconfig with:\ncp /etc/rancher/k3s/k3s.yaml ~/.kube/config chmod 600 ~/.kube/config\rCopy\rInstalling Kong Ingress Controller Following the Install KIC\rguide:\nwget https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml # kca is simply an alias for kubectl apply -f kca standard-install.yaml\rCopy\rNext, create a kong-gatewayclass.yaml:\necho \u0026#34; --- apiVersion: gateway.networking.k8s.io/v1 kind: GatewayClass metadata: name: kong annotations: konghq.com/gatewayclass-unmanaged: \u0026#39;true\u0026#39; spec: controllerName: konghq.com/kic-gateway-controller --- apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: kong spec: gatewayClassName: kong listeners: - name: proxy port: 80 protocol: HTTP \u0026#34; \u0026gt;\u0026gt; kong-gatewayclass.yaml kca kong-gatewayclass.yaml\rCopy\rInstall Kong via Helm Add the Kong Helm charts:\nhelm repo add kong https://charts.konghq.com helm repo update\rCopy\rInstall Kong Ingress Controller and Kong Gateway:\nhelm install kong kong/ingress -n kong --create-namespace Copy\rTest connectivity with:\nexport PROXY_IP=$(kubectl get svc --namespace kong kong-gateway-proxy -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) echo $PROXY_IP curl -i $PROXY_IP\rCopy\rInfo\rFurther investigation into Kong charts\rreveals that KONG_ADMIN_GUI_URL might be restricted to enterprise editions.\rAdding Kong Ingresses To replicate the previous routing, set up kong-ingress.yaml:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kong-ingress namespace: llm annotations: konghq.com/strip-path: \u0026#39;true\u0026#39; spec: ingressClassName: kong rules: - http: paths: - path: /localai pathType: ImplementationSpecific backend: service: name: localai-server-svc port: number: 80 - http: paths: - path: /phi2 pathType: ImplementationSpecific backend: service: name: llama-phi2-svc port: number: 80\rCopy\rApply the ingress rules:\nkca kong-ingress.yaml\rCopy\rKIC In Action For testing purposes, I added the following to Windows System32\\drivers\\etc\\hosts file:\n192.168.68.222 kic.local\rCopy\rTesting on the localai path from WSL:\ncurl http://kic.local/localai/v1/models\rCopy\rTesting on the phi2 path:\ncurl http://kic.local/phi2/docs\rCopy\rThat concludes the exploration of KIC! Enjoy experimenting! Happy New Year!\n","date":"2024-01-01","permalink":"https://seehiong.github.io/posts/2024/01/exploring-kong-ingress-controller-kic/","summary":"Embark on a journey into the new year by exploring Kong Ingress Controller (KIC) in your home lab. This guide, transitioning from a previous ‚Ä¶","tags":["K3s","K8s","Helm","Kong","KIC","HomeLab"],"title":"Exploring Kong Ingress Controller (KIC)"},{"content":"In this comprehensive guide, we will walk through the process of integrating Kong\r, a robust unified API platform, into our home lab environment.\nPrerequistes To begin, I will start with a fresh Ubuntu server instance. We\u0026rsquo;ll start by installing Docker and configuring it for non-root usage:\nsudo apt install docker.io sudo usermod -aG docker pi # Run Docker without sudo by logging back in or executing this su - pi\rCopy\rBuilding a Custom Kong Image As we are utilizing the open-source version (OSS), let\u0026rsquo;s create a custom Kong image\r.\nFirst, download the docker-entrypoint.sh script:\nwget https://raw.githubusercontent.com/Kong/docker-kong/master/docker-entrypoint.sh chmod +x docker-entrypoint.sh\rCopy\rNext, obtain the Debian build from here\rand rename it to kong.deb:\nwget https://packages.konghq.com/public/gateway-35/deb/debian/pool/bullseye/main/k/ko/kong_3.5.0/kong_3.5.0_amd64.deb\rCopy\rThe Dockerfile, accompanied by the docker-entrypoint.sh and kong.deb files located in the same folder, is as follows:\nFROM debian:bullseye-slim COPY kong.deb /tmp/kong.deb RUN set -ex; \\ apt-get update \\ \u0026amp;\u0026amp; apt-get install --yes /tmp/kong.deb \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; rm -rf /tmp/kong.deb \\ \u0026amp;\u0026amp; chown kong:0 /usr/local/bin/kong \\ \u0026amp;\u0026amp; chown -R kong:0 /usr/local/kong \\ \u0026amp;\u0026amp; ln -s /usr/local/openresty/luajit/bin/luajit /usr/local/bin/luajit \\ \u0026amp;\u0026amp; ln -s /usr/local/openresty/luajit/bin/luajit /usr/local/bin/lua \\ \u0026amp;\u0026amp; ln -s /usr/local/openresty/nginx/sbin/nginx /usr/local/bin/nginx \\ \u0026amp;\u0026amp; kong version COPY docker-entrypoint.sh /docker-entrypoint.sh USER kong ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] EXPOSE 8000 8443 8001 8444 8002 8445 8003 8446 8004 8447 STOPSIGNAL SIGQUIT HEALTHCHECK --interval=10s --timeout=10s --retries=10 CMD kong health CMD [\u0026#34;kong\u0026#34;, \u0026#34;docker-start\u0026#34;]\rCopy\rBuild the Kong custom image:\ndocker build --platform linux/amd64 --no-cache -t kong-image:3.5.0 . # Check kong version, should return 3.5.0 docker run -it --rm kong-image:3.5.0 kong version\rCopy\rSince we plan to configure services and routes later, we cannot use the DB-less mode. Follow the install Kong Gateway on Docker\rinstallation guide to create the Docker network and start a PostgreSQL container:\ndocker network create kong-net docker run -d --name kong-database \\ --network=kong-net \\ -p 5432:5432 \\ -e \u0026#34;POSTGRES_USER=kong\u0026#34; \\ -e \u0026#34;POSTGRES_DB=kong\u0026#34; \\ -e \u0026#34;POSTGRES_PASSWORD=kongpass\u0026#34; \\ postgres:13\rCopy\rPrepare the database with our custom image:\ndocker run --rm --network=kong-net \\ -e \u0026#34;KONG_DATABASE=postgres\u0026#34; \\ -e \u0026#34;KONG_PG_HOST=kong-database\u0026#34; \\ -e \u0026#34;KONG_PG_PASSWORD=kongpass\u0026#34; \\ -e \u0026#34;KONG_PASSWORD=test\u0026#34; \\ kong-image:3.5.0 kong migrations bootstrap\rCopy\rLastly, replace the GUI URL with your IP address and run the Kong Gateway:\ndocker run -d --name kong-gateway \\ --network=kong-net \\ -e \u0026#34;KONG_DATABASE=postgres\u0026#34; \\ -e \u0026#34;KONG_PG_HOST=kong-database\u0026#34; \\ -e \u0026#34;KONG_PG_USER=kong\u0026#34; \\ -e \u0026#34;KONG_PG_PASSWORD=kongpass\u0026#34; \\ -e \u0026#34;KONG_PROXY_ACCESS_LOG=/dev/stdout\u0026#34; \\ -e \u0026#34;KONG_ADMIN_ACCESS_LOG=/dev/stdout\u0026#34; \\ -e \u0026#34;KONG_PROXY_ERROR_LOG=/dev/stderr\u0026#34; \\ -e \u0026#34;KONG_ADMIN_ERROR_LOG=/dev/stderr\u0026#34; \\ -e \u0026#34;KONG_ADMIN_LISTEN=0.0.0.0:8001\u0026#34; \\ -e \u0026#34;KONG_ADMIN_GUI_URL=http://192.168.68.113:8002\u0026#34; \\ -e KONG_LICENSE_DATA \\ -p 8000:8000 \\ -p 8443:8443 \\ -p 8001:8001 \\ -p 8444:8444 \\ -p 8002:8002 \\ -p 8445:8445 \\ -p 8003:8003 \\ -p 8004:8004 \\ kong-image:3.5.0\rCopy\rThe following commands can be helpful during the initial setup:\n# Kill and remove the docker container docker kill kong-gateway docker container rm kong-gateway # Tail the Kong Gateway logs for debugging purposes docker logs -f kong-gateway\rCopy\rAs the final step, after confirming the successful operation of the custom image, proceed to join as a K3s node, tag the image and push it to our home lab registry:\n# Join as K3s node curl -sfL https://get.k3s.io | K3S_URL=https://192.168.68.115:6443 K3S_TOKEN=K10e848701b18977c63d7abfce920cf66c0f19bdd18a40862b2e7a14b89c4eb2742::server:ac92f2b7ccebbb46bf241bdaea3c99bf sh - echo -e \u0026#39; --docker\\n --insecure-registry=http://192.168.68.115:30500\u0026#39; | sudo tee -a /etc/systemd/system/k3s-agent.service # Restart K3s agent sudo systemctl daemon-reload sudo systemctl restart k3s-agent # Setup local registry sudo bash -c \u0026#39;cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;192.168.68.115:30500\u0026#34; ] } EOF\u0026#39; sudo bash -c \u0026#39;cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/default/docker DOCKER_OPTS=\u0026#34;--config-file=/etc/docker/daemon.json\u0026#34; EOF\u0026#39; sudo systemctl restart docker docker tag kong-image:3.5.0 192.168.68.115:30500/kong-image:3.5.0 docker push 192.168.68.115:30500/kong-image:3.5.0\rCopy\rDeploying PostgreSQL to the Home Lab For PostgreSQL deployment, use the kong-db-deploy-svc.yaml file, ensuring data storage on the host machine hp:\napiVersion: apps/v1 ‚Ä¶","date":"2023-12-31","permalink":"https://seehiong.github.io/posts/2023/12/streamlining-api-management-with-kong/","summary":"This comprehensive guide walks you through integrating Kong, a powerful unified API platform, into your home lab environment. Starting with Docker ‚Ä¶","tags":["K3s","K8s","Kong","PostgreSQL","Metallb","HomeLab","Phi2"],"title":"Streamlining API Management with Kong"},{"content":"Referring to the Building an AI application with Langchaing4j\rguide, the deployment of necessary Docker images, LocalAI, and Chroma to our Home Lab is outlined.\nCreating custom LocalAI image Begin with pulling the latest image using the Install and Run Models\rguide:\ndocker pull quay.io/go-skynet/local-ai:v2.2.0\rCopy\rNow, run LocalAI from the ~/localai folder and download a model:\ndocker run -p 8080:8080 -v $PWD/models:/models -ti --rm quay.io/go-skynet/local-ai:v2.2.0 --models-path /models --context-size 2000 --threads 4 --debug=true # The model will be downloaded to ~/localai/models folder curl http://127.0.0.1:8080/models/apply -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;github:go-skynet/model-gallery/gpt4all-j.yaml\u0026#34;}\u0026#39;\rCopy\rNext, create a Dockerfile and include the downloaded model under our custom image:\nFROM quay.io/go-skynet/local-ai:v2.2.0 ENV HOST 0.0.0.0 RUN mkdir /models COPY ./models /models EXPOSE 8080 CMD [\u0026#34;/usr/bin/local-ai\u0026#34;]\rCopy\rBuild it with:\ndocker build . -t localai:v2.2.0 # Test run the custom image docker run -p 8080:8080 -ti --rm localai:v2.2.0 --models-path /models --context-size 2000 --threads 4 --debug=true # Check the model curl http://len:8080/models # Tag the image docker tag localai:v2.2.0 192.168.68.115:30500/localai:v2.2.0 # Push the image (refer \u0026#34;Docker Image Push and Deployment\u0026#34; of Deploying OpenAI-Compatible LLAMA CPP Server in Home Lab with K3S) docker push 192.168.68.115:30500/localai:v2.2.0\rCopy\rDeploying Custom LocalAI to Home Lab This is the localai-deploy.yaml file targeting to run from the len server:\napiVersion: apps/v1 kind: Deployment metadata: name: localai-server namespace: llm spec: replicas: 1 selector: matchLabels: app: localai-server template: metadata: labels: app: localai-server spec: containers: - name: localai-server image: 192.168.68.115:30500/localai:v2.2.0 ports: - containerPort: 8080 args: - \u0026#34;--models-path\u0026#34; - \u0026#34;/models\u0026#34; nodeSelector: kubernetes.io/hostname: len\rCopy\rThis is the localai-svc.yaml file, exposing nodePort as 30808:\napiVersion: v1 kind: Service metadata: name: localai-server-svc namespace: llm spec: selector: app: localai-server type: NodePort ports: - name: http protocol: TCP port: 80 targetPort: 8080 nodePort: 30808\rCopy\rDeploy to the Home Lab:\nkca localai-deploy.yaml kca localai-svc.yaml\rCopy\rNote\rIf you are unable to pull the image from the agent, please refer to my previous post\rand perform the following:\nsudo vi /etc/systemd/system/k3s-agent.service\rCopy\rAdd the following at the bottom of the existing file:\nExecStart=/usr/local/bin/k3s \\ agent \\ --docker --insecure-registry=http://192.168.68.115:30500\rCopy\rRestart the k3s-agent:\nsudo systemctl daemon-reload sudo systemctl restart k3s-agent\rCopy\rDeploying Chroma to Home Lab The HostPath /docker-data/chroma will be created in the HostName len. This is the chroma-deploy.yaml file to be deployed to our Home Lab:\napiVersion: apps/v1 kind: Deployment metadata: name: chroma-server namespace: llm spec: replicas: 1 selector: matchLabels: app: chroma-server template: metadata: labels: app: chroma-server name: chroma-server spec: containers: - name: chroma-server image: ghcr.io/chroma-core/chroma:0.4.21 ports: - containerPort: 8000 volumeMounts: - name: chroma-data mountPath: /chroma/chroma volumes: - name: chroma-data hostPath: path: \u0026#34;/docker-data/chroma\u0026#34; nodeSelector: kubernetes.io/hostname: len\rCopy\rTo expose this service for our usage, this is the chroma-svc.yaml:\napiVersion: v1 kind: Service metadata: name: chroma-server-svc namespace: llm spec: selector: app: chroma-server type: NodePort ports: - name: http protocol: TCP port: 80 targetPort: 8000 nodePort: 30800\rCopy\rAs usual, we will deploy using our alias command:\nkca chroma-deploy.yaml kca chroma-svc.yaml\rCopy\rIntegrating Langchain4j Application with Home Lab Returning to the Langchain4j application\rcodebase, certain changes are required to integrate our application into the Home Lab.\nEmbed Endpoint Modification With the above setup, this is the URL change (http://len:30800) made to EmbeddingServer.java:\nprivate EmbeddingStore\u0026lt;TextSegment\u0026gt; getEmbeddingStore() { if (embeddingStore == null) { embeddingStore = ChromaEmbeddingStore.builder().baseUrl(\u0026#34;http://len:30800\u0026#34;) .collectionName(randomUUID()).build(); } return embeddingStore; }\rCopy\rRetrieve Endpoint Modification Based on our LocalAI setup, in the ModelService.java, the LOCAL_AI_URL is changed to:\nprivate static final String LOCAL_AI_URL = \u0026#34;http://len:30808\u0026#34;;\rCopy\r","date":"2023-12-29","permalink":"https://seehiong.github.io/posts/2023/12/ai-integration-localai-chroma-and-langchain4j/","summary":"Explore AI integration in a home lab with LocalAI, Chroma, and Langchain4j. Begin by creating a custom LocalAI image, deploying it alongside Chroma, ‚Ä¶","tags":["K3s","K8s","Cluster","LocalAI","Langchain4j","HomeLab","Chroma","Java","Docker"],"title":"AI Integration: LocalAI, Chroma, and Langchain4j"},{"content":"In this post, I will provide updates on my transition to utilizing Hugo\rfor my tech blog.\nSetup Process To begin, I recommend installing Chocolately\r, a free and open-source package manager designed for Windows.\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://community.chocolatey.org/install.ps1\u0026#39;))\rCopy\rProceed to install Hugo using Chocolately:\nchoco install hugo-extended\rCopy\rAfter installation, I opted for the Ananke\rtheme. Upon completing your post, you can view your site at http://localhost:1313:\nhugo serve\rCopy\rGiscus Integration Giscus\rserves as a comment system powered by Github Discussions\r.\nI created a giscus.html file under the \\themes\\ananke\\layouts\\partials folder with the following contents:\n\u0026lt;p class=\u0026#34;f5 b mb3\u0026#34;\u0026gt;Post a comment:\u0026lt;/p\u0026gt; \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;xxxxxxxx/seehiong.github.io\u0026#34; data-repo-id=\u0026#34;R_xxxxxxxxxx\u0026#34; data-category=\u0026#34;General\u0026#34; data-category-id=\u0026#34;DIC_xxxxxxxxxxxxxxxx\u0026#34; data-mapping=\u0026#34;url\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;top\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;en\u0026#34; data-loading=\u0026#34;lazy\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt;\rCopy\rAdd the Giscus partial to the \\themes\\ananke\\layouts_default_\\single.html:\n{{- .Content -}} {{- partial \u0026#34;tags.html\u0026#34; . -}} \u0026lt;div class=\u0026#34;mt6 instapaper_ignoref\u0026#34;\u0026gt; {{ if .Site.Config.Services.Disqus.Shortname }} {{ template \u0026#34;_internal/disqus.html\u0026#34; . }} {{ end }} {{ if .Site.Params.commentoEnable }} {{- partial \u0026#34;commento.html\u0026#34; . -}} {{ end }} {{ if .Site.Params.giscusEnable }} {{- partial \u0026#34;giscus.html\u0026#34; . -}} {{ end }} \u0026lt;/div\u0026gt; {{- partial \u0026#34;post-paginator.html\u0026#34; . -}}\rCopy\rIn my config.toml:\n[params] giscusEnable = true\rCopy\rPagefind Installation Pagefind\ris a fully static search library.\nFirst, install Node.js\ron WSL.\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/master/install.sh | bash nvm ls nvm install --lts\rCopy\rNavigate to your working Hugo directory, install and run Pagefind\r:\nnpx pagefind --site \u0026#34;public\u0026#34;\rCopy\rInfo\rIf you are unable to see the site bar from hugo serve, copy the public/pagefind folder to your static folder.\rAdd the following to \\themes\\ananke\\layouts\\partials\\head-additions.html:\n\u0026lt;link href=\u0026#34;/pagefind/pagefind-ui.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;/pagefind/pagefind-ui.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\rCopy\rNext, adds these lines to \\themes\\ananke\\layouts\\partials\\site-header.html:\n\u0026lt;div style=\u0026#34;max-width: 50%; margin: clamp(2px, 1vw, 10px) auto;\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;search\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; window.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, (event) =\u0026gt; { new PagefindUI({ element: \u0026#34;#search\u0026#34;, showSubResults: true }); }); \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt;\rCopy\rEnjoy experimenting with Giscus and Pagefind on your Hugo site. I hope this information proves helpful in your endeavors!\nOptional Since I wanted to index only the main post, following pagefind indexing\r, these are the customizations that I made.\nAdd data-pagefind-body to \\themes\\ananke\\layouts_default\\single.html file: {{ define \u0026#34;main\u0026#34; }} {{ $section := .Site.GetPage \u0026#34;section\u0026#34; .Section }} \u0026lt;article class=\u0026#34;flex-l flex-wrap justify-between mw8 center ph3\u0026#34; data-pagefind-body\u0026gt; \u0026lt;header class=\u0026#34;mt4 w-100\u0026#34;\u0026gt; ...\rCopy\rAdd data-pagefind-ignore to \\themes\\ananke\\layouts\\partials\\summary-with-image.html file: {{ partial \u0026#34;func/warn\u0026#34; `You are currently using \u0026#39;partial \u0026#34;summary-with-image\u0026#34;\u0026#39; in your project templates. You should replace it with \u0026#39;.Render \u0026#34;summary-with-image\u0026#34;\u0026#39; as the use of this partial will be deprecated in future releases. More info here: https://github.com/theNewDynamic/gohugo-theme-ananke/releases/tag/v2.8.1` }} {{ $featured_image := partial \u0026#34;func/GetFeaturedImage.html\u0026#34; . }} \u0026lt;article class=\u0026#34;bb b--black-10\u0026#34; data-pagefind-ignore\u0026gt; ...\rCopy\rAdd data-pagefind-ignore to \\themes\\ananke\\layouts\\partials\\tags.html file: \u0026lt;ul class=\u0026#34;pa0\u0026#34; data-pagefind-ignore\u0026gt; {{ range .GetTerms \u0026#34;tags\u0026#34; }} ...\rCopy\r","date":"2023-12-26","permalink":"https://seehiong.github.io/posts/2023/12/upgrading-my-tech-blog-hugo-giscus-and-pagefind-integration/","summary":"Explore my journey of enhancing my tech blog by adopting Hugo, a powerful static site generator. Discover the streamlined setup process using ‚Ä¶","tags":["Hugo","Giscus","Pagefind","Blog","Chocolately","Ananke"],"title":"Upgrading My Tech Blog: Hugo, Giscus, and Pagefind Integration"},{"content":"In this guide, I\u0026rsquo;ll walk you through the process of installing GitLab\r, a comprehensive suite of tools for version control, continuous integration, continuous delivery, and more, in my Home Lab collection.\nPreparation After obtaining the latest Ubuntu Server\r, I utilized Rufus\r, a utility for formatting and creating bootable USB flash drives.\nInstallation Following the installation instructions\r, initiate a quick installation using the following command:\ncurl -s https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash\rCopy\rUpgrading to the Latest Version Referencing the official repositories for upgrading\r, upgrade GitLab to the latest version:\nsudo apt update \u0026amp;\u0026amp; sudo apt install gitlab-ce # List all the versions sudo apt-cache madison list gitlab-ce # Gitlab upgrade to a specific version, e.g. version 17.3.3 sudo apt install gitlab-ce=17.3.3-ce.0\rCopy\rSetting External URL Begin by configuring the external URL:\nsudo vi /etc/gitlab/gitlab.rb # Search for external_url and input the IP address external_url \u0026#39;http://192.168.68.126\u0026#39;\rCopy\rOnce configured, start the GitLab instance:\nsudo gitlab-ctl reconfigure\rCopy\rRetrieve the default admin account password:\nsudo vi /etc/gitlab/initial_root_password\rCopy\rVisit the configured address, http://192.168.68.126:\nDeactivate sign-up restrictions:\nClink the Save changes button:\nMigrating from Old to New Repository 1. Clone the Existing Repository Clone the existing repository to your local machine:\ngit clone \u0026lt;old_repo_url\u0026gt;\rCopy\r2. Set up SSH key Generate an SSH key on your working machine: ssh-keygen -t rsa -b 4096 eval \u0026#34;$(ssh-agent -s)\u0026#34; # Save the key to ~/.ssh/gitlab_id_rsa ssh-add ~/.ssh/gitlab_id_rsa # Export the public key cat ~/.ssh/gitlab_id_rsa.pub\rCopy\rUpdate shell profile (e.g. ~/.bashrc) by adding the following lines to the bottom: eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/gitlab_id_rsa\rCopy\rRestart your shell or source the configuration: source ~/.bashrc # or source ~/.zshrc for Zsh\rCopy\rSave the SSH public key in GitLab. Since th SSH key pair is not in the default location, save these settings in ~/.ssh/config file: Host 192.168.68.126 PreferredAuthentications publickey IdentityFile C:\\Users\\seehi\\.ssh\\gitlab_id_rsa\rCopy\r3. Create a New Repository Navigate to the cloned repository directory and update the remote URL:\ncd \u0026lt;local_repo_directory\u0026gt; # In this example, a new group called personal is created; the new repo is not yet created git remote set-url origin git@192.168.68.126:personal/langchain4j-spring-boot.git\rCopy\r4. Push to the New Repository Push the local repository:\ngit push -u origin main\rCopy\rInstall GitLab Runner Referencing Install GitLab Runner\r, add the official GitLab repository:\ncurl -L \u0026#34;https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh\u0026#34; | sudo bash\rCopy\rInstall the latest GitLab Runner version:\n# To install or update the GitLab Runner sudo apt-get update sudo apt-get install gitlab-runner # Verify if GitLab Runner is running sudo gitlab-runner status\rCopy\rInstall Docker Referring to Install Docker Engine on Ubuntu\r, set up Docker\u0026rsquo;s apt repository:\n# Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update # Install the Docker packages sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Verify Docker Engine installation sudo docker run hello-world # Adds user to docker group (needs to re-login to take effect) sudo usermod -aG docker pi\rCopy\rRegister GitLab Runner In the project settings, navigate to the CI/CD option, expand the Runners section, and click on the New project runner button:\nSet the tag (e.g., docker-linux), select Run untagged jobs, and click the Create runner button:\nOnce the runner is created, copy the command provided:\nNext, paste the command to register a runner\r:\n# You may use sudo to save config to /etc/gitlab-runner/config.toml instead (otherwise follow optional section) gitlab-runner register --url http://192.168.68.126 --token glrt--LuoR7v6HNubypzQusrz # You can run this in the background gitlab-runner run\rCopy\rRun GitLab Pipeline Here\u0026rsquo;s a sample .gitlab-ci.yml file:\nvariables: IMAGE_GRADLE_JAVA: gradle:jdk17 stages: - build gradle-build: image: $IMAGE_GRADLE_JAVA stage: build script: - echo \u0026#34;Building\u0026#34; - sh $CI_PROJECT_DIR/gradlew clean build\rCopy\rTo trigger the build ‚Ä¶","date":"2023-12-24","permalink":"https://seehiong.github.io/posts/2023/12/gitlab-setup-installation-migration-and-ci/cd-simplified/","summary":"In this guide, I\u0026rsquo;ll walk you through the process of installing GitLab\r, a comprehensive suite of tools for version control, continuous ‚Ä¶","tags":["GitLab","CI/CD","Docker","HomeLab","Registry","Pipeline"],"title":"GitLab Setup: Installation, Migration, and CI/CD Simplified"},{"content":"Commencing my week-long Christmas break, I extend the concepts from my previous post\rto establish an OpenAI-compatible server in my Home Lab\r.\nTechnical Setup After fine-tuning a sample Dockerfile\r, I reinstalled my Ubuntu server, incorporating necessary adjustments. The subsequent setup commands, reflecting my Home Lab\u0026rsquo;s new IP address (192.168.68.115), include:\nsudo apt update \u0026amp; sudo apt upgrade -y # Install docker sudo apt install docker.io sudo usermod -aG docker pi # Install Anaconda curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh chmod +x Anaconda3-2023.09-0-Linux-x86_64.sh ./Anaconda3-2023.09-0-Linux-x86_64.sh # Init conda source /home/pi/anaconda3/bin/activate conda init conda create -n docker-llama python conda activate docker-llama Copy\rThe corresponding Dockerfile features:\nFROM python:3-slim-bullseye # We need to set the host to 0.0.0.0 to allow outside access ENV HOST 0.0.0.0 COPY ./phi-2.Q4_K_M.gguf . # Install the package RUN apt update \u0026amp;\u0026amp; apt install -y libopenblas-dev ninja-build build-essential pkg-config RUN pip install --upgrade pip RUN python -m pip install --no-cache-dir --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context RUN CMAKE_ARGS=\u0026#34;-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\u0026#34; pip install --no-cache-dir --force-reinstall llama_cpp_python==0.2.24 --verbose # Run the server CMD [\u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;llama_cpp.server\u0026#34;, \u0026#34;--model\u0026#34;, \u0026#34;/phi-2.Q4_K_M.gguf\u0026#34;]\rCopy\rFor the Micorsoft\u0026rsquo;s Phi2\rmodel, I downloaded the GGUF format via here\r:\nwget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\rCopy\rDocker Image Build and Run The image, packaged with Microsoft\u0026rsquo;s Phi2 model, is built using:\ndocker build . -t llama-microsoft-phi2:v0.2.24\rCopy\rTo run the image:\ndocker run -p 8000:8000 --rm -it llama-microsoft-phi2:v0.2.24\rCopy\rNote\rTo resolve the \u0026ldquo;failed to mlock\u0026rdquo; warning, add \u0026ndash;cap-add IPC_LOCK like so:\ndocker run --cap-add IPC_LOCK -p 8000:8000 --rm -it llama-microsoft-phi2:v0.2.24\rCopy\rDocker Image Push and Deployment Establishing a local registry on the new Ubuntu server is the first step:\nsudo vi /etc/docker/daemon.json\rCopy\rInsert the following content into daemon.json:\n{ \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;192.168.68.115:30500\u0026#34; ] }\rCopy\rConfigure Docker options:\nsudo vi /etc/default/docker\rCopy\rAdd the line:\nDOCKER_OPTS=\u0026#34;--config-file=/etc/docker/daemon.json\u0026#34;\rCopy\rRestart Docker:\nsudo systemctl restart docker\rCopy\rTag and push the image to the home lab:\ndocker image ls docker tag llama-microsoft-phi2:v0.2.24 192.168.68.115:30500/llama-microsoft-phi2:v0.2.24 docker push 192.168.68.115:30500/llama-microsoft-phi2:v0.2.24\rCopy\rFor larger image layers, bypass retry errors using:\nsudo mkdir -p /etc/systemd/system/docker.service.d sudo vi /etc/systemd/system/docker.service.d/http-proxy.conf\rCopy\rAdd to http-proxy.conf:\n[Service] Environment=\u0026#34;NO_PROXY=localhost,127.0.0.1,192.168.68.115\u0026#34;\rCopy\rReload Docker:\nsudo systemctl daemon-reload sudo systemctl restart docker\rCopy\rJoining Home Lab as a K3S Node Join as a node:\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.68.115:6443 K3S_TOKEN=K10e848701b18977c63d7abfce920cf66c0f19bdd18a40862b2e7a14b89c4eb2742::server:ac92f2b7ccebbb46bf241bdaea3c99bf sh -\rCopy\rConfigure the insecure registry for K3S agents:\nsudo vi /etc/systemd/system/k3s-agent.service # Restart k3s agent after the change sudo systemctl daemon-reload sudo systemctl restart k3s-agent\rCopy\rChange ExecStart of k3s-agent.service to:\nExecStart=/usr/local/bin/k3s \\ agent \\ --docker --insecure-registry=http://192.168.68.115:30500\rCopy\rDeployment to K3S in Home Lab Create a llama-phi2.yaml for deployment (IPC_LOCK setting for resolving \u0026ldquo;failed to mlock\u0026rdquo; warning):\napiVersion: apps/v1 kind: Deployment metadata: name: llama-phi2 namespace: llm spec: replicas: 1 selector: matchLabels: app: llama-phi2 template: metadata: labels: app: llama-phi2 name: llama-phi2 spec: containers: - name: llama-phi2 image: 192.168.68.115:30500/llama-microsoft-phi2:v0.2.24 imagePullPolicy: IfNotPresent resources: requests: memory: \u0026#34;6Gi\u0026#34; limits: memory: \u0026#34;6Gi\u0026#34; ports: - containerPort: 8000 securityContext: capabilities: add: - IPC_LOCK imagePullSecrets: - name: regcred\rCopy\rDeploy using:\nkca llama-phi2.yaml\rCopy\rFor service exposure, create llama-phi2-svc.yaml:\napiVersion: v1 kind: Service metadata: name: llama-phi2-svc namespace: llm spec: selector: app: llama-phi2 type: NodePort ports: - name: http protocol: TCP port: 80 targetPort: 8000 nodePort: 30000\rCopy\rApply to the K3S cluster:\nkca llama-phi2-svc.yaml\rCopy\rAccess the llama-phi2 server through nodePort 3000:\nTip\rIf you want a straight forward label-based match, you may use node selector to use a specific host to run the pod:\nspec: nodeSelector: hostname: alien # Alternatively you may consider the default ‚Ä¶","date":"2023-12-22","permalink":"https://seehiong.github.io/posts/2023/12/deploying-openai-compatible-llama-cpp-server-with-k3s/","summary":"In this post, I expand my Home Lab by adding a perpetual LLAMA model for on-demand inferencing. The steps involve crafting a Dockerfile, packaging ‚Ä¶","tags":["Phi2","Uvicorn","Docker","LLaMA","HomeLab","K3s","OpenAI"],"title":"Deploying OpenAI-Compatible LLAMA CPP Server with K3S"},{"content":"Discover the capabilities of Agent AutoBuild\rin my recent exploration with Autogen using app.py.\nSetup Configuration In my model setup configuration, defined in OAI_CONFIG_LIST, I\u0026rsquo;m leveraging the latest version of Autogen (pyautogen==0.2.2) with the following specifications:\n[ { \u0026#34;model\u0026#34;: \u0026#34;gpt-4\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;NULL\u0026#34;, \u0026#34;base_url\u0026#34;: \u0026#34;http://192.168.68.114:1234/v1\u0026#34; } ]\rCopy\rEnvisioning a groundbreaking initiative, I\u0026rsquo;ve embedded my vision within the instruction:\ncustom_task = \u0026#34;\u0026#34;\u0026#34; I\u0026#39;m enthusiastic about launching an innovative project ‚Äì a software academy tailored for children aged 7 to 16 with no prior coding experience. The focus is on AI and introductory Python programming to impart essential technical skills crucial for their future. To bring this vision to life, I\u0026#39;d like the language model\u0026#39;s assistance in detailing the course agenda, developing a comprehensive curriculum, and establishing a step-by-step approach towards our goal. Together, we aim to create a compelling plan that not only educates but also inspires the next generation of tech enthusiasts. Let\u0026#39;s embark on this exciting journey by shaping the foundation of our software academy.\u0026#34; \u0026#34;\u0026#34;\u0026#34;\rCopy\rApp.py Overview The impressive AutoBuild functionality streamlines the automated creation of multi-agent systems designed for intricate tasks. Leveraging the Dolphin 2.2.1 Mistral 7B model\rthrough LM Studio\r, here is the entirety of my app.py:\nimport autogen from autogen.agentchat.contrib.agent_builder import AgentBuilder config_path = \u0026#39;OAI_CONFIG_LIST\u0026#39; default_llm_config = { \u0026#39;temperature\u0026#39;: 0, \u0026#39;timeout\u0026#39;: 600 } builder = AgentBuilder(config_path=config_path, builder_model=\u0026#39;gpt-4\u0026#39;, agent_model=\u0026#39;gpt-4\u0026#39;) custom_task = \u0026#34;\u0026#34;\u0026#34; I\u0026#39;m enthusiastic about launching an innovative project ‚Äì a software academy tailored for children aged 7 to 16 with no prior coding experience. The focus is on AI and introductory Python programming to impart essential technical skills crucial for their future. To bring this vision to life, I\u0026#39;d like the language model\u0026#39;s assistance in detailing the course agenda, developing a comprehensive curriculum, and establishing a step-by-step approach towards our goal. Together, we aim to create a compelling plan that not only educates but also inspires the next generation of tech enthusiasts. Let\u0026#39;s embark on this exciting journey by shaping the foundation of our software academy. \u0026#34;\u0026#34;\u0026#34; agent_list, agent_configs = builder.build(custom_task + \u0026#39;\\nOnly identify the agent name and separate them with comma.\u0026#39;, default_llm_config) def start_task(execution_task: str, agent_list: list, llm_config: dict): config_list = autogen.config_list_from_json(config_path, filter_dict={\u0026#34;model\u0026#34;: [\u0026#34;gpt-4\u0026#34;]}) group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12) manager = autogen.GroupChatManager( groupchat=group_chat, llm_config={\u0026#34;config_list\u0026#34;: config_list, **llm_config} ) agent_list[0].initiate_chat(manager, message=execution_task) start_task( execution_task=custom_task, agent_list=agent_list, llm_config=default_llm_config )\rCopy\rOutput Showcase Autogen seamlessly organizes the different agents as required:\nExplore snippets from the discussion rounds:\n1st Set 2nd Set 3rd Set Final Set (specified that the maximum is 12) Agent AutoBuild Approach Referring back to my previous post\r, I seamlessly incorporated the AutoBuild agent into addressing my previous question. Continuing with this approach, I\u0026rsquo;ve refined my task to seek insights into the intricacies of debating the statement, All art requires courage. Do you agree?. These are the modified python code:\nautogen.AssistantAgent.DEFAULT_SYSTEM_MESSAGE = \u0026#34;\u0026#34;\u0026#34;You are a helpful AI assistant. Define traits for the assigned position to complete the task. You will assume this position and do not derail from the assigned task. Reply in less than 100 words. Reply \u0026#34;TERMINATE\u0026#34; in the end when everything is done. \u0026#34;\u0026#34;\u0026#34; custom_task = \u0026#34;\u0026#34;\u0026#34; I am preparing for my General Paper examination article on the topic \u0026#39;Does all art require courage?\u0026#39;. Craft an introduction, provide a structure for the subsequent content with examples to substantiate the points and end with a conclusion. The article must be less than 1000 words, no infographics or programming is needed. \u0026#34;\u0026#34;\u0026#34; agent_list, agent_configs = builder.build( custom_task + \u0026#39;\\n[INST]Provide the agent name and nothing else, separete them with comma.[/INST]\u0026#39;, default_llm_config, code_execution_config=False) def start_task(execution_task: str, agent_list: list, llm_config: dict): config_list = autogen.config_list_from_json(config_path, filter_dict={\u0026#34;model\u0026#34;: [\u0026#34;gpt-4\u0026#34;]}) group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=4, ‚Ä¶","date":"2023-12-17","permalink":"https://seehiong.github.io/posts/2023/12/unveiling-agent-autobuild-in-autogen/","summary":"In this blog, I explored Autogen\u0026rsquo;s Agent AutoBuild and experimented with the Mixtral 8x7B model. I configured Autogen, envisioning a software ‚Ä¶","tags":["Autogen","LM Studio","LLM","Java","AI","Dolphin","Mixtral","Multi-Agent"],"title":"Unveiling Agent AutoBuild in Autogen"},{"content":"In the pursuit of enhancing Autogen\u0026rsquo;s capabilities, I drew inspiration from 0xlws\u0026rsquo; fork\rsupporting JavaScript. This led me to embark on a journey to modify Autogen, enabling robust support for Java code execution.\nSetting up Begin by ensuring that Java is installed on your Windows Subsystem for Linux (WSL) using the following command:\nsudo apt install openjdk-17-jdk-headless\rCopy\rModifying Autogen Clone the latest changes from Autogen\u0026rsquo;s repository:\ngit clone https://github.com/microsoft/autogen.git\rCopy\rKey modifications were made to code_utils.py and conversable_agent.py to seamlessly integrate Java support. Here\u0026rsquo;s a snippet of the changes:\n# conversable_agent.py # @@ -1088,6 +1088,19 @@ class ConversableAgent(Agent): elif lang in [\u0026#34;java\u0026#34;]: if code.startswith(\u0026#34;// filename: \u0026#34;): filename = code[12 : code.find(\u0026#34;\\n\u0026#34;)].strip() else: filename = None exitcode, logs, image = self.run_code( code, lang=\u0026#34;java\u0026#34;, filename=filename, **self._code_execution_config, ) elif lang == \u0026#39;unknown\u0026#39;: exitcode, logs, image = (0, \u0026#34;Language is unknown.\u0026#34;, None) # code_utils.py # @@ -83,7 +83,8 @@ def infer_lang(code): def infer_lang(code): \u0026#34;\u0026#34;\u0026#34;infer the language for the code. TODO: make it robust. \u0026#34;\u0026#34;\u0026#34; valid_prefixes = [\u0026#34;python \u0026#34;, \u0026#34;pip\u0026#34;, \u0026#34;python3 \u0026#34;, \u0026#34;java \u0026#34;, \u0026#34;javac \u0026#34;] if any(code.startswith(prefix) for prefix in valid_prefixes): return \u0026#34;sh\u0026#34; # @@ -215,7 +216,7 @@ def timeout_handler(signum, frame): def _cmd(lang): if lang.startswith(\u0026#34;python\u0026#34;) or lang in [\u0026#34;bash\u0026#34;, \u0026#34;sh\u0026#34;, \u0026#34;powershell\u0026#34;, \u0026#34;java\u0026#34;]: return lang if lang in [\u0026#34;shell\u0026#34;]: return \u0026#34;sh\u0026#34; if lang in [\u0026#34;ps1\u0026#34;]: return \u0026#34;powershell\u0026#34; raise NotImplementedError(f\u0026#34;{lang} not recognized in code execution\u0026#34;)\rCopy\rFor detailed changes, refer to the full git diff output.\nRecompile the modified changes:\npip install .\rCopy\rGenerating Java code To incorporate Java code execution, additional instructions have been added to the SYSTEM_MESSAGE:\nFor Java-specific code:\r- If you want the user to save the code in a file before executing it, put // filename: \u0026lt;filename\u0026gt; inside the code block as the first line.\r- Ensure that the filename matches the class name and has the same capitalization.\rCopy\rHere\u0026rsquo;s the entire app.py showcasing the integration:\nimport autogen config_list = [ { \u0026#39;api_key\u0026#39;: \u0026#39;NULL\u0026#39;, \u0026#39;base_url\u0026#39;: \u0026#39;http://192.168.68.114:1234/v1\u0026#39; }, ] llm_config = { \u0026#34;cache_seed\u0026#34;: 42, \u0026#34;temperature\u0026#34;: 0, \u0026#34;config_list\u0026#34;: config_list } SYSTEM_MESSAGE = \u0026#34;\u0026#34;\u0026#34;You are a helpful AI assistant. Solve tasks using your coding and language skills. For Java-specific code: - If you want the user to save the code in a file before executing it, put // filename: \u0026lt;filename\u0026gt; inside the code block as the first line. - Ensure that the filename matches the class name and has the same capitalization. In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute. 1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself. 2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly. Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill. When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can\u0026#39;t modify your code. So do not suggest incomplete code which requires users to modify. Don\u0026#39;t use a code block if it\u0026#39;s not intended to be executed by the user. If you want the user to save the code in a file before executing it, put # filename: \u0026lt;filename\u0026gt; inside the code block as the first line. Don\u0026#39;t include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use \u0026#39;print\u0026#39; function for the output when relevant. Check the execution result returned by the user. If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can\u0026#39;t be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try. When you find an answer, verify the answer carefully. Include verifiable ‚Ä¶","date":"2023-12-10","permalink":"https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/","summary":"In this post, I explored enhancing Autogen\u0026rsquo;s capabilities by enabling seamless Java code execution. Drawing inspiration from 0xlws\u0026rsquo; fork ‚Ä¶","tags":["Autogen","LM Studio","LLM","Java","AI","WSL","Docker","Dolphin","Mistral"],"title":"Empowering Autogen: Enabling Seamless Java Code Execution"},{"content":"In this post, I\u0026rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post Exploration with Autogen\rand following the example of Automated Multi Agent Chat\r, we\u0026rsquo;ll delve into the steps to create a dynamic debate environment.\nAgent Setup I\u0026rsquo;ll be setting up two agents: for_motion and against_motion. Each agent will engage in a debate on a given topic, providing examples and substantiating their points. A facilitator will oversee the debate rounds, ensuring that each response exceeds 300 words.\n# Agent Setup for_motion = autogen.AssistantAgent( name=\u0026#34;for_motion\u0026#34;, llm_config=llm_config, system_message=\u0026#34;\u0026#34;\u0026#34;You are debating from within yourself, for the motion of the topic being raised. For each round, provide examples to substantiate your points for the motion. DO NOT include any conclusion. DO NOT thank each other. Ensure that your reply is consistently more than 300 words.\u0026#34;\u0026#34;\u0026#34; ) against_motion = autogen.AssistantAgent( name=\u0026#34;against_motion\u0026#34;, llm_config=llm_config, system_message=\u0026#34;\u0026#34;\u0026#34;You are debating from within yourself, against the motion of the topic being raised. For each round, provide examples to substantiate your points against the motion. DO NOT include any conclusion. DO NOT thank each other. Ensure that your reply is consistently more than 300 words.\u0026#34;\u0026#34;\u0026#34; ) facilitator = autogen.UserProxyAgent( name=\u0026#34;facilitator\u0026#34;, system_message=\u0026#34;You are the faciliator of the debate. DO NOT participate in the debate. Remind everyone to reply in more than 300 words. Ensure that your reply is NOT more than 50 words.\u0026#34;, human_input_mode=\u0026#34;NEVER\u0026#34;, code_execution_config=False, llm_config=llm_config, )\rCopy\rScript Execution The debate unfolds with the facilitator guiding multiple rounds between the two agents. As they engage, their messages are collected, consolidated, and passed on to the next assistant. This subsequent assistant plays a crucial role in organizing the PROVIDED_CONTEXT and crafting a cohesive article from the accumulated debate.\nFor reference, ensure you are using the latest version of autogen (requirements.txt: pyautogen==0.2.1).\nFull Script in app.py Here\u0026rsquo;s the entire script placed in app.py for reference:\nimport autogen from typing import List, Dict config_list = [ { \u0026#39;api_key\u0026#39;: \u0026#39;NULL\u0026#39;, \u0026#39;base_url\u0026#39;: \u0026#39;http://192.168.68.114:1234/v1\u0026#39; }, ] llm_config = { \u0026#34;cache_seed\u0026#34;: 42, \u0026#34;config_list\u0026#34;: config_list } # Agent Setup for_motion = autogen.AssistantAgent( name=\u0026#34;for_motion\u0026#34;, llm_config=llm_config, system_message=\u0026#34;\u0026#34;\u0026#34;You are debating from within yourself, for the motion of the topic being raised. For each round, provide examples to substantiate your points for the motion. DO NOT include any conclusion. DO NOT thank each other. Ensure that your reply is consistently more than 300 words.\u0026#34;\u0026#34;\u0026#34; ) against_motion = autogen.AssistantAgent( name=\u0026#34;against_motion\u0026#34;, llm_config=llm_config, system_message=\u0026#34;\u0026#34;\u0026#34;You are debating from within yourself, against the motion of the topic being raised. For each round, provide examples to substantiate your points against the motion. DO NOT include any conclusion. DO NOT thank each other. Ensure that your reply is consistently more than 300 words.\u0026#34;\u0026#34;\u0026#34; ) facilitator = autogen.UserProxyAgent( name=\u0026#34;facilitator\u0026#34;, system_message=\u0026#34;You are the faciliator of the debate. DO NOT participate in the debate. Remind everyone to reply in more than 300 words. Ensure that your reply is NOT more than 50 words.\u0026#34;, human_input_mode=\u0026#34;NEVER\u0026#34;, code_execution_config=False, llm_config=llm_config, ) messages: List[Dict] = [] groupchat = autogen.GroupChat( agents=[facilitator, for_motion, against_motion], messages=messages, max_round=9, allow_repeat_speaker=False, speaker_selection_method=\u0026#34;round_robin\u0026#34;, ) manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config) facilitator.initiate_chat(manager, message=\u0026#34;All art requires courage. Do you agree?\u0026#34;) system_message = \u0026#34;\u0026#34;\u0026#34;\\nYou are a professional writer tasked with synthesizing various perspectives from the provided contents. DO NOT introduce any points not present in the provided context. Based on the ```PROVIDED_CONTEXT```, create a continuous and seamless flow of ideas throughout the article. Your article must include 1x INTRODUCTION and 1x CONCLUSION. Elaborate extensively on each point you make, providing in-depth analysis and examples to support your arguments. Explore multiple additional perspectives or counterarguments to enrich the discussion. Ensure that your response is comprehensive and thoroughly covers the topic. The article must be at least 1000 words; aim for a clear and concise style within this range.\u0026#34;\u0026#34;\u0026#34; system_message += \u0026#34;\\n\\nStart by stating the article\u0026#39;s title as: ```\u0026#34; + ‚Ä¶","date":"2023-12-08","permalink":"https://seehiong.github.io/posts/2023/12/multi-agent-conservation-with-autogen/","summary":"In this post, I\u0026rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post ‚Ä¶","tags":["Autogen","LM Studio","LLM","AI","Summarisation","Multi-Agent"],"title":"Multi-agent Conservation with Autogen"},{"content":"AutoGen, an innovative framework available on GitHub\r, empowers the development of LLM (Large Language Model) applications. These applications utilize multiple agents that engage in conversation to collaboratively solve tasks.\nIn conjunction with AutoGen, LM Studio\rprovides a platform to discover, download, and run local LLMs. In this blog post, we\u0026rsquo;ll delve into the integration of AutoGen with LM Studio, showcasing a step-by-step guide on setting up a local LLM application served through LM Studio.\nInstalling LM Studio Begin by installing LM Studio and downloading the Dolphin 2.2.1 Mistral 7B model. Customize your setup by configuring the context length to 8192, enabling GPU acceleration with GPU layers set to 9, and setting CPU threads to 8.\nOnce the download is complete, start the server. The default port will be http://localhost:1234/v1.\nIf you plan to execute Python code within the Windows Subsystem for Linux (WSL), access port 1234 through the local IP address. Test the connection with the provided curl command.\ncurl http://192.168.68.118:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Introduce yourself.\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39;\rCopy\rInstalling Anaconda Proceed to install Anaconda\rusing the following commands:\n# Install necessary packages for GUI with Linux apt-get install libgl1-mesa-glx libegl1-mesa libxrandr2 libxrandr2 libxss1 libxcursor1 libxcomposite1 libasound2 libxi6 libxtst6 # Download the installer curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh # Install Anaconda ./Anaconda3-2023.09-0-Linux-x86_64.sh # Initialise conda source /home/pi/anaconda3/bin/activate conda init\rCopy\rAutoGen at work In Visual Studio Code, open a terminal within WSL, and create a virtual environment:\nconda create -n pyautogen python=3.10\rCopy\rDefine your requirements.txt file with essential libraries:\npyautogen==0.2.0\rdocker==6.1.3\rbeautifulsoup4\ryFinance\rpandas\rmatplotlib\rsympy\rCopy\rActivate the environment with conda activate pyautogen and install the requirements:\npip install -r requirements.txt\rCopy\rFollowing the guidelines outlined in the Getting Started\rdocumentation, create your app.py script:\nimport autogen config_list = [ { \u0026#39;api_key\u0026#39;: \u0026#39;NULL\u0026#39;, \u0026#39;base_url\u0026#39;: \u0026#39;http://192.168.68.118:1234/v1\u0026#39; }, ] llm_config = { \u0026#34;cache_seed\u0026#34;: 42, \u0026#34;temperature\u0026#34;: 0, \u0026#34;config_list\u0026#34;: config_list } assistant = autogen.AssistantAgent( name=\u0026#34;assistant\u0026#34;, llm_config=llm_config, is_termination_msg=lambda x: x.get(\u0026#34;content\u0026#34;,\u0026#34;\u0026#34;).rstrip().endswith(\u0026#34;TERMINATE\u0026#34;), ) user_proxy = autogen.UserProxyAgent( name=\u0026#34;user_proxy\u0026#34;, human_input_mode=\u0026#34;NEVER\u0026#34;, max_consecutive_auto_reply=10, llm_config=llm_config, code_execution_config={ \u0026#34;work_dir\u0026#34;: \u0026#34;coding\u0026#34;, \u0026#34;use_docker\u0026#34;: False }, system_message=\u0026#34;\u0026#34;\u0026#34;\u0026#34;Reply TERMINATE if the task has been solved at full satisfaction or the code executed without issue. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\u0026#34;\u0026#34;\u0026#34; ) user_proxy.initiate_chat(assistant, message=\u0026#34;Plot a chart of NVDA and TESLA stock price change YTD.\u0026#34;)\rCopy\rExecute the script with python app.py to generate the stock price comparison chart:\nDespite experimenting with various local models, it\u0026rsquo;s worth noting that certain models, including the Dolphin-Mistral model, may not terminate automatically. For this instance, termination only occur after 10 consecutive auto-replies!\n","date":"2023-12-02","permalink":"https://seehiong.github.io/posts/2023/12/exploring-autogen-with-lm-studio-and-local-llm/","summary":"I explored AutoGen, an innovative framework on GitHub, enabling the development of Large Language Model (LLM) applications. Collaborating with LM ‚Ä¶","tags":["Autogen","LM Studio","Anaconda","AI","Dolphin","Mistral"],"title":"Exploring AutoGen with LM Studio and Local LLM"},{"content":"In the relentless pursuit of optimal disk space and lightning-fast inference speeds, I embarked on an exciting upgrade journey by integrating the formidable Lexar NM790 M.2 2280 PCIe SSD\r. This blog post unfolds in two parts: the first chronicles the meticulous migration of my Windows 11 to this powerhouse SSD, while the second unveils the secrets behind the enhanced inferencing speed for the Langchain4j application\r.\nPart 1: Seamless OS Migration with Clonezilla Amidst a sea of software promising seamless disk cloning, I found solace in the reliability of Clonezilla\r, a robust open-source tool for disk imaging and cloning.\nAfter securely installing the Lexar SSD into slot 2 of my PC, I tapped into the power of Clonezilla. Following the Clone your SSD or Hard Drive\rguide, I navigated through the process, opting for the -k1 option to create a proportional partition table.\nThe result? A seamlessly cloned SSD, now the master boot, offering a tangible boost in inferencing speed as all my LLM models found their new home on this high-performance SSD.\nPart 2: GPU Acceleration Unleashed Step 1: Installing CUDA for Windows My journey continued with the installation of CUDA for Microsoft Windows\r. Armed with the NVIDIA CUDA Toolkit, I confirmed the successful installation using two simple PowerShell commands: nvcc \u0026ndash;version and nvidia-smi.\nStep 2: NVIDIA Container Toolkit Magic The next leg of this GPU odyssey involved the installation of the NVIDIA Container Toolkit\r. Delving into the realm of Windows Subsystem for Linux (WSL), I executed a series of commands to seamlessly integrate this toolkit into my environment.\n# Setting up the APT repository and configuring it curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\ \u0026amp;\u0026amp; \\ sudo apt-get update # install the NVIDIA container toolkit package sudo apt-get install -y nvidia-container-toolkit\rCopy\rConfiguring Docker for this newfound power involved a few more commands:\n# Configuring containter runtime sudo nvidia-ctk runtime configure --runtime=docker # Restarting docker daemon sudo systemctl restart docker # If docker.service is not found, you may reinstall and reconfigure runtime sudo apt-get --reinstall install docker.io\rCopy\rFrom the Docker Desktop settings, I ventured into extra configurations, applied the changes, and restarted:\n\u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } }\rCopy\rThe grand test unfolded with a sample workload command:\nsudo docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi\rCopy\rInfo\rBe vigilant! Without the Docker Desktop setting, you might encounter the ominous error depicted below:\nThe Grand Finale Bringing it all together, the magic command to launch the LocalAI image in a GPU Docker container was unveiled:\ndocker run -p 8080:8080 -v c:/local-ai-models:/models -ti --rm quay.io/go-skynet/local-ai:v1.40.0-cublas-cuda12 --models-path /models --context-size 2000 --threads 8 --debug=true\rCopy\rAnd behold, as we queried using the WizardLM model, the GPU acceleration became apparent, turning our tech realm into a high-speed universe.\nComparing it to the past, the results spoke for themselves‚Äîour inference speed had ascended to new heights!\nParting Thoughts In this journey, we\u0026rsquo;ve showcased the synergy of SSD storage and GPU acceleration, breathing life into the WizardLM model and witnessing a remarkable surge in inference speed. A testament to the relentless pursuit of tech optimization, this transformation opens new doors for Langchain4j enthusiasts, inviting you to explore the boundless possibilities of elevated performance. Elevate your tech game; dive into the future with Langchain4j and redefine what\u0026rsquo;s possible!\n","date":"2023-11-30","permalink":"https://seehiong.github.io/posts/2023/11/boosting-inference-speed-ssd-and-gpu-acceleration/","summary":"Embarking on an exhilarating upgrade journey, I chronicle the seamless migration to the powerful Lexar NM790 SSD and unveil the secrets behind ‚Ä¶","tags":["LocalAI","Clonezilla","CUDA","WizardLM","AI","GPU","NVIDIA"],"title":"Boosting Inference Speed: SSD and GPU Acceleration"},{"content":"Expanding upon the concepts introduced in the previous post\rand drawing inspiration from RAG over code\r, this article dives into the integration of a Retrieval-Augmented Generation (RAG) service. The goal is to empower users to query their Java codebase effectively.\nGetting Started To embark on this journey, I\u0026rsquo;ve opted for Java Parser\r, a powerful tool for traversing Java source code. Let\u0026rsquo;s begin by incorporating the latest version of Java Parser into our build.gradle file:\n... dependencies { // Ohter dependencies implementation \u0026#39;com.github.javaparser:javaparser-symbol-solver-core:3.25.6\u0026#39; }\rCopy\rCreating new Services In this section, we created the essential services to enhance the capabilities of our AI application. We introduce two key services, each serving a distinct purpose in our system architecture.\nJavaParsingService The JavaParsingService is introduced to facilitate codebase analysis using the Java Parser library. This service lays the foundation for code traversal and integrates with Langchain4j components for embedding and retrieval. Below is the detailed service implementation:\npackage com.seehiong.ai.service; import static dev.langchain4j.data.document.Document.DOCUMENT_TYPE; import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO; import java.io.IOException; import java.nio.file.Paths; import java.util.ArrayList; import java.util.List; import java.util.Optional; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import com.github.javaparser.ParseResult; import com.github.javaparser.ast.CompilationUnit; import com.github.javaparser.ast.PackageDeclaration; import com.github.javaparser.ast.body.ClassOrInterfaceDeclaration; import com.github.javaparser.ast.visitor.VoidVisitor; import com.github.javaparser.ast.visitor.VoidVisitorAdapter; import com.github.javaparser.utils.SourceRoot; import dev.langchain4j.data.document.Document; import dev.langchain4j.data.document.DocumentSplitter; import dev.langchain4j.data.document.Metadata; import dev.langchain4j.data.document.splitter.DocumentSplitters; import dev.langchain4j.data.embedding.Embedding; import dev.langchain4j.data.segment.TextSegment; import dev.langchain4j.memory.chat.MessageWindowChatMemory; import dev.langchain4j.model.chat.ChatLanguageModel; import dev.langchain4j.model.embedding.EmbeddingModel; import dev.langchain4j.model.openai.OpenAiTokenizer; import dev.langchain4j.retriever.EmbeddingStoreRetriever; import dev.langchain4j.retriever.Retriever; import dev.langchain4j.service.AiServices; import dev.langchain4j.service.SystemMessage; import dev.langchain4j.store.embedding.EmbeddingStore; import dev.langchain4j.store.embedding.EmbeddingStoreIngestor; @Service public class JavaParsingService { @Autowired private EmbeddingStoreService embeddingStoreSvc; interface JavaCodeAgent { @SystemMessage({ \u0026#34;Imagine you are a highly experienced Java programmer tasked with explaining the structure and functionality of the provided Java code.\u0026#34;, \u0026#34;Your goal is to conduct a thorough analysis of the codebase, highlighting key aspects such as design patterns, architectural choices, and coding practices.\u0026#34; }) String query(String userMessage); } static class ClassNameCollector extends VoidVisitorAdapter\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; { @Override public void visit(ClassOrInterfaceDeclaration n, List\u0026lt;String\u0026gt; collector) { super.visit(n, collector); collector.add(n.getNameAsString()); } } static class JavaDocumentLoader { public Document load(String javaCode, Optional\u0026lt;PackageDeclaration\u0026gt; packageName) { Document document = Document.from(javaCode, Metadata.from(DOCUMENT_TYPE, \u0026#34;java\u0026#34;)); document.metadata().add(\u0026#34;package name\u0026#34;, packageName); return document; } } public String load(EmbeddingModel embeddingModel, String project) { JavaDocumentLoader javaLoader = new JavaDocumentLoader(); EmbeddingStore\u0026lt;TextSegment\u0026gt; embeddingStore = embeddingStoreSvc.getEmbeddingStore(); EmbeddingStoreIngestor embeddingStoreIngestor = embeddingStoreSvc.getEmbeddingStoreIngestor(embeddingModel); // Parse all source files SourceRoot sourceRoot = new SourceRoot(Paths.get(project)); List\u0026lt;ParseResult\u0026lt;CompilationUnit\u0026gt;\u0026gt; parseResults; List\u0026lt;String\u0026gt; className = new ArrayList\u0026lt;\u0026gt;(); try { parseResults = sourceRoot.tryToParse(); for (ParseResult\u0026lt;CompilationUnit\u0026gt; parseResult : parseResults) { if (parseResult.getResult().isPresent()) { CompilationUnit unit = parseResult.getResult().get(); Document document = javaLoader.load(unit.toString(), unit.getPackageDeclaration()); DocumentSplitter splitter = DocumentSplitters.recursive(100, 0, new OpenAiTokenizer(GPT_3_5_TURBO)); List\u0026lt;TextSegment\u0026gt; segments = splitter.split(document); List\u0026lt;Embedding\u0026gt; embeddings = embeddingModel.embedAll(segments).content(); embeddingStore.addAll(embeddings, segments); embeddingStoreIngestor.ingest(document); VoidVisitor\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; ‚Ä¶","date":"2023-11-11","permalink":"https://seehiong.github.io/posts/2023/11/rag-over-java-code-with-langchain4j/","summary":"In my latest post, I delve into seamlessly integrating Retrieval-Augmented Generation (RAG) with Java code using Langchain4j. Drawing inspiration from ‚Ä¶","tags":["langchain4j","LocalAI","Spring Boot","Java","Chroma","GPT4All","OpenAI","WizardLM","AI","Backend","BE","LLM","RAG"],"title":"RAG over Java code with Langchain4j"},{"content":"In this blog post, I\u0026rsquo;ll walk you through my journey of harnessing the capabilities of langchain4j\rto craft a powerful AI application using Java, specifically with a local language model. Unlike my previous exploration with Python, this post focuses on the Java implementation with Langchain4j.\nGetting Started To kick things off, I\u0026rsquo;ve chosen STS4\ras my Integrated Development Environment (IDE) and opted for Java 17\ras my programming language. Leveraging Postman\ras my API platform and Spring Boot\ras the framework of choice, let\u0026rsquo;s delve into the process.\nSetting up a Spring Boot Application To initiate the project, I began by creating a Spring Starter Project and selecting the Spring Web option. Here\u0026rsquo;s a snippet of the setup:\nSpring Boot Application Here\u0026rsquo;s my sprint boot application:\npackage com.seehiong.ai; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class AiApplication { public static void main(String[] args) { SpringApplication.run(AiApplication.class, args); } }\rCopy\rGradle Build Configuration My build.gradle file outlines the dependencies, including Langchain4j components:\nplugins { id \u0026#39;java\u0026#39; id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;3.1.5\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.1.3\u0026#39; } group = \u0026#39;com.seehiong\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; java { sourceCompatibility = \u0026#39;17\u0026#39; } repositories { mavenCentral() } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; implementation (\u0026#39;dev.langchain4j:langchain4j:0.23.0\u0026#39;) { exclude group: \u0026#34;commons-logging\u0026#34;, module: \u0026#34;commons-logging\u0026#34; } implementation \u0026#39;dev.langchain4j:langchain4j-core:0.23.0\u0026#39; implementation \u0026#39;dev.langchain4j:langchain4j-chroma:0.23.0\u0026#39; implementation \u0026#39;dev.langchain4j:langchain4j-open-ai:0.23.0\u0026#39; implementation \u0026#39;dev.langchain4j:langchain4j-local-ai:0.23.0\u0026#39; implementation \u0026#39;dev.langchain4j:langchain4j-embeddings-all-minilm-l6-v2:0.23.0\u0026#39;\timplementation \u0026#39;org.mapdb:mapdb:3.0.10\u0026#39; } tasks.named(\u0026#39;test\u0026#39;) { useJUnitPlatform() }\rCopy\rApplication Configuration As a standard practice, I\u0026rsquo;ve created an application.properties file in the src/main/resources directory to specify server configurations:\nserver.port: 8888\rCopy\rSetting Up the Controller and Service Let\u0026rsquo;s continue our journey by establishing the controller and service components of our Java application, seamlessly integrating the power of Langchain4j.\nController Setup Begin by setting up your controller. Below is the skeleton of an empty controller ready to be infused with the capabilities of Langchain4j.\npackage com.seehiong.ai.controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController @RequestMapping(value = \u0026#34;/ai\u0026#34;) public class AiController { // We will add the services along the way }\rCopy\rService Setup In the service layer, let\u0026rsquo;s explore the heart of our application. As highlight in the official langchain4j, we can now try out OpenAI\u0026rsquo;s gpt-3.5-turbo and text-embedding-ada-002 models with LangChain4j for free by simply using the API key \u0026ldquo;demo\u0026rdquo;. Here\u0026rsquo;s the sample model service for providing the demo model.\npackage com.seehiong.ai.service; import org.springframework.stereotype.Service; import dev.langchain4j.model.chat.ChatLanguageModel; import dev.langchain4j.model.openai.OpenAiChatModel; @Service public class ModelService { private ChatLanguageModel demoModel; public ChatLanguageModel getDemoModel() { if (demoModel == null) { demoModel = OpenAiChatModel.withApiKey(\u0026#34;demo\u0026#34;); } return demoModel; } // We will be adding more models here in the subsequent sections }\rCopy\r1. Implementing Chat Functionality Building on the previous implementation, we\u0026rsquo;ve now introduced a dedicated ChatService to handle the generation of chat responses. Here is the updated code:\nChatService implementation: package com.seehiong.ai.service; import org.springframework.stereotype.Service; import dev.langchain4j.model.chat.ChatLanguageModel; @Service public class ChatService { public String generate(ChatLanguageModel model, String text) { return model.generate(text); } }\rCopy\rUpdated AiController: public class AiController { @Autowired private ModelService modelSvc; @Autowired private ChatService chatSvc; @GetMapping(\u0026#34;/chat\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; chat(@RequestParam(\u0026#34;text\u0026#34;) String text) { String response = chatSvc.generate(modelSvc.getDemoModel(), text); return new ResponseEntity\u0026lt;\u0026gt;(response, HttpStatus.OK); } }\rCopy\rThe ChatService is responsible for the generation of chat responses using the provided model. We\u0026rsquo;ve introduced the /chat endpoint that accepts ‚Ä¶","date":"2023-11-07","permalink":"https://seehiong.github.io/posts/2023/11/building-an-ai-application-with-langchain4j/","summary":"I embarked on a journey to harness the capabilities of Langchain4j, crafting a powerful AI application in Java using the local language model. ‚Ä¶","tags":["langchain4j","LocalAI","Spring Boot","Java","Backend","BE","Chroma","GPT4All","AI","LLM"],"title":"Building an AI Application with Langchain4j"},{"content":"Machine Learning Compilation for LLM, or MLC LLM\r, is a cutting-edge universal deployment solution for large language models. In this blog post, we\u0026rsquo;ll guide you through the setup process and show you how to harness the immense potential of MLC LLM.\nSetting Up Your Environment To get started with MLC LLM, you need to set up your environment properly. Follow these steps:\n1. Install TVM TVM\ris a critical component for MLC LLM. You can install it locally using pip:\npip install apache-tvm\rCopy\rCheck your TVM build options with the following command:\npython3 -c \u0026#34;import tvm; print(\u0026#39;\\n\u0026#39;.join(f\u0026#39;{k}: {v}\u0026#39; for k, v in tvm.support.libinfo().items()))\u0026#34;\rCopy\r2. Install Conda Conda is a versatile package manager that facilitates dependency management. For a smooth MLC LLM experience, you can install conda\ronto Windows Subsystem for Linux (WSL):\nwget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh sh ./Miniconda3-py39_4.12.0-Linux-x86_64.sh\rCopy\rAfter installation, make sure to update Conda, install conda-libmamba-solver, and set it as the default solver:\n# update conda conda update --yes -n base -c defaults conda # install `conda-libmamba-solver` conda install --yes -n base conda-libmamba-solver # set it as the default solver conda config --set solver libmamba\rCopy\rValidate your Conda installation with:\nconda info | grep platform\rCopy\r3. Install Vulkan SDK For optimal performance, you\u0026rsquo;ll need to install the Vulkan SDK\r:\nVulkanSDK-1.3.261.1-Installer.exe\rCopy\rTo validate the installation, run the following commands:\nsudo apt-get install vulkan-tools # Check GPU information vulkaninfo\rCopy\rExploring MLC Chat 1. Create a New Conda Environment Now that your environment is set up, let\u0026rsquo;s explore MLC Chat. We\u0026rsquo;ll run the CLI version of MLC LLM\r:\nCreate new conda environment:\nconda create -n mlc-chat-venv -c mlc-ai -c conda-forge mlc-chat-nightly conda activate mlc-chat-venv conda install git git-lfs git lfs install mkdir -p dist/prebuilt git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib cd dist/prebuilt git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\rCopy\r2. Run MLC Chat You\u0026rsquo;re now ready to run MLC Chat:\ncd ../.. mlc_chat_cli --model Llama-2-7b-chat-hf-q4f16_1\rCopy\rThis is the output of the sample question:\nWith MLC LLM and MLC Chat set up, you have the tools to explore the world of machine learning and natural language understanding. The possibilities are limitless, and we can\u0026rsquo;t wait to see what you create.\n","date":"2023-09-02","permalink":"https://seehiong.github.io/posts/2023/09/unlocking-the-power-of-machine-learning-with-mlc-llm/","summary":"I delve into the transformative realm of MLC LLM, an advanced universal deployment solution for extensive language models. My post guides you ‚Ä¶","tags":["MLC","LLM","Conda","TVM","Vulkan","AI","WSL","LLaMA2"],"title":"Unlocking the Power of Machine Learning with MLC LLM"},{"content":"\rvLLM\ris an open-source library designed for rapid LLM (Large Language Model) inference and deployment. It leverages their novel algorithm called PagedAttention, which optimizes the management of attention keys and values.\nPreparation In this blog post, I will share my experience of utilizing vLLM on a WSL (Windows Subsystem for Linux) instance running Ubuntu 22.04. Let\u0026rsquo;s start by setting up the environment:\nInstalling WSL and Configuring Ubuntu Begin by installing WSL and configuring it to use Ubuntu as the default distribution:\nwsl --install\rwsl --update\r# Sets ubuntu as the default wsl --set-default ubuntu\rCopy\rInstalling NVIDIA GPU Drivers for WSL For efficient vLLM utilization, you need the latest NVIDIA Windows GPU Driver that fully supports WSL 2. Do not install any NVIDIA GPU Linux driver within WSL 2. Visit NVIDIA CUDA on WSL\rand download the appropriate driver\r. For example:\n536.96-quadro-rtx-desktop-notebook-win10-win11-64bit-international-dch-whql.exe\rCopy\rInstalling the CUDA Toolkit Inside Ubuntu 22.04, install the CUDA Toolkit by following these steps:\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.2.1/local_installers/cuda-repo-wsl-ubuntu-12-2-local_12.2.1-1_amd64.deb sudo dpkg -i cuda-repo-wsl-ubuntu-12-2-local_12.2.1-1_amd64.deb sudo cp /var/cuda-repo-wsl-ubuntu-12-2-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda\rCopy\rInstalling Docker To manage containers effectively, install Docker using these commands:\nsudo apt install docker.io sudo usermod -aG docker pi\rCopy\rSetting Up Let\u0026rsquo;s dive into the process of setting up vLLM for efficient language model serving. This section outlines the steps to follow within the NVIDIA PyTorch Docker image.\nUsing the NVIDIA PyTorch Docker Image Before we proceed, please ensure that you have Docker Desktop for Windows installed and running on your system. This step is essential to leverage the power of containerization. Once you have Docker Desktop ready, we can move on to the next step.\nTo streamline the setup, we\u0026rsquo;ll use the recommended NVIDIA PyTorch Docker image. Be prepared for this step to take some time due to the image\u0026rsquo;s size and complexity. Run the following command to initiate the container:\ndocker run --gpus all -it --rm --shm-size=8g nvcr.io/nvidia/pytorch:23.10-py3\rCopy\rInstalling vLLM With the existing Torch package removed, proceed to install vLLM using pip within the container. Please note that this installation might also take a significant amount of time:\npip install vllm==0.2.6\rCopy\rSample output (from previous vllm version):\nLaunching the API Server Now, it\u0026rsquo;s time to deploy vLLM as an API server. Follow these steps to start the server using the default OPT-125M model:\npython -m vllm.entrypoints.api_server --host 0.0.0.0 --port 8000\rCopy\rPlease note that when running from within a Docker container, accessing the API server from the Windows host system may require additional configuration due to networking differences.\nCreating a Docker Image Snapshot Begin by identifying the current container ID from a WSL shell:\ndocker ps\rCopy\rCreate a snapshot of the current container state and give it a name, such as \u0026ldquo;vLLM\u0026rdquo;:\ndocker commit \u0026lt;container_id\u0026gt; vllm:0.2.6\rCopy\rWith the snapshot configured, expose the Docker container\u0026rsquo;s port for external access:\ndocker run -p 192.168.68.123:8000:8000 --gpus all -it --rm --shm-size=8g vllm:0.2.6\rCopy\rOnce inside the Docker container, initiate the API server using the following command:\npython -m vllm.entrypoints.api_server --host 0.0.0.0 --port 8000\rCopy\rAs per sample official example, the following query demonstrates how to interact with the model using the API:\ncurl http://192.168.68.123:8000/generate \\ -d \u0026#39;{ \u0026#34;prompt\u0026#34;: \u0026#34;San Francisco is a\u0026#34;, \u0026#34;use_beam_search\u0026#34;: true, \u0026#34;n\u0026#34;: 4, \u0026#34;temperature\u0026#34;: 0 }\u0026#39;\rCopy\rThese steps conclude the process of launching the API server and creating a Docker image snapshot. Your vLLM setup is now ready for serving language models efficiently. The blog post has covered the essential aspects of setting up and deploying vLLM, empowering you to leverage its capabilities effectively.\nTroubleshooting It seems like due to the WSL issue, I am unable to load any models like mistralai/Mistral-7B-v0.1 or mosaicml/mpt-7b on my machine using WSL:\ndocker run -p 192.168.68.123:8000:8000 --runtime nvidia --gpus all -it --rm --shm-size=64g vllm:0.2.6 python -m vllm.entrypoints.api_server --host 0.0.0.0 --port 8000 --model mistralai/Mistral-7B-v0.1 --gpu-memory-utilization=0.95\rCopy\r","date":"2023-08-20","permalink":"https://seehiong.github.io/posts/2023/08/utilizing-vllm-for-efficient-language-model-serving/","summary":"Embarking on my journey with vLLM, I explore its potential for streamlined Large Language Model (LLM) inference and deployment. The blog details my ‚Ä¶","tags":["vLLM","LLM","Uvicorn","Docker","AI","GPU","WSL","CUDA"],"title":"Utilizing vLLM for Efficient Language Model Serving"},{"content":"\rK3S\ris a lightweight and easy-to-install Kubernetes distribution, making it an ideal choice for running a Kubernetes cluster in your home lab. In this blog post, we will walk you through the step-by-step process of setting up K3s on an Ubuntu Server 22.04.2 LTS.\n1 Setting up K3S 1.1 Installing Ubuntu Server 22.04.2 LTS To start, we\u0026rsquo;ll install Ubuntu server 22.04.2 LTS\ron our laptop. You can verify the Linux distribution using the following command:\nlsb_release -a\rCopy\r1.2 Installing K3S The next step is to install K3s. K3s can be easily installed with a single command:\ncurl -sfL https://get.k3s.io | sh -\rCopy\rIf you see these info, you may install with these commands:\n# [INFO] Host iptables-save/iptables-restore tools not found\r# [INFO] Host ip6tables-save/ip6tables-restore tools not found\rsudo apt-get install iptables\rsudo apt-get install iptables-persistent\rCopy\rInstall worker nodes After the k3s setup is completed, you may obtain token with:\nmynodetoken=$(sudo cat /var/lib/rancher/k3s/server/node-token) # Sample format: K10e848701b18977c63d7abfce920cf66c0f19bdd18a40862b2e7a14b89c4eb2742::server:ac92f2b7ccebbb46bf241bdaea3c99bf echo $mynodetoken\rCopy\rFrom the client node, execute this command (with k3s server IP and mynodetoken):\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.68.132:6443 K3S_TOKEN=K10e848701b18977c63d7abfce920cf66c0f19bdd18a40862b2e7a14b89c4eb2742::server:ac92f2b7ccebbb46bf241bdaea3c99bf sh -\rCopy\r1.3 Creating Useful Aliases To simplify interactions with K3s, we\u0026rsquo;ll set up some convenient aliases. Open the bash configuration file:\nvi ~/.bashrc\rCopy\rAdd the following aliases to the file:\nalias kc=\u0026#39;sudo kubectl\u0026#39; alias kca=\u0026#39;kc apply -f\u0026#39;\rCopy\rSave and apply the changes:\nsource ~/.bashrc\rCopy\r1.4 Verifying the Installation Congratulations! Your K3s cluster is up and running. You can verify its status with kc get nodes:\n2 Setting up Portainer In addition to K3s, we will use Portainer\r, an efficient and user-friendly Docker and Kubernetes management system, to streamline the management of our home lab. Portainer is a lightweight, free, and open-source tool that will help us keep our containerized applications in check.\n2.1 Create the Portainer Folder First, let\u0026rsquo;s create a directory named portainer and navigate into it:\nmkdir portainer cd portainer\rCopy\r2.2 Download and Deploy Portainer to K3s Cluster Next, we\u0026rsquo;ll download the latest YAML file for Portainer and save it as deploy.yaml:\nwget https://raw.githubusercontent.com/portainer/k8s/master/deploy/manifests/portainer/portainer.yaml -O deploy.yaml\rCopy\rNow, let\u0026rsquo;s deploy Portainer to our K3s cluster:\nkca deploy.yaml\rCopy\rYou can check the status of the deployment using the following command:\nkc get svc portainer -n portainer\rCopy\rThe above output shows that Portainer is running on an arbitrary port, in this case, 30777 at hostname alien. Access the Portainer UI using the following URL:\nhttp://alien:30777\rCopy\rPlease access it before the instance timed out; otherwise, you will need to restart the service:\n# Get the portainer node kc get po -A # Delete the pod in portainer namespace by its specific name kc delete po -n portainer portainer-696988fd4-fzh24\rCopy\r2.3 Add Kubernetes Environment to Portainer Proceed to the Quick Setup Page on the Portainer UI and click on \u0026ldquo;Add Environment.\u0026rdquo;\nNext, select the Kubernetes option:\nExecute the following command to configure the agent:\nwget https://downloads.portainer.io/ce2-18/portainer-agent-k8s-lb.yaml -O agent-lb.yaml kca agent-lb.yaml\rCopy\rTo ensure the agent is running correctly, you can use the following command:\nkc get po -n portainer\rCopy\rAnd to get the node port:\nkc get svc -n portainer\rCopy\r2.4 Connect Portainer to K3s Cluster Enter the name and environment address in Portainer and click on \u0026ldquo;Connect\u0026rdquo;:\nCongratulations! You have successfully connected Portainer to your K3s cluster:\n3 Setting up Local Registry We will now set up a local Docker registry on our K3s cluster. A local registry allows us to efficiently manage container images and streamline the deployment process within our home lab environment.\n3.1 Install Docker on the K3s Cluster First, we need to install Docker on the K3s cluster. Run the following commands to install Docker and add the user pi to the docker group for easy management:\nsudo apt install docker.io sudo usermod -aG docker pi\rCopy\r3.2 Configure Docker for the Local Registry Next, we will configure Docker to use our local registry. Open the Docker daemon configuration file:\nsudo vi /etc/docker/daemon.json\rCopy\rCopy and paste the following content into daemon.json:\n{ \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;192.168.68.132:30500\u0026#34; ] }\rCopy\rSave and close the file. Now, edit the Docker default configuration file:\nsudo vi /etc/default/docker\rCopy\rAdd the following line to set the Docker options:\nDOCKER_OPTS=\u0026#34;--config-file=/etc/docker/daemon.json\u0026#34;\rCopy\rAfterward, restart Docker to apply the ‚Ä¶","date":"2023-07-30","permalink":"https://seehiong.github.io/posts/2023/07/setting-up-k3s/","summary":"In my latest blog post, I share my journey setting up K3S, a lightweight Kubernetes distribution, in my home lab. With a step-by-step guide, I install ‚Ä¶","tags":["K3s","K8s","Cluster","FastAPI","Docker","Portainer","HomeLab"],"title":"Setting up K3s"},{"content":"Having recently completed the enlightening Generative AI with Large Language Models\rcourse, where we gained invaluable knowledge and hands-on skills, we are now excited to share an exhilarating experience of running the LLaMA model in a Dockerized container.\nIn this guide, we\u0026rsquo;ll walk you through the setup and demonstrate how to unleash the full potential of running LLaMA Server within a Docker container.\nThe Setup Before we delve into the magic of LLaMA, let\u0026rsquo;s set up our application structure. To ensure smooth execution, we\u0026rsquo;ve structured our project as follows:\n|‚îÄ‚îÄ main.py\r‚îú‚îÄ‚îÄ Dockerfile\r‚îú‚îÄ‚îÄ requirements.txt\r|‚îÄ‚îÄ WizardLM-7B-uncensored.ggmlv3.q5_0.bin\rCopy\rThe main.py file contains the heart of our application, while the Dockerfile and requirements.txt facilitate the containerization process and handle dependencies. The WizardLM-7B-uncensored.ggmlv3.q5_0.bin model can be obtained from here\r.\nStarting Simple Let\u0026rsquo;s begin with a simple setup of our FastAPI application in the main.py file:\nfrom fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/\u0026#34;) async def index(): return {\u0026#34;Hello\u0026#34;: \u0026#34;World!\u0026#34;}\rCopy\rNext, we\u0026rsquo;ll tackle the Dockerfile to ensure smooth containerization:\nFROM python:3.11 WORKDIR /app COPY ./requirements.txt /app/requirements.txt RUN pip install --no-cache-dir -r /app/requirements.txt RUN useradd -m -u 1000 pi USER pi ENV HOME=/home/pi WORKDIR $HOME/app COPY --chown=pi . $HOME/app EXPOSE 8088 CMD python -m uvicorn main:app --host 0.0.0.0 --port 8088\rCopy\rOur requirements.txt file lists the necessary dependencies for our LLaMA Server:\nfastapi==0.85.1\rrequests==2.28\ruvicorn==0.18.3\rlangchain==0.0.234\rchromadb==0.3.29\rpypdf==3.12.2\rllama-cpp-python==0.1.72\rsentence_transformers==2.2.2\rCopy\rLet\u0026rsquo;s harness the true power of LLaMA by containerizing it with Docker. Follow these simple steps to get started:\nChange directory into your project folder (e.g., c:\\ai\\docker) cd c:\\ai\\docker\rCopy\rBuild the Docker image for our FastAPI application and tag it as fastapi docker build . -t fastapi\rCopy\rRun the Docker container, exposing port 8088 to access our LLaMA server docker run --rm -it -p 8088:8088/tcp fastapi\rCopy\rBuilding the AI Chatbot: From PDF Loading to FastAPI Integration In our previous post\r, we explored the exciting world of building a chatbot tailored specifically to interact with PDF files. From here on, we will repackage it, by importing all the essential dependencies that make this magic possible:\nfrom langchain.document_loaders import PyPDFLoader from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma from langchain.embeddings import HuggingFaceEmbeddings from langchain.llms import LlamaCpp from langchain.chains.question_answering import load_qa_chain\rCopy\rWe\u0026rsquo;ll kick things off by loading the Java Design Patterns 101 PDF using our handy PDF loader: def load(): loader = PyPDFLoader(\u0026#34;/home/pi/app/java-design-patterns-101.pdf\u0026#34;) return loader.load()\rCopy\rTo ensure seamless processing, we\u0026rsquo;ll split the texts into manageable chunks: def split(): text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\u0026#34;\\n\u0026#34;) return text_splitter.split_documents(app.document)\rCopy\rNext, we\u0026rsquo;ll leverage the power of Chroma to persist the index for faster retrieval: def persist(): vectordb = Chroma.from_documents(app.texts, persist_directory=\u0026#34;/home/pi/app/chroma\u0026#34;) vectordb.persist() return vectordb\rCopy\rNow comes the exciting part! We\u0026rsquo;ll put our AI chatbot to work and answer your queries: def query(question): docs = app.retriever.get_relevant_documents(query) answer = app.chain.run(input_documents=docs, question=question) return answer\rCopy\rWith FastAPI as our backbone, we\u0026rsquo;ll orchestrate the entire process by executing the following code on startup: @app.on_event(\u0026#34;startup\u0026#34;) async def startup_event(): app.document = load() app.texts = split() app.embeddings = HuggingFaceEmbeddings(model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) app.vectordb = persist() app.retriever = app.vectordb.as_retriever() app.llm = LlamaCpp(model_path=\u0026#34;/home/pi/app/WizardLM-7B-uncensored.ggmlv3.q5_0.bin\u0026#34;, verbose=True, n_ctx=4096) app.chain = load_qa_chain(app.llm, chain_type=\u0026#34;stuff\u0026#34;)\rCopy\rIt\u0026rsquo;s time to experience the power of our AI chatbot firsthand! Our new endpoint, \u0026ldquo;/chatpdf,\u0026rdquo; is all you need to interact with your Java Design Patterns 101 PDF: @app.get(\u0026#34;/chatpdf\u0026#34;) async def chat_pdf(question): answer = query(question) return {\u0026#34;answer\u0026#34;: answer}\rCopy\rOur Docker environment loads the LLM and downloads the required model, setting the stage for seamless interactions with your PDFs.\nChatbot for pdf in docker In this final segment, we unveil the full potential of our AI chatbot for PDFs, now fully Dockerized for seamless deployment. The main.py file is where the magic happens, integrating the power of FastAPI, HuggingFace ‚Ä¶","date":"2023-07-15","permalink":"https://seehiong.github.io/posts/2023/07/unleashing-the-power-of-llama-server-in-docker-container/","summary":"After completing the Generative AI with Large Language Models course, I\u0026rsquo;m thrilled to share my Dockerized experience running the LLaMA model. ‚Ä¶","tags":["LLaMA","Docker","FastAPI","Uvicorn","AI","OpenBLAS","Chatbot"],"title":"Unleashing the Power of LLaMA Server in Docker Container"},{"content":"In continuation with the previous\rpost, we will explore the power of AI by leveraging the whisper.cpp\rlibrary to convert audio to text, extracting audio from YouTube videos using yt-dlp\r, and demonstrating how to utilize AI models like GPT4All and OpenAI for summarization.\nSetting Up the Environment To get started, we need to set up the necessary tools and libraries. Follow the steps below:\nInstalling whisper.cpp: Begin by cloning the whisper.cpp repository from GitHub and downloading the base.en model. git clone https://github.com/ggerganov/whisper.cpp.git cd ~/whisper.cpp/models ./download-ggml-model.sh base.en\rCopy\rVerifying the Installation: Once the model is downloaded, you can test the installation by running a sample command: Downloading ggml model base.en from \u0026#39;https://huggingface.co/ggerganov/whisper.cpp\u0026#39; ... ggml-base.en.bin 100%[========================\u0026gt;] 141.11M 10.9MB/s in 14s Done! Model \u0026#39;base.en\u0026#39; saved in \u0026#39;models/ggml-base.en.bin\u0026#39; You can now use it like this: $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav\rCopy\rInstalling OpenBLAS: Ensure that OpenBLAS\r, a mathematical library, is installed on your system. This step is necessary for building whisper.cpp with OpenBLAS support. cd ~/whisper.cpp make clean WHISPER_OPENBLAS=1 make -j\rCopy\rThis is the sample output (with BLAS = 1):\nInstalling FFmpeg: FFmpeg\ris required for audio conversion. Install it using the appropriate package manager for your system. sudo apt install ffmpeg\rCopy\rInstalling yt-dlp and whispercpp: Use pip to install yt-dlp\r, a YouTube downloader, and the Python binding\rfor whisper.cpp. python3 -m pip install -U yt-dlp pip install whispercpp ffmpeg-python\rCopy\rExtracting Audio from YouTube Videos In this section, we will demonstrate how to extract audio from YouTube videos using the yt-dlp library. Here\u0026rsquo;s the code snippet:\nfrom __future__ import unicode_literals import yt_dlp def extract_audio(urls): ydl_opts = { \u0026#39;format\u0026#39;: \u0026#39;wav/bestaudio/best\u0026#39;, \u0026#39;postprocessors\u0026#39;: [{ \u0026#39;key\u0026#39;: \u0026#39;FFmpegExtractAudio\u0026#39;, \u0026#39;preferredcodec\u0026#39;: \u0026#39;wav\u0026#39;, }], \u0026#39;verbose\u0026#39;: \u0026#39;true\u0026#39;, \u0026#39;outtmpl\u0026#39;: \u0026#39;sample_audio\u0026#39;, } with yt_dlp.YoutubeDL(ydl_opts) as ydl: ydl.download(urls) URLS = [\u0026#39;https://www.youtube.com/watch?v=BaW_jenozKc\u0026#39;] extract_audio(URLS)\rCopy\rUtilizing Whisper to Transcribe Audio With the audio extracted, we can now utilize the whisper.cpp library to transcribe the audio into text. Follow the code snippet below:\nfrom whispercpp import Whisper w = Whisper.from_pretrained(\u0026#34;base.en\u0026#34;) import ffmpeg import numpy as np def transcribe_audio(filename): try: y, _ = ( ffmpeg.input(filename, threads=0) .output(\u0026#34;-\u0026#34;, format=\u0026#34;s16le\u0026#34;, acodec=\u0026#34;pcm_s16le\u0026#34;, ac=1, ar=16000) .run(cmd=[\u0026#34;ffmpeg\u0026#34;, \u0026#34;-nostdin\u0026#34;], capture_stdout=True, capture_stderr=True) ) except ffmpeg.Error as e: raise RuntimeError(f\u0026#34;Failed to load audio: {e.stderr.decode()}\u0026#34;) from e arr = np.frombuffer(y, np.int16).flatten().astype(np.float32) / 32768.0 return w.transcribe(arr) filename = \u0026#34;sample_audio.wav\u0026#34; transcribe_audio(filename)\rCopy\rThis code snippet uses the Whisper library to transcribe the audio stored in the \u0026ldquo;sample_audio.wav\u0026rdquo; file. The audio is processed using ffmpeg to convert it into a format suitable for transcription. The resulting transcription is stored in the transcripted variable and printed to the console:\nPutting it Together: Extracting and Summarizing YouTube Videos Now let\u0026rsquo;s bring everything together by extracting audio from a YouTube video and using AI models for summarization.\nFirst, we\u0026rsquo;ll download audio from a YouTube video. In this example, we\u0026rsquo;ll use the video LangChain Explained in 13 Minutes\ras an example. The downloaded audio will be saved as \u0026ldquo;sample_audio.wav\u0026rdquo;. Here\u0026rsquo;s the code snippet to download the audio:\nURLS = [\u0026#39;https://www.youtube.com/watch?v=aywZrzNaKjs\u0026#39;] extract_audio(URLS) filename = \u0026#34;sample_audio.wav\u0026#34;\rCopy\rNext, we\u0026rsquo;ll transcribe the audio using the Whisper library as shown earlier:\ntranscripted = transcribe_audio(filename) print(transcripted)\rCopy\rNow, let\u0026rsquo;s summarize the transcribed text using an AI model like GPT4All:\nfrom langchain.llms import GPT4All llm = GPT4All(model=\u0026#34;/home/pi/models/ggml-gpt4all-j-v1.3-groovy.bin\u0026#34;, n_ctx=2048) from langchain.chains.summarize import load_summarize_chain from langchain.chains import AnalyzeDocumentChain summary_chain = load_summarize_chain(llm, chain_type=\u0026#34;map_reduce\u0026#34;) summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain) summarize_document_chain.run(transcripted)\rCopy\rThe code above demonstrates the utilization of the GPT4All model for summarizing the transcribed text. It involves loading the required chains and initiating the summarization process.\nHowever, it appears that the generated results from the AI model are not ‚Ä¶","date":"2023-06-16","permalink":"https://seehiong.github.io/posts/2023/06/how-to-summarize-youtube-videos-in-minutes-ii/","summary":"In this comprehensive guide, I explore AI-powered techniques to extract and summarize YouTube videos using tools like Whisper.cpp, GPT4All, LLaMA.cpp, ‚Ä¶","tags":["LangChain","Youtube","GPT4All","OpenAI","Whisper","LLaMA","AI"],"title":"How to summarize YouTube Videos in Minutes (II)"},{"content":"Hey there, readers! Today, I\u0026rsquo;m thrilled to introduce you to an incredible tool that will completely transform the way you summarize YouTube videos. Get ready to dive into the captivating world of video content summarization using the powerful GPT4All. Trust me, this is an opportunity you don\u0026rsquo;t want to miss!\nSetting up the Magic Before we embark on this exciting journey, let\u0026rsquo;s ensure we have everything we need to get started. Install the necessary dependencies by running the following command:\npip install youtube-transcript-api transformers\rCopy\rOnce you\u0026rsquo;re all set, let\u0026rsquo;s move on to loading the transcripts of a fascinating YouTube video titled LangChain Explained in 13 Minutes\r. Here\u0026rsquo;s how you can achieve that using Python:\nfrom langchain.document_loaders import YoutubeLoader url = \u0026#39;https://www.youtube.com/watch?v=aywZrzNaKjs\u0026#39; loader = YoutubeLoader.from_youtube_url(url) transcript = loader.load() print(transcript)\rCopy\rBreaking it Down: Chunking the Transcripts To overcome the limitations of the LLM (Language Model), we need to divide the transcript into manageable chunks. If you\u0026rsquo;re unfamiliar with this process, take a look at my previous post\ron setting up GPT4All. For a more detailed explanation, explore the Recursive Character\rand Summarization\rexamples.\nHere\u0026rsquo;s how you can split the transcript using Python:\nfrom langchain.chains.summarize import load_summarize_chain from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 200) texts = text_splitter.split_documents(transcript) print(len(texts)) print(texts)\rCopy\rEnter GPT4All: Your Summarization Superpower Now that we have our transcript chunks ready, it\u0026rsquo;s time to unleash the power of GPT4All for summarization. Brace yourself for amazement! Here\u0026rsquo;s how you can set it up:\nfrom langchain.llms import GPT4All from langchain.prompts import PromptTemplate llm = GPT4All(model=\u0026#34;X:/ggml-gpt4all-j-v1.3-groovy.bin\u0026#34;, n_ctx=2048, n_threads=8)\rCopy\rLet the Magic Unfold: Executing the Chain Now, it\u0026rsquo;s time to witness the magic in action. Run the chain and watch as GPT4All generates a summary of the video:\nchain = load_summarize_chain(llm, chain_type=\u0026#34;map_reduce\u0026#34;, verbose=True) summary = chain.run(texts)\rCopy\rPrepare to be amazed as GPT4All works its wonders!\nimport textwrap wrapped_text = textwrap.fill(summary, width=80) print(wrapped_text)\rCopy\rIn this guide, we\u0026rsquo;ve utilized the powerful Map Reduce\rchain type. If you\u0026rsquo;re curious, you can explore other available options here\r.\nSummarizing with OpenAI (Optional) As a comparison, let\u0026rsquo;s explore an alternative approach using OpenAI for summarization. Follow the code snippet below to make the switch:\nimport os from dotenv import load_dotenv, find_dotenv _ = load_dotenv(find_dotenv()) from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature=0.9) chain = load_summarize_chain(llm, chain_type=\u0026#34;map_reduce\u0026#34;, verbose=True) summary = chain.run(texts)\rCopy\rAnd here\u0026rsquo;s the summary generated by OpenAI:\nStay Tuned: The Future of Video Content Summarization\nExciting things lie ahead! In my next\rblog post, we\u0026rsquo;ll explore the thrilling world of video contents without embedded transcripts. Stay tuned for more exciting updates!\n","date":"2023-06-10","permalink":"https://seehiong.github.io/posts/2023/06/how-to-summarize-youtube-videos-in-minutes-i/","summary":"Hey folks! Today, I\u0026rsquo;m stoked to introduce you to the game-changer that is GPT4All for summarizing YouTube videos. Join me on this journey of ‚Ä¶","tags":["LangChain","Youtube","GPT4All","OpenAI","AI","Summarization"],"title":"How to summarize YouTube Videos in Minutes (I)"},{"content":"Recently, I embarked on an exhilarating journey into the realm of receipt OCR using LangChain and OpenAI, inspired by the captivating course on LangChain for LLM Application Development\r. This exploration allowed me to unlock the full potential of PyTesseract, an extraordinary Python tool that serves as my guiding light for optical character recognition (OCR). By harnessing the power of OpenCV and seamlessly integrating OpenAI into the workflow, I aimed to compile the most optimal OCR results and validate them using LangChain\u0026rsquo;s impressive llm-math tool. Join me on this exciting adventure as we unravel the intricacies of receipt OCR and discover the true potential of LangChain, OpenAI, and PyTesseract.\nPyTesseract: Harnessing Optical Character Recognition Power To begin my exploration, I first ensured I had all the necessary tools at my disposal. Depending on the operating system, the Tesseract installer for Windows\rcan be downloaded from here. With the installation complete, I equipped myself with essential Python packages by executing the following command:\npip install pytesseract pillow opencv-python\rCopy\rNow that everything was set, I put PyTesseract to the test using a sample receipt. Here\u0026rsquo;s a glimpse of the image we\u0026rsquo;ll be working with:\nTo witness the capabilities of PyTesseract firsthand, I present you with the following code snippet:\nfrom PIL import Image import pytesseract # Defines the abosulate path to the executable pytesseract.pytesseract.tesseract_cmd = r\u0026#39;C:/Program Files/Tesseract-OCR/tesseract.exe\u0026#39; file = \u0026#39;C:/ocr/sample-receipt-ocr.jpeg\u0026#39; img = Image.open(file) print(pytesseract.image_to_string(img))\rCopy\rRunning this code for the first time, I eagerly awaited the results:\nJoin me on this exciting journey as we delve deeper into the remarkable capabilities of PyTesseract, uncovering its potential to extract text from images with unparalleled precision and accuracy.\nPytesseract and OpenCV: Unleashing Combined Power As I continued my exploration with the sample receipt, I refined my code to generate a comprehensive dictionary containing the OCR results obtained through various methods. Let me share with you the code responsible for this accomplishment:\nfrom PIL import Image import pytesseract import cv2 # Set Tesseract executable path pytesseract.pytesseract.tesseract_cmd = r\u0026#39;C:/Program Files/Tesseract-OCR/tesseract.exe\u0026#39; file = \u0026#39;C:/ocr/sample-receipt-ocr.jpeg\u0026#39; def perform_ocr(image, name): text = pytesseract.image_to_string(image) ocr_results.append({\u0026#39;name\u0026#39;: name, \u0026#39;text\u0026#39;: text}) # Read the image using OpenCV cvImage = cv2.imread(file) # Initialize OCR results list ocr_results = [] # Perform OCR on the original image fileImage = Image.open(file) perform_ocr(fileImage, \u0026#39;FILE\u0026#39;) # Perform OCR on the OpenCV image perform_ocr(cvImage, \u0026#39;CV\u0026#39;) # Perform OCR on the grayscale image gray = cv2.cvtColor(cvImage, cv2.COLOR_BGR2GRAY) perform_ocr(gray, \u0026#39;GRAY\u0026#39;) # Perform OCR on the thresholded image _, threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU) perform_ocr(threshold, \u0026#39;THRESHOLD\u0026#39;) # Perform OCR on the denoised image denoised = cv2.medianBlur(cvImage, 3) perform_ocr(denoised, \u0026#39;DENOISED\u0026#39;)\rCopy\rThroughout this process, I meticulously experimented with different methods to extract the text from the receipt. With each method, I added the results to a dictionary, capturing both the name of the method and the corresponding OCR output.\nOpenAI: Unleashing Language Model Power Allow me to introduce the next phase of our exploration: OpenAI. We have carefully crafted a prompt that will guide us through the integration of OpenAI into our OCR (Optical Character Recognition) use case:\ntemplate_string = \u0026#34;\u0026#34;\u0026#34;The ocr_result variable stores a list of results obtained from the OCR (Optical Character Recognition) process. \\ Each item in the list represents a specific configuration or variation of the OCR process, along with the corresponding extracted text. Please compare and merge the OCR results only when you are confident and avoid making assumptions. Provide the following mandatory items in the given order if they exist in the OCR results: Company Invoice number Date Sub-total Any extra charges Rounding adjustment Total amount Itemised result (if any) If possible, please provide the best effort list of itemized results with the pricing for each item. Please format the result as a string. OCR Text: {ocr_text} \u0026#34;\u0026#34;\u0026#34;\rCopy\rBy seamlessly configuring OpenAI into our workflow, we gain the ability to extract vital information from the OCR results with remarkable precision and clarity. Here\u0026rsquo;s how we set up OpenAI:\nimport os import openai from dotenv import load_dotenv, find_dotenv _ = load_dotenv(find_dotenv()) # read local .env file openai.api_key = os.environ[\u0026#39;OPENAI_API_KEY\u0026#39;] from langchain.chat_models import ChatOpenAI # To control the randomness and creativity ‚Ä¶","date":"2023-06-06","permalink":"https://seehiong.github.io/posts/2023/06/receipt-ocr-with-langchain-openai-and-pytesseract/","summary":"Recently, I embarked on an exhilarating journey into the realm of receipt OCR using LangChain and OpenAI, inspired by the captivating course on ‚Ä¶","tags":["OpenAI","LangChain","OCR","PyTesseract","AI","OpenCV","LLM-Math"],"title":"Receipt OCR with LangChain, OpenAI and PyTesseract"},{"content":"In this blog post, we will explore the usage of LangFlow\r, a Python library available on PyPI, to streamline the process of capturing ideas and conducting proof-of-concepts for our intended use case. Considering the current \u0026ldquo;trend\u0026rdquo; of tech layoffs, there might be a time (touch wood) where there is a need to go for interviews and fill-up various interview forms that require filling out personal information. Building upon the previous blog post on running GPT4All for PostgreSQL with LangChain (referenced here\r), we will now leverage LangFlow and OpenAI to automate the population of a sample employment form with our personal data stored in PostgreSQL.\nLet\u0026rsquo;s dive into the details and explore how LangFlow and OpenAI can simplify the auto-filling of employment forms, making the process more efficient and time-saving.\nSetup To get started with auto-filling a sample employment form using LangFlow and OpenAI, follow these steps:\nStep 1: Install LangFlow You can install LangFlow from PyPI using pip. Open your terminal or command prompt and execute the following command:\npip install langflow\rCopy\rStep 2: Run LangFlow Next, run LangFlow by executing the following command in your terminal or command prompt:\npython3 -m langflow\rCopy\rThis will start the LangFlow application.\nStep 3: Create the PostgreSQL Table Create a simple table named \u0026ldquo;person\u0026rdquo; in your PostgreSQL database, where you will store all the required information for an employment application form. Use the following SQL query to create the table:\nCREATE TABLE person ( id SERIAL PRIMARY KEY, firstname VARCHAR(50), lastname VARCHAR(50), address VARCHAR(100), email VARCHAR(100) );\rCopy\rStep 4: Download the Sample Fillable PDF Download the sample fillable PDF form template that you want to auto-fill. You can find a sample fillable PDF form template here\r.\nWith these setup steps completed, you are now ready to proceed with auto-filling the sample employment form using LangFlow and OpenAI.\nLangFlow Prototype and Potential Issues During the process of prototyping my use case using LangFlow, I encountered some challenges that I believe are worth highlighting. Since there were no existing examples\ravailable for SQLDatabaseChain in the LangFlow documentation, I had to explore different combinations and configurations to make it work. However, despite my efforts, I encountered the following error:\nError: too many values to unpack (expected 2)\rCopy\rThis error message indicates that there might be an issue related to the number of values being unpacked in the workflow. It took some time and investigation to identify the root cause of this error.\nTo tackle the problem, I carefully examined the LangFlow repository\rand made sure that the input data provided to each step was in the correct format. I also verified the number of output values produced by each step to ensure they matched the expected number of values for the subsequent steps. However, the error persisted.\nThe LangFlow diagram shown in the provided screenshot represents the flow I created to auto-fill the PDF form. Although it didn\u0026rsquo;t work as intended, it served as a starting point for my exploration:\nFrom the next section, I will proceed with the auto-filling proof-of-concepts using llama.cpp, langchain and postgreSQL.\nPreparation Before we proceed with auto-filling the sample employment form, we need to extract the form fields from the fillable PDF. Follow the steps below:\nImport the necessary libraries, PyPDF2 and json: import PyPDF2, json\rCopy\rOpen the fillable PDF file using the open() function, and create a PDF reader object: pdf_file = open(\u0026#39;fill-in-pdf-form-template.pdf\u0026#39;, \u0026#39;rb\u0026#39;) pdf_reader = PyPDF2.PdfReader(pdf_file)\rCopy\rRetrieve the form text fields from the PDF using the get_form_text_fields() method: form_text_fields = pdf_reader.get_form_text_fields()\rCopy\rCreate a dictionary to store the form field names as keys and placeholder values as values: json_data = {} for form_text_field in form_text_fields: json_data[form_text_field] = f\u0026#34;\u0026lt;{form_text_field.lower()}\u0026gt;\u0026#34;\rCopy\rConvert the dictionary to a JSON string using the json.dumps() function: json_string = json.dumps(json_data) print(json_string)\rCopy\rThe generated JSON, using JSON Pretty Print\r, will look like this:\n{ \u0026#34;City\u0026#34;: \u0026#34;\u0026lt;city\u0026gt;\u0026#34;, \u0026#34;PhoneNumber\u0026#34;: \u0026#34;\u0026lt;phonenumber\u0026gt;\u0026#34;, \u0026#34;PostalCode\u0026#34;: \u0026#34;\u0026lt;postalcode\u0026gt;\u0026#34;, \u0026#34;Address\u0026#34;: \u0026#34;\u0026lt;address\u0026gt;\u0026#34;, \u0026#34;Activities\u0026#34;: \u0026#34;\u0026lt;activities\u0026gt;\u0026#34;, \u0026#34;FirstName\u0026#34;: \u0026#34;\u0026lt;firstname\u0026gt;\u0026#34;, \u0026#34;HistorySupervisor\u0026#34;: \u0026#34;\u0026lt;historysupervisor\u0026gt;\u0026#34;, \u0026#34;HistoryPhoneNumber\u0026#34;: \u0026#34;\u0026lt;historyphonenumber\u0026gt;\u0026#34;, \u0026#34;HistoryPosition\u0026#34;: \u0026#34;\u0026lt;historyposition\u0026gt;\u0026#34;, \u0026#34;HistoryDuties\u0026#34;: \u0026#34;\u0026lt;historyduties\u0026gt;\u0026#34;, \u0026#34;HistoryLeaving\u0026#34;: \u0026#34;\u0026lt;historyleaving\u0026gt;\u0026#34;, \u0026#34;HistoryCompany\u0026#34;: \u0026#34;\u0026lt;historycompany\u0026gt;\u0026#34;, \u0026#34;LastName\u0026#34;: ‚Ä¶","date":"2023-05-26","permalink":"https://seehiong.github.io/posts/2023/05/autofill-pdf-with-langchain-and-langflow/","summary":"In this journey, I explore automating PDF autofill using LangChain and LangFlow. Leveraging LangFlow and OpenAI, I streamline the employment form ‚Ä¶","tags":["OpenAI","LLaMA","LangChain","LangFlow","PDF","PostgreSQL","AI"],"title":"Autofill PDF with LangChain and LangFlow"},{"content":"In this post, I will walk you through the process of setting up Python GPT4All\ron my Windows PC. Additionally, I will demonstrate how to utilize the power of GPT4All along with SQL Chain\rfor querying a postgreSQL database.\nPrerequisites Before we proceed with the installation process, it is important to have the necessary prerequisites in place.\nTo follow along with this guide, make sure you have the following:\nJupyter Notebook\rinstalled using Anaconda PostgreSQL\rsetup and ready to use Having these prerequisites fulfilled is crucial for the successful execution of the upcoming steps.\nFurthermore, for this demonstration, I have downloaded the GPT4All-J v1.3-groovy\rmodel.\nPreparations To begin, open Jupyter Notebook and install the necessary dependencies by running the following command:\npip install langchain azure.core gpt4all psycopg2\rCopy\rOnce the dependencies are installed, proceed with setting up LangChain and loading the model. Use the following code snippet:\nfrom langchain import PromptTemplate, LLMChain from langchain.llms import GPT4All from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler local_path=\u0026#39;X:/ggml-gpt4all-j-v1.3-groovy.bin\u0026#39; callbacks = [StreamingStdOutCallbackHandler()] llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\rCopy\rHere\u0026rsquo;s an image showcasing the Jupyter Notebook setup with LangChain and GPT4All model:\nFor the purposes of this demonstration, I utilized the PostgreSQL version of Open Source Shakespeare site\ras the sample database. We will be performing queries against this Shakespearean data throughout the guide.\nTo visualize the structure of the database, an Entity-Relationship (ER) diagram can be immensely helpful. Here is the ER diagram as viewed in DBeaver\r:\nThe diagram provides a comprehensive overview of the entities and their relationships within the database, aiding in understanding the data model and facilitating the execution of queries.\nQuerying GPT4All Now that we have set up the prompt template and LangChain, we can proceed with querying GPT4All:\ntemplate = \u0026#34;\u0026#34;\u0026#34;Question: {question} Answer: Let\u0026#39;s think step by step.\u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate(template=template, input_variables=[\u0026#34;question\u0026#34;]) llm_chain = LLMChain(prompt=prompt, llm=llm) question = \u0026#34;How does Shakespeare present the love between Romeo and Juliet?\u0026#34; llm_chain.run(question)\rCopy\rQuerying GPT4All with PostgreSQL To enable querying from PostgreSQL using SQLChain, we need to set up the SQLDatabase and SQLDatabaseChain. Here\u0026rsquo;s an example:\nfrom langchain import SQLDatabase, SQLDatabaseChain # Set up the SQLDatabase by providing the PostgreSQL connection URI and specifying the tables to include db = SQLDatabase.from_uri(\u0026#34;postgresql://postgres:postgres@192.168.68.132:5432/postgres\u0026#34;, include_tables=[\u0026#39;paragraph\u0026#39;,\u0026#39;chapter\u0026#39;,\u0026#39;character\u0026#39;,\u0026#39;work\u0026#39;]) # Define the prompt template _DEFAULT_TEMPLATE = \u0026#34;\u0026#34;\u0026#34;Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Use the following format: Question: \u0026#34;Question here\u0026#34; SQLQuery: \u0026#34;SQL Query to run\u0026#34; SQLResult: \u0026#34;Result of the SQLQuery\u0026#34; Answer: \u0026#34;Final answer here\u0026#34; Only use the following tables: {table_info} If someone asks for the book written, they really mean the work table. Question: {input}\u0026#34;\u0026#34;\u0026#34; PROMPT = PromptTemplate( input_variables=[\u0026#34;input\u0026#34;, \u0026#34;table_info\u0026#34;, \u0026#34;dialect\u0026#34;], template=_DEFAULT_TEMPLATE ) # Create an instance of SQLDatabaseChain db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True)\rCopy\rWith these setups in place, you can proceed to query PostgreSQL using SQLChain and prompt GPT4All with the obtained results:\nresponse = db_chain(\u0026#34;How many books are there in the work table\u0026#34;)\rCopy\rNote\rPlease note that generating the response may take some time, especially on a Windows PC. The process involves interacting with the database, running queries, and generating a response using GPT4All, which can be computationally intensive.\rQuerying OpenAI with PostgreSQL As a comparison, you can also utilize OpenAI for querying PostgreSQL using SQLChain. Here\u0026rsquo;s an example:\nfrom langchain.llms import OpenAI import os OpenAI.api_key = os.getenv(\u0026#39;OPENAI_API_KEY\u0026#39;) llm = OpenAI() _DEFAULT_TEMPLATE = \u0026#34;\u0026#34;\u0026#34;Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Use the following format: Question: \u0026#34;Question here\u0026#34; SQLQuery: \u0026#34;SQL Query to run\u0026#34; SQLResult: \u0026#34;Result of the SQLQuery\u0026#34; Answer: \u0026#34;Final answer here\u0026#34; Only use the following tables: {table_info} If someone asks for the book written, they really mean the work table. Question: {input}\u0026#34;\u0026#34;\u0026#34; from langchain import PromptTemplate PROMPT = PromptTemplate( ‚Ä¶","date":"2023-05-21","permalink":"https://seehiong.github.io/posts/2023/05/running-gpt4all-for-your-postgresql-with-langchain/","summary":"In this exploration, I guide you through setting up GPT4All on a Windows PC and demonstrate its synergy with SQL Chain for PostgreSQL queries using ‚Ä¶","tags":["GPT4All","OpenAI","LangChain","PostgreSQL","Jupyter","AI"],"title":"Running GPT4All for your PostgreSQL with LangChain"},{"content":"Referencing the previous post\r, we will run a web server which aims to act as a drop-in replacement for the OpenAI API, which can in turn be used by byogpt\r.\nPreparation (3 mins)\nPipenv\raims to help users manage environments, dependencies and imported packages and I will be using it in this guide.\npip install pipenv uvicorn fastapi sse_starlette pipenv shell\rCopy\rThis is the command to install the server:\nCMAKE_ARGS=\u0026#34;-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\u0026#34; pip install llama-cpp-python==0.2.24 --upgrade --force-reinstall --no-cache-dir\rCopy\rTo run the server:\npython3 -m llama_cpp.server --model ~/phi-2.Q4_K_M.gguf\rCopy\rTunnel to Server (2 mins)\nSince I am working from a windows PC, download and install PuTTY\r, an SSH and telnet client for Windows platform.\nOpen PuTTY and enter the IP addresses of my remote Ubuntu machine in the \u0026ldquo;Host Name\u0026rdquo; field, save the session. Under the \u0026ldquo;Connection\u0026rdquo; section, click on the \u0026ldquo;SSH\u0026rdquo; to expand the options and click on \u0026ldquo;Tunnels\u0026rdquo;.\nIn the \u0026ldquo;Source port\u0026rdquo; field, enter 8888 (or any other port number of your choice) and in the \u0026ldquo;Destination\u0026rdquo; field, enter \u0026rsquo;localhost:8000'.\nSelect the \u0026ldquo;Local\u0026rdquo; option and click on the \u0026ldquo;Add\u0026rdquo; button. The \u0026ldquo;Forwarded ports\u0026rdquo; section should now display the following entry:\nGo back to the \u0026ldquo;Session\u0026rdquo; section and save the session again.\nClick on the \u0026ldquo;Open button\u0026rdquo; to establish the SSH connection to the remote Ubuntu machine.\nEnter the username and password for the remote Ubuntu machine when prompted.\nThat\u0026rsquo;s it! You have successfully set up PuTTY in Windows to tunnel Ubuntu port 8000 to a local Windows port.\nNavigate to the following to access the Open API:\nhttp://localhost:8888/docs\rCopy\rConnecting from BYO-GPT (2 mins)\nBy changing the gpt_constant.dart\r, we can easily swap and connect to the above server. The change is as such:\nconst openaiChatCompletionEndpoint = \u0026#39;http://localhost:8888/v1/chat/completions\u0026#39;; const openaiCompletionEndpoint = \u0026#39;http://localhost:8888/v1/completions\u0026#39;;\rCopy\r","date":"2023-05-13","permalink":"https://seehiong.github.io/posts/2023/05/running-llama-server-in-local-machine/","summary":"In continuation from my previous post, I prepared the environment using Pipenv and installed the OpenAI-like web server with specific CMAKE arguments. ‚Ä¶","tags":["ChatGPT","ByoGPT","LLaMA","AI","Putty","Microsoft","Phi2"],"title":"Running LLaMA server in local machine"},{"content":"Extending the use case on the previous post\r, I will demostrate how you could ingest your own PDF file to your own LLaMa model in local machine\r.\nPreparation (2 mins)\nLet\u0026rsquo;s start off by installing Chroma\r, the open-source embedding database:\npip install chromadb pypdf\rCopy\rIngesting your PDF (5 mins)\nFor a start, I will ingest this Java-Design-Patterns\rfile: python3\rCopy\rfrom langchain.document_loaders import PyPDFLoader loader = PyPDFLoader(\u0026#34;/home/pi/java-design-patterns-101.pdf\u0026#34;) document = loader.load()\rCopy\rNext, I will split the document into chunks. Please change the chunk_size attributes based on your source contents and requirements. from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\u0026#34;\\n\u0026#34;) texts = text_splitter.split_documents(document)\rCopy\rWe will be using the default Chroma\u0026rsquo;s embeddings\r. By default, Chroma uses all-MiniLM-L6-v2. from langchain.vectorstores import Chroma vectordb = Chroma.from_documents(texts, persist_directory=\u0026#34;.\u0026#34;, metadatas=[{\u0026#34;source\u0026#34;: f\u0026#34;{i}-jdp\u0026#34;} for i in range(len(texts))]) vectordb.persist()\rCopy\rThe chroma DB will be stored in the current directory, stored as chroma-collections.parquet, chroma-embeddings.parquest and index folder\nWarning\rIt seems like the current codebase does not use default embedding function anymore. If you face this issue, you may provide the embeddings as such:\nfrom langchain.embeddings import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) vectordb = Chroma.from_documents(texts, embeddings, persist_directory=\u0026#34;.\u0026#34;, metadatas=[{\u0026#34;source\u0026#34;: f\u0026#34;{i}-jdp\u0026#34;} for i in range(len(texts))]) vectordb.persist()\rCopy\rTo load from the persisted DB, you can use this instead:\nfrom langchain.vectorstores import Chroma vectordb = Chroma(persist_directory=\u0026#34;/home/pi\u0026#34;)\rCopy\rTo load our local llama model on the local machine: from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.llms import LlamaCpp callback = [StreamingStdOutCallbackHandler()] llm = LlamaCpp( model_path=\u0026#34;/home/pi/llama.cpp/models/7B/ggml-model-q4_0.bin\u0026#34;, callbacks=callback, verbose=True, n_ctx=4096 )\rCopy\rQuestion and Answering (10 mins)\nLet\u0026rsquo;s do a question-answering with the PDF source over the vector database, with RetrievalQAWithSourcesChain\rretriever = vectordb.as_retriever() query=\u0026#34;What is a Singleton pattern as described by David Gallardo? Summarize in less than 100 words for a beginner\u0026#34; docs=retriever.get_relevant_documents(query) from langchain.chains.question_answering import load_qa_chain chain = load_qa_chain(llm, chain_type=\u0026#34;stuff\u0026#34;) chain.run(input_documents=docs, question=query)\rCopy\rYou may learn about Index-related chains\rfor combing your own data and start exploring question ansewering over your own documents! Happy chaining! Running with OpenBLAS (optional) (2 mins)\nReferencing to issues#32\r, I manage to get it run with BLAS by re-installating llama-cpp-python:\nLLAMA_OPENBLAS=on pip install --force-reinstall --ignore-installed --no-cache-dir llama-cpp-python\rCopy\rThis is the run with BLAS=1:\n","date":"2023-05-07","permalink":"https://seehiong.github.io/posts/2023/05/building-chatbot-for-your-pdf-files-with-langchain/","summary":"In this post, I extend the use case from my previous post to demonstrate building a ChatBot for PDF files using LangChain. In the preparation phase, I ‚Ä¶","tags":["ChatGPT","LLaMA","LangChain","PDF","Chroma","AI","OpenBLAS"],"title":"Building ChatBot for your PDF files with LangChain"},{"content":"\rLangChain\ris a framework for developing applications powered by language models. With the previous post\rsetup, I will follow closely to using Llama.cpp within LangChain\rfor building the simplest form of chain with LangChain.\nPreparation (2 mins)\nFirst, installs the required python packages:\nsudo pip install llama-cpp-python langchain Copy\rLLM Model (3 mins)\nRuns the following python codes through the interactive session: # Runs python3 from the location where the model file is located cd /home/pi/llama.cpp/models/13B python3\rCopy\rSets up the necessary import and formates the prompt template: from langchain.llms import LlamaCpp from langchain import PromptTemplate, LLMChain from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler template = \u0026#34;\u0026#34;\u0026#34;Question: {question} Answer: Assume that you are a Singaporean, let\u0026#39;s response in Singlish!\u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate(template=template, input_variables=[\u0026#34;question\u0026#34;])\rCopy\rSince there is a recent LangChain PR\ron the refactoring of CallbackManager, this is the modified code: callback = [StreamingStdOutCallbackHandler()] llm = LlamaCpp( model_path=\u0026#34;./ggml-model-q4_0.bin\u0026#34;, callbacks=callback, verbose=True )\rCopy\rCreates the chain by taking the question, formats it with promptTemplate and passes the formated response to LLM: llm_chain = LLMChain(prompt=prompt, llm=llm) question = \u0026#34;How can using LangChain and LLM help me ah?\u0026#34; llm_chain.run(question)\rCopy\rMemory (2 mins)\nAdds conversation buffer memory\rto the chain. This is the modified prompt: from langchain.prompts import PromptTemplate # Define the prompt template template = \u0026#34;\u0026#34;\u0026#34;This is a custom converation between you and AI. The AI is talkative and provides many specific details from the context. {history} Me: {input} AI:\u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate( input_variables=[\u0026#34;history\u0026#34;, \u0026#34;input\u0026#34;], template=template )\rCopy\rSets up the memory as such: from langchain.memory import ConversationBufferMemory from langchain.chains import ConversationChain # Initialize the conversation memory memory = ConversationBufferMemory(memory_key=\u0026#34;history\u0026#34;, input_key=\u0026#34;input\u0026#34;) # Initialize the conversation conversation = ConversationChain( llm=llm, verbose=True, prompt=prompt, memory=ConversationBufferMemory() )\rCopy\rChats with the AI: # Start the conversation with an initial message conversation.predict(input=\u0026#34;Hi!\u0026#34;)\rCopy\rContinues with user inputs: # Continue the conversation with user inputs while True: user_input = input(\u0026#34;You: \u0026#34;) conversation.predict(input=user_input)\rCopy\rAnd that\u0026rsquo;s all for this post! Will be exploring into the different areas in the upcoming posts. Stay tuned! ","date":"2023-05-01","permalink":"https://seehiong.github.io/posts/2023/05/building-a-basic-chain-with-langchain/","summary":"With the LangChain framework and a setup from a previous post, I delve into building a basic chain using Llama.cpp within LangChain. Following ‚Ä¶","tags":["ChatGPT","LLaMA","LangChain","AI"],"title":"Building a Basic Chain with LangChain"},{"content":"Preparation (30 mins)\nLLaMA\ris a collection of foundation language models ranging from 7B to 65B parameters.\nIn this guide, I will be using and following Georgi Gergano\u0026rsquo;s llama.cpp\r, a inference of LLaMA model in pure C/C++.\nI will be setting this up in a Ubuntu machine with 32Gb.\nTo prepare for the build system, I installed these:\nsudo apt install git cmake build-essential python3 python3-pip\rCopy\rClone and build the C/C++ codes: git clone https://github.com/ggerganov/llama.cpp -b b1680 cd llama.cpp # Checks that b1680 tag is checked out git describe --tags # using CMake mkdir build cd build cmake ..\rCopy\rcmake --build . --config Release\rCopy\rFor the Micorsoft\u0026rsquo;s Phi2\rmodel, I downloaded the GGUF format via here\r: wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\rCopy\rThat\u0026rsquo;s all! By following the provided setup, you can now comfortably run LLaMA (ChatGPT-like) model on your local machine without any worries about exposing your prompt or data. Running LLaMA model locally (5 mins)\nMicrosoft\u0026rsquo;s Phi2 Model This is an example of a few-shot interaction:\n./build/bin/main -m ~/phi-2.Q4_K_M.gguf -n 128 --repeat_penalty 1.0 --color -i -r \u0026#34;User:\u0026#34; -f prompts/chat-with-bob.txt\rCopy\rThis is the sample response for my prompt:\nhow to create a chatgpt like app in flutter, give me the step by step code starting with the main.dart\rCopy\rRunning with OpenBLAS (optional) (2 mins)\nOpenBLAS\ris an optimized Basic Linear Algebra Subprograms (BLAS) library. You may install with:\nsudo apt-get install libopenblas-dev\rCopy\rRebuilding llama with OpenBLAS on,\ncmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS cmake --build . --config Release # Rebuild agin after running the below command, if you see similar error: # CMake Error at /usr/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:230 #(message): # Could NOT find PkgConfig (missing: PKG_CONFIG_EXECUTABLE) sudo apt-get install pkg-config\rCopy\rWith the same prompt, this is the sample response with some speed improvements:\n","date":"2023-04-30","permalink":"https://seehiong.github.io/posts/2023/04/running-llama-model-locally/","summary":"In this thorough guide, I prepared my Ubuntu machine (32GB) for the LLaMA (Language Model) build. Following Georgi Gergano\u0026rsquo;s llama.cpp, I ‚Ä¶","tags":["ChatGPT","LLaMA","AI","Microsoft","Phi2","OpenBLAS"],"title":"Running LLaMA model locally"},{"content":"Developing a user-friendly interface to converse with ChatGPT via OpenAI\u0026rsquo;s API with your own openAI API token.\nDeveloping BYO-GPT with Flutter (Total Setup Time: 10 mins)\nIn this post, I will develop a \u0026ldquo;Bring Your Own - Generative Pre-Trained Transformer\u0026rdquo;, a user-friendly interface to converse with ChatGPT via OpenAI\u0026rsquo;s API\rwith Flutter\r.\nInstalling Flutter and IDE (4 mins)\nYou may download the latest Flutter SDK\rand follow the installation guide and update your path. To check the current version, you may use flutter \u0026ndash;version:\nYou may use any IDE but for me, I am using VSCode\r, with Dart and Flutter plugins installed.\nSetting project up (1 min)\nYou may start a flutter project by issuing flutter create project_byogpt. This is my initial pubspec.yaml. You may perform a manual dependency refresh with flutter pub get command:\nname: project_byogpt description: A new Flutter project. publish_to: \u0026#39;none\u0026#39; version: 1.0.0+1 environment: sdk: \u0026#39;\u0026gt;=2.19.6 \u0026lt;3.0.0\u0026#39; dependencies: flutter: sdk: flutter cupertino_icons: ^1.0.2 http: ^0.13.5 provider: ^6.0.5 dev_dependencies: flutter_test: sdk: flutter flutter_lints: ^2.0.0 flutter: uses-material-design: true\rCopy\rTo follow through this guide, you may replace your main.dart with:\nimport \u0026#39;package:flutter/material.dart\u0026#39;; void main() =\u0026gt; runApp(const MyApp()); class MyApp extends StatelessWidget { const MyApp({super.key}); @override Widget build(BuildContext context) { return MaterialApp( title: \u0026#34;BYO-GPT\u0026#34;, theme: ThemeData( primarySwatch: Colors.green, ), home: Scaffold( appBar: AppBar( title: const Text(\u0026#39;BYO-GPT\u0026#39;), ), body: Stack( children: \u0026lt;Widget\u0026gt;[ Container( color: Colors.black, margin: const EdgeInsets.only(bottom: 80), child: const Placeholder(), ), const Placeholder(), ], ), ), ); } }\rCopy\rAt any time, you may start running the app via menu Run \u0026gt; Run Without Debugging. You will see similar result in the default Chrome browser:\nCreating the Widgets and Models (3 mins)\nFirst, I will create the user\u0026rsquo;s bubble (under lib/widgets/user_bubble.dart) for displaying all user\u0026rsquo;s prompts:\nimport \u0026#39;package:flutter/material.dart\u0026#39;; class UserBubble extends StatelessWidget { const UserBubble(this.message, {super.key}); final String message; @override Widget build(BuildContext context) { return Stack( children: \u0026lt;Widget\u0026gt;[ Row(mainAxisAlignment: MainAxisAlignment.end, children: [ Container( decoration: BoxDecoration( color: Theme.of(context).primaryColorDark, borderRadius: const BorderRadius.only( topLeft: Radius.circular(12), topRight: Radius.circular(12), bottomLeft: Radius.circular(12), bottomRight: Radius.circular(0), ), ), width: 200, padding: const EdgeInsets.symmetric( vertical: 10, horizontal: 16, ), margin: const EdgeInsets.symmetric( vertical: 15, horizontal: 8, ), child: Column( crossAxisAlignment: CrossAxisAlignment.end, children: [ Text( message, style: const TextStyle( color: Colors.white, ), textAlign: TextAlign.end, ), ], ), ), ]), ], ); } }\rCopy\rSecond, let\u0026rsquo;s create the GPT\u0026rsquo;s bubble (under lib/widgets/gpt_bubble.dart) for displaying all GPT\u0026rsquo;s responses:\nimport \u0026#39;package:flutter/material.dart\u0026#39;; class GptBubble extends StatelessWidget { const GptBubble(this.message, {super.key}); final String message; @override Widget build(BuildContext context) { return Stack( children: [ Row(mainAxisAlignment: MainAxisAlignment.start, children: [ Container( decoration: BoxDecoration( color: Theme.of(context).colorScheme.secondary, borderRadius: const BorderRadius.only( topLeft: Radius.circular(12), topRight: Radius.circular(12), bottomLeft: Radius.circular(0), bottomRight: Radius.circular(12), ), ), width: 200, padding: const EdgeInsets.symmetric( vertical: 10, horizontal: 16, ), margin: const EdgeInsets.symmetric( vertical: 15, horizontal: 8, ), child: Column( crossAxisAlignment: CrossAxisAlignment.start, children: [ Text( message, style: const TextStyle( color: Colors.white, ), textAlign: TextAlign.start, ), ], ), ), ]), ], ); } }\rCopy\rThird, I created the user\u0026rsquo;s input (under lib/widgets/user_input.dart) and with the GPT image downloaded from here\rand pasted under assets/images (which does not exists during flutter project creation) folder:\nimport \u0026#39;package:flutter/material.dart\u0026#39;; import \u0026#39;package:provider/provider.dart\u0026#39;; import \u0026#39;../models/chat_model.dart\u0026#39;; class UserInput extends StatelessWidget { final TextEditingController chatcontroller; const UserInput({ Key? key, required this.chatcontroller, }) : super(key: key); @override Widget build(BuildContext context) { return Align( alignment: Alignment.bottomCenter, child: Container( padding: const EdgeInsets.only( top: 10, bottom: 10, left: 5, right: 5, ), decoration: const BoxDecoration( color: Colors.green, border: Border( top: BorderSide( color: Colors.greenAccent, width: 0.5, ), ), ), child: Row( children: [ Expanded( flex: 1, child: Image.asset( ‚Ä¶","date":"2023-04-29","permalink":"https://seehiong.github.io/posts/2023/04/developing-byo-gpt-with-flutter/","summary":"I dedicate around 10 minutes to create BYO-GPT, a Flutter app that allows easy interaction with ChatGPT through OpenAI\u0026rsquo;s API. After installing ‚Ä¶","tags":["ChatGPT","ByoGPT","GPT","Flutter","IDE","FE","Frontend"],"title":"Developing BYO-GPT with Flutter"},{"content":"Building a CI/CD pipeline on a Raspberry PI Cluster (Part 4), with SonarQube integrating into Jenkins and Gitlab\nBuilding a CI/CD pipeline on a Raspberry PI Cluster (Part IV) (Total Setup Time: 15 mins)\nIn continuation from part 3\rof this guide, I will add SonarQube\rinto my CI/CD pipeline. This will enahnce our workflow with continuous code quality and code security.\nInstalling SonarQube (30 mins)\nFirst, I download the SonarQube Community Edition\rinto my Windows machine.\n# Copy into kubernetes cluster master scp sonarqube-9.0.0.45539.zip ubuntu@master1:/tmp mkdir ~/sonarqube cd ~/sonarqube mv /tmp/sonarqube-9.0.0.45539.zip ~/sonarqube\rCopy\rSecond, downloads the Java Service Wrapper\rfor Linux, armhf architecture:\n# Copy into kubernetes cluster master scp wrapper-linux-armhf-64-3.5.45.tar.gz ubuntu@master1:/tmp mv /tmp/wrapper-linux-armhf-64-3.5.45.tar.gz ~/sonarqube\rCopy\rThird, prepares the Dockerfile for Raspberry PI:\n# Copy and paste below into Dockerfile vi Dockerfile\rCopy\rFROM balenalib/raspberrypi4-64-debian-openjdk:11-bullseye ENV SONARQUBE_HOME=/opt/sonarqube # Modified for debian-bullseye RUN groupadd -g 1000 sonarqube RUN useradd -d /home/${user} -u 1000 -g sonarqube -m sonarqube EXPOSE 9000 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install unzip -y WORKDIR /opt COPY sonarqube-9.0.0.45539.zip wrapper-linux-armhf-64-3.5.45.tar.gz /opt/ RUN unzip sonarqube-9.0.0.45539.zip \\ \u0026amp;\u0026amp; tar -zxvf wrapper-linux-armhf-64-3.5.45.tar.gz \\ \u0026amp;\u0026amp; rm -f sonarqube-9.0.0.45539.zip wrapper-linux-armhf-64-3.5.45.tar.gz \\ \u0026amp;\u0026amp; mv sonarqube-9.0.0.45539 sonarqube \\ \u0026amp;\u0026amp; mv wrapper-linux-armhf-64-3.5.45 wrapper \\ \u0026amp;\u0026amp; cp -r sonarqube/bin/linux-x86-64/ sonarqube/bin/linux-pi/ \\ \u0026amp;\u0026amp; cp -f wrapper/bin/wrapper sonarqube/bin/linux-pi \\ \u0026amp;\u0026amp; cp -f wrapper/lib/libwrapper.so sonarqube/bin/linux-pi/lib/libwrapper.so \\ \u0026amp;\u0026amp; cp -f wrapper/lib/wrapper.jar sonarqube/bin/linux-pi/lib/wrapper.jar \\ \u0026amp;\u0026amp; rm -rf wrapper \\ \u0026amp;\u0026amp; apt-get purge -y unzip \\ \u0026amp;\u0026amp; apt-get autoremove -y \\ \u0026amp;\u0026amp; chown -R sonarqube:sonarqube ${SONARQUBE_HOME} USER sonarqube VOLUME ${SONARQUBE_HOME}/data ${SONARQUBE_HOME}/extensions ${SONARQUBE_HOME}/logs/ WORKDIR ${SONARQUBE_HOME} CMD [\u0026#34;/bin/bash\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;./bin/linux-pi/sonar.sh console\u0026#34;]\rCopy\rFourth, builds and tags the docker image:\ndocker build -t seehiong/sonarqube:1.0 .\rCopy\rFifth, prepares the sonarqube deployment:\n# Copy and paste below into sonarqube-deployment.yaml vi sonarqube-deployment.yaml\rCopy\rapiVersion: v1 kind: Service metadata: name: sonarqube namespace: seehiong annotations: metallb.universe.tf/allow-shared-ip: home-net spec: ports: - port: 9000 name: sonarqube-http targetPort: 9000 selector: app: sonarqube type: LoadBalancer loadBalancerIP: 192.168.100.250 --- apiVersion: apps/v1 kind: Deployment metadata: name: sonarqube namespace: seehiong spec: selector: matchLabels: app: sonarqube strategy: type: Recreate template: metadata: labels: app: sonarqube spec: hostname: sonarqube initContainers: - name: init image: arm64v8/busybox:latest command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown -R 1000:1000 /opt/sonarqube/data /opt/sonarqube/extensions /opt/sonarqube/logs \u0026amp; sleep 30\u0026#34;] volumeMounts: - name: sonarqube-data-persistent-storage mountPath: /opt/sonarqube/data - name: sonarqube-ext-persistent-storage mountPath: /opt/sonarqube/extensions - name: sonarqube-log-persistent-storage mountPath: /opt/sonarqube/logs containers: - name: sonarqube image: seehiong/sonarqube:1.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9000 volumeMounts: - name: sonarqube-data-persistent-storage mountPath: /opt/sonarqube/data - name: sonarqube-ext-persistent-storage mountPath: /opt/sonarqube/extensions - name: sonarqube-log-persistent-storage mountPath: /opt/sonarqube/logs nodeSelector: hostname: master2 volumes: - name: sonarqube-data-persistent-storage persistentVolumeClaim: claimName: sonarqube-data-pvc - name: sonarqube-ext-persistent-storage persistentVolumeClaim: claimName: sonarqube-ext-pvc - name: sonarqube-log-persistent-storage persistentVolumeClaim: claimName: sonarqube-log-pvc\rCopy\rLastly, adds the required Longhorn volume, sonarqube-data-pvc, sonarqube-etc-pvc and sonarqube-log-pvc. You may start sonarqube pod by running (the default login is admin / admin):\nkubectl apply -f sonarqube-deployment.yaml # Pushes image to JFrog container registry docker tag seehiong/sonarqube:1.0 art.local:8081/seehiong/sonarqube:latest docker push art.local:8081/seehiong/sonarqube:latest\rCopy\rInstalling Gitlab (2 hrs)\nFirst, to work with SonarQube, I replace my Gitea with Gitlab installation\r. By following this guide\r, I build with the following:\nmkdir ~/gitlab cd ~/gitlab vi build.sh # Copy these into build.sh #!/bin/bash VERSION=13.4.0 # Replace BASE_URL and VERSION when the official arm64 package is available BASE_URL=https://packages.gitlab.com/gitlab/gitlab-ce/packages/ubuntu/focal/ git clone ‚Ä¶","date":"2021-07-18","permalink":"https://seehiong.github.io/posts/2021/07/ci/cd-pipeline-on-pi-cluster-iv/","summary":"In the fourth part of my Raspberry Pi CI/CD pipeline series, I integrated SonarQube into the workflow for continuous code quality and security. I ‚Ä¶","tags":["Raspberry Pi","K8s","CI/CD","Pipeline","SonarQube","GitLab","Jenkins"],"title":"CI/CD Pipeline on Pi Cluster (IV)"},{"content":"Building a CI/CD pipeline on a Raspberry PI Cluster (Part 3), with JFrog Container Registry supporting our Docker containers and Helm Chart repositories\nBuilding a CI/CD pipeline on a Raspberry PI Cluster (Part III) (Total Setup Time: 15 mins)\nContinue from part 2\rof this guide, I will add JFrog Container Registry\rto my CI/CD pipeline.\nInstalling Container Registry (5 mins)\nFirst, downloads the JFrog Container Registry\r.\nmkdir ~/artifactory/jcr cd ~/artifactory/jcr curl https://bintray.com/jfrog/artifactory/download_file?file_path=jfrog-artifactory-jcr-6.23.13.zip -o jfrog-artifactory-jcr-6.23.13.zip\rCopy\rSecond, prepares the Dockerfile for Raspberry PI:\n# Copy and paste below into Dockerfile vi Dockerfile\rCopy\rFROM balenalib/raspberrypi4-64-debian-openjdk:11-bullseye EXPOSE 8081 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install wget unzip -y WORKDIR /opt COPY jfrog-artifactory-jcr-6.23.13.zip /opt RUN mkdir jfrog \\ \u0026amp;\u0026amp; mv jfrog-artifactory-jcr-6.23.13.zip jfrog WORKDIR /opt/jfrog RUN export JFROG_HOME=/opt/jfrog RUN unzip jfrog-artifactory-jcr-6.23.13.zip \\ \u0026amp;\u0026amp; mv artifactory-jcr-6.23.13 artifactory \\ \u0026amp;\u0026amp; cd artifactory/bin WORKDIR /opt/jfrog/artifactory/bin CMD ./artifactoryctl\rCopy\rThird, builds and tags the docker image:\ndocker build -t seehiong/artifactory-jcr:1.0 .\rCopy\rFourth, prepare the JFrog container registry deployment:\n# Copy and paste below into jfrog-jcr-deployment.yaml vi jfrog-jcr-deployment.yaml\rCopy\rapiVersion: v1 kind: Service metadata: name: jfrog-jcr namespace: seehiong annotations: metallb.universe.tf/allow-shared-ip: home-net spec: ports: - port: 8081 targetPort: 8081 nodePort: 30181 name: jfrog-jcr-http selector: app: jfrog-jcr type: LoadBalancer loadBalancerIP: 192.168.100.249 --- apiVersion: apps/v1 kind: Deployment metadata: name: jfrog-jcr namespace: seehiong spec: selector: matchLabels: app: jfrog-jcr template: metadata: labels: app: jfrog-jcr spec: containers: - name: jfrog-jcr image: seehiong/artifactory-jcr:1.0 imagePullPolicy: Never ports: - containerPort: 8081 name: jfrog-jcr-http volumeMounts: - name: jfrog-jcr-data-persistent-storage mountPath: /opt/jfrog/artifactory/data - name: jfrog-jcr-etc-persistent-storage mountPath: /opt/jfrog/artifactory/etc initContainers: - name: init-volume image: arm64v8/busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;sleep 30\u0026#34;] nodeSelector: role: master volumes: - name: jfrog-jcr-data-persistent-storage persistentVolumeClaim: claimName: jfrog-jcr-data-pvc - name: jfrog-jcr-etc-persistent-storage persistentVolumeClaim: claimName: jfrog-jcr-etc-pvc\rCopy\rLastly, adds the required Longhorn volume, jfrog-jcr-data-pvc and jfrog-jcr-etc-pvc, similar to this:\nConfiguring JFrog Container Registry (8 mins)\nIt takes a while for Container Registry to setup. You may peek at the progress by:\nkubectl get po -n seehiong # Get logs (remember to replace with your pod name) kubectl logs -n seehiong jfrog-jcr-6c5b868bcf-lqppt -f # Get inside the container kubectl exec -n seehiong -it jfrog-jcr-6c5b868bcf-lqppt -- bash\rCopy\rFirst, logs into Container Registry using the default admin user and password.\nSecond, clicks Admin \u0026gt; Repository \u0026gt; Local menu item. adds New Local Repositories.\nThird, creates your own account and from Admin \u0026gt; Security \u0026gt; Permission menu. Adds a permission:\nFourth, clicks Admin \u0026gt; Configuration \u0026gt; HTTP Settings menu item. Selects Repository Path and uses Embedded Tomcat. With this setting, you will not require reverse proxy server.\nJFrog Container Registry Setup (2 mins)\nFirst, from testing insecure registry\r, adds insecure-registries setting in your /etc/docker/daemon.json:\n{ \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;insecure-registries\u0026#34;: [\u0026#34;art.local:8081\u0026#34;] }\rCopy\rSecond, adds art.local into your /etc/hosts, similar to this entry:\n192.168.100.249 art.local\rCopy\rThird, logs into JFrog Container Registry with this command:\ndocker login -u seehiong art.local:8081 # Tags and pushs artifactory-jcr image into JFrog Container Registry docker tag seehiong/artifactory-jcr:1.0 art.local:8081/seehiong/artifactory-jcr:latest docker push art.local:8081/seehiong/artifactory-jcr:latest\rCopy\rFourth, referencing pull an image from a private registry\r, creates your own registry secrets.\nkubectl create secret generic regcred --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson # Generates the output yaml kubectl get secret regcred --output=yaml\rCopy\r# Saves the output as registry-secrets.yaml and add namespace accordingly apiVersion: v1 data: .dockerconfigjson: ... kind: Secret metadata: name: regcred namespace: seehiong type: kubernetes.io/dockerconfigjson\rCopy\r# Applies the registry secrets with namespace defined kubectl delete secret regcred kubectl apply -f registry-secrets.yaml\rCopy\rFifth, ‚Ä¶","date":"2021-07-04","permalink":"https://seehiong.github.io/posts/2021/07/ci/cd-pipeline-on-pi-cluster-iii/","summary":"In the third part of my Raspberry Pi CI/CD pipeline series, I incorporated JFrog Container Registry. Following the previous guide, I installed the ‚Ä¶","tags":["Raspberry Pi","K8s","CI/CD","Pipeline","JFrog","Jenkins"],"title":"CI/CD Pipeline on Pi Cluster (III)"},{"content":"Building a CI/CD pipeline part 2 on a Raspberry PI Cluster, with JFrog Artifactory as the repository manager\nBuilding a CI/CD pipeline on a Raspberry PI Cluster (Part II) (Total Setup Time: 40 mins)\nContinuing from part I\rof this guide, I will add JFrog Artifactory\rto my CI/CD pipeline.\nPreparation (1 min)\nReferencing from my previous post on maven agent\r, let\u0026rsquo;s configure maven-agent to mount Longhorn volume. Navigate to Manage Jenkins \u0026gt; Manage Nodes and Clouds \u0026gt; Configure Clouds. Expand on Pod Template details and add a volume:\nClaim Name: maven-agent-pvc\rMount Path: /home/jenkins/.m2\rCopy\rInstalling Artifactory (13 mins)\nFirst, download the JFrog Artifactory OSS\redition. Due to some yq\rissues on Raspberry PI, I download the older version instead:\nmkdir ~/artifactory cd ~/artifactory curl https://releases.jfrog.io/artifactory/bintray-artifactory/org/artifactory/oss/jfrog-artifactory-oss/6.23.21/jfrog-artifactory-oss-6.23.21.zip -o jfrog-artifactory-oss-6.23.21.zip\rCopy\rSecond, prepare the Dockerfile for Raspberry PI:\n# Copy and paste below into Dockerfile vi Dockerfile\rCopy\rFROM balenalib/raspberrypi4-64-debian-openjdk:11-bullseye EXPOSE 8081 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install wget unzip -y WORKDIR /opt COPY jfrog-artifactory-oss-6.23.21.zip /opt RUN mkdir jfrog \\ \u0026amp;\u0026amp; mv jfrog-artifactory-oss-6.23.21.zip jfrog WORKDIR /opt/jfrog RUN export JFROG_HOME=/opt/jfrog RUN unzip jfrog-artifactory-oss-6.23.21.zip \\ \u0026amp;\u0026amp; mv artifactory-oss-6.23.21 artifactory \\ \u0026amp;\u0026amp; cd artifactory/bin WORKDIR /opt/jfrog/artifactory/bin CMD ./artifactoryctl\rCopy\rThird, build and tag the docker image:\ndocker build -t seehiong/artifactory:1.0 .\rCopy\rLast, create the required Longhorn volumes and prepare the Kubernetes deployment file. NOTE: I add delay in the init-container in order for Longhorn to stabilize. Also note that this directory structure is different from the latest Artifactory versions.\n# Copy and paste below into artifactory-deployment.yaml vi artifactory-deployment.yaml\rCopy\rapiVersion: v1 kind: Service metadata: name: artifactory namespace: seehiong annotations: metallb.universe.tf/allow-shared-ip: home-net spec: ports: - port: 8081 targetPort: 8081 nodePort: 30081 name: artifact-http selector: app: artifactory type: LoadBalancer loadBalancerIP: 192.168.100.250 --- apiVersion: apps/v1 kind: Deployment metadata: name: artifactory namespace: seehiong spec: selector: matchLabels: app: artifactory template: metadata: labels: app: artifactory spec: containers: - name: artifactory image: seehiong/artifactory:1.0 imagePullPolicy: Never ports: - containerPort: 8081 name: artifact-http volumeMounts: - name: artifactory-data-persistent-storage mountPath: /opt/jfrog/artifactory/data - name: artifactory-etc-persistent-storage mountPath: /opt/jfrog/artifactory/etc initContainers: - name: init-volume image: arm64v8/busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;sleep 30\u0026#34;] volumes: - name: artifactory-data-persistent-storage persistentVolumeClaim: claimName: artifactory-data-pvc - name: artifactory-etc-persistent-storage persistentVolumeClaim: claimName: artifactory-etc-pvc\rCopy\rConfiguring Artifactory (10 mins)\nIt takes a while for Artifactory to setup. In my case, I add maven during startup. You may peek at the progress by:\nkubectl get po -n seehiong # Get logs (remember to replace with your pod name) kubectl logs -n seehiong artifactory-99b667d47-jqxkv -f # Get inside the container kubectl exec -n seehiong -it artifactory-99b667d47-jqxkv -- bash\rCopy\rFirst, log into Artifactory using the default admin user and password. Creates your own account. From Admin \u0026gt; Security \u0026gt; Permission menu, adds a permission:\nSecond, from Artifacts menu, click on Set Me Up button. Select the maven, enter password and click on Generate Maven Settings button.\nThird, click on Generate Settings button and scroll down to the settings file. Click on Download Snippet link and copy this settings.xml file to the .m2 folder of the maven-agent-pvc volume. NOTE: Please remember to modify the user and password of your newly created account, which has deploy permission.\nLastly, copy the contents of the distribution management and paste into your project\u0026rsquo;s pom.xml file. NOTE: Depending on your setting, enter either libs-snapshot or libs-release, such as:\n\u0026lt;distributionManagement\u0026gt;\r\u0026lt;repository\u0026gt;\r\u0026lt;id\u0026gt;central\u0026lt;/id\u0026gt;\r\u0026lt;name\u0026gt;artifactory-99b667d47-gzcwk-releases\u0026lt;/name\u0026gt;\r\u0026lt;url\u0026gt;http://192.168.100.250:8081/artifactory/libs-snapshot\u0026lt;/url\u0026gt;\r\u0026lt;/repository\u0026gt;\r\u0026lt;/distributionManagement\u0026gt;\rCopy\rConfiguring Jenkins (1 min)\nFirst, from Manage Jenkins \u0026gt; Manage Plugins, click on Available tab and search for Artifactory. Installs Artifactory plugin.\nSecond, from Manage Jenkins \u0026gt; Configure System, scroll to JFrog section. Enters Artifactory URL and the deployer credential.\nDeploy JAR to Artifactory (5 mins)\nFirst, configure your project\u0026rsquo;s Jenkinsfile to include a deploy ‚Ä¶","date":"2021-06-21","permalink":"https://seehiong.github.io/posts/2021/06/ci/cd-pipeline-on-pi-cluster-ii/","summary":"In the second installment of my Raspberry Pi CI/CD pipeline series, I seamlessly integrated JFrog Artifactory. Following the groundwork laid in [Part ‚Ä¶","tags":["Raspberry Pi","K8s","CI/CD","Pipeline","JFrog","Jenkins","Longhorn"],"title":"CI/CD Pipeline on Pi Cluster (II)"},{"content":"Building a CI/CD pipeline on a Raspberry PI Cluster, with 3 master and 1 worker nodes, enclosed in a custom-made LEGO structure\nBuilding a CI/CD pipeline on a Raspberry PI Cluster (Part I) (Total Setup Time: 40 mins)\nIn this series, I will build my own CI/CD pipeline, with tools that are configured to run on Raspberry PI Cluster and Longhorn\r, a HA Raspberry PI Cluster. By end of this guide, you will have a self-hosted Git service\r, working hand-in-hand with Jenkins\r.\nPreparation (5 mins)\nMetallb\ris a load-balancer implementation for bare metal Kubernetes clusters. You may refer to the previous guide\ron Metallb. Let\u0026rsquo;s install it by with these commands:\nmkdir ~/metallb cd ~/metallb wget https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml -O metallb-namespace.yaml kubectl apply -f metallb-namespace.yaml wget https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml -O metallb.yaml kubectl apply -f metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026#34;$(openssl rand -base64 128)\u0026#34;\rCopy\rThis is my layer2 configuration, in metallb-config.yaml:\napiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.100.200-192.168.100.250 kubectl apply -f metallb-config.yaml\rCopy\rI create 4 volumes, namely mysql, gitea, jenkins and maven-agent, specific to each of the tools.\nInstalling Mysql (5 mins)\nReferencing Gitea for Kubernetes Cluster\r, these are the steps for installing Mysql.\nkubectl create namespace seehiong docker pull mysql/mysql-server:latest mkdir ~/mysql cd ~/mysql vi mysql-deployment.yaml\rCopy\r# Insert below into mysql-deployment.yaml apiVersion: v1 kind: Service metadata: name: mysql namespace:seehiong spec: ports: - port: 3306 selector: app: mysql clusterIP: None --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql namespace:seehiong spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: hostname: mysql containers: - image: mysql/mysql-server:latest imagePullPolicy: IfNotPresent name: mysql env: # Use secret in real usage - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pvc\rCopy\rkubectl apply -f mysql-deployment.yaml\rCopy\rYou may check on the pod ID and run the following scripts:\nkubectl get pods -n seehiong kubectl exec --namespace=seehiong -it mysql-6587f996b5-qrcr6 -- /bin/bash mysql -u root -p\rCopy\r# Execute these within the docker prompt, with username=gitea, password=gitea, database=gitedb CREATE USER \u0026#39;gitea\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;gitea\u0026#39;; CREATE DATABASE giteadb; GRANT ALL PRIVILEGES ON giteadb.* TO \u0026#39;gitea\u0026#39;; FLUSH PRIVILEGES;\rCopy\rInstalling Gitea (5 mins)\nFirst, installs Gitea by running thse commands:\napt install docker-compose mkdir ~/gitea cd ~/gitea vi docker-compose.yml\rCopy\r# Insert below into docker-compose.yml version: \u0026#34;2\u0026#34; networks: gitea: external: false services: server: image: gitea/gitea:latest environment: - USER_UID=1000 - USER_GID=1000 - DB_TYPE=mysql - DB_HOST=mysql:3306 - DB_NAME=giteadb - DB_USER=gitea - DB_PASSWD=gitea restart: always networks: - gitea volumes: - ./gitea:/data - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro ports: - \u0026#34;3000:3000\u0026#34; - \u0026#34;222:22\u0026#34;\rCopy\r# Builds, starts and stops container docker-compose up -d docker-compose down\rCopy\rSecond, creates the deployment file for Gitea:\nvi gitea-deployment.yaml\rCopy\r# Insert below into gitea-deployment.yaml apiVersion: v1 kind: Service metadata: name: gitea namespace: seehiong annotations: metallb.universe.tf/allow-shared-ip: home-net spec: ports: - port: 3000 targetPort: 3000 nodePort: 30000 name: gitea-http - port: 2222 targetPort: 2222 nodePort: 32222 name: gitea-ssh selector: app: gitea type: LoadBalancer loadBalancerIP: 192.168.100.250 --- apiVersion: apps/v1 kind: Deployment metadata: name: gitea namespace: seehiong spec: replicas: 1 selector: matchLabels: app: gitea strategy: type: Recreate template: metadata: labels: app: gitea spec: hostname: gitea containers: - image: gitea/gitea:latest imagePullPolicy: IfNotPresent name: gitea ports: - containerPort: 3000 name: gitea-http - containerPort: 22 name: gitea-ssh volumeMounts: - name: gitea-persistent-storage mountPath: /data volumes: - name: gitea-persistent-storage persistentVolumeClaim: claimName: gitea-pvc\rCopy\rkubectl apply -f gitea-deployment.yaml kubectl get svc -n seehiong\rCopy\rThird, access to the configured url, i.e. 192.168.100.250:3000 and follows the gitea installation. For my case, the parameters are:\nusername: gitea\rpassword: gitea\rdatabase: giteadb\rCopy\rInstalling Jenkins (5 mins)\nBy referencing Jenkins for Kuberenetes Cluster\r, ‚Ä¶","date":"2021-06-13","permalink":"https://seehiong.github.io/posts/2021/06/ci/cd-pipeline-on-pi-cluster-i/","summary":"In this series, I documented my journey building a CI/CD pipeline on a Raspberry Pi Cluster, featuring 3 master and 1 worker nodes, all housed in a ‚Ä¶","tags":["Raspberry Pi","K8s","CI/CD","Pipeline","MySQL","Gitea","Jenkins"],"title":"CI/CD Pipeline on Pi Cluster (I)"},{"content":"By running Raspberry PI Cluster with Longhorn, you will have a simplified, easy to deploy cloud-native persistent block storage\nRaspberry Pi Cluster and Longhorn (Total Setup Time: 30 mins)\nIn this section, I will install Longhorn\r, a highly available persistance storage for Kubernetes. This guide assumes that you have a working Raspberry PI Cluster. If you do not have, please follow Kubernetes Cluster with Ansible\ror HA Kubernetes Pi Cluster (Part I)\r.\nPreparation (10 mins)\nFirst, you need to install helm\r.\ncurl https://baltocdn.com/helm/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \u0026#34;deb https://baltocdn.com/helm/stable/debian/ all main\u0026#34; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm # Check helm version helm version # Helm version result version.BuildInfo{Version:\u0026#34;v3.6.0\u0026#34;, GitCommit:\u0026#34;7f2df6467771a75f5646b7f12afb408590ed1755\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.16.3\u0026#34;}\rCopy\rSecond, for the managing of Calico\rresources, I installed the command line interface\r.\ncurl -o calicoctl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.18.4/calicoctl-linux-arm64 chmod +x calicoctl ./calicoctl version # Export configuration export KUBECONFIG=~/.kube/config export DATASTORE_TYPE=kubernetes # Check the configuration calicoctl get workloadendpoints calicoctl get nodes\rCopy\rThird, if you wish to, you may install kubernetes-dashboard\rwith these commands:\n# Installs Web UI curl https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml -o kubernetes-dashboard.yaml kubectl apply -f kubernetes-dashboard.yaml # Creates cluster-admin role kubectl create serviceaccount dashboard kubectl create clusterrolebinding dashboard --clusterrole=cluster-admin --serviceaccount=default:dashboard # Gets login token kubectl get secrets kubectl describe secret dashboard-token-7xmhx # Runs proxy on your host machine kubectl proxy\rCopy\rFinally, you may access the Kubernetes Web UI by pointing to:\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\rCopy\rInstalling Longhorn (20 mins)\nFirst, following the official guide\r, please run environment_check.sh and verify that all is good to go.\nSecond, install Longhorn with Helm 3 with these commands:\nkubectl create namespace longhorn-system helm install longhorn longhorn/longhorn --namespace longhorn-system # Confirm that deployment is successful (this might take quite a while) watch kubectl get pod -n longhorn-system\rCopy\rThird, you may access the Longhorn using these commands:\n# Get name of longhorn-frontend kubectl get svc -n longhorn-system # Runs port-forward on your host machine kubectl port-forward services/longhorn-frontend 8080:http -n longhorn-system\rCopy\rThat\u0026rsquo;s it! The Raspberry PI Cluster with Longhorn is ready! Now, you have a highly availale persistance storage.\nTroubleshooting Calico node running but show up as 0/1 With the CLI command, calicoctl node status, I realized that the peer address are in different range.\n# Check IP address ip addr # Re-configure to using same interface kubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=interface=eth0 # Re-apply Kubernetes setting kubectl apply -f ~/calico-etcd.yaml\rCopy\rlonghorn-system namespace deletion stuck in terminating While uninstalling Longhorn, I faced this issue.\nhelm uninstall longhorn -n longhorn-system kubectl delete namespace longhorn-system # Run this command when namespace deletion takes forever kubectl get namespace longhorn-system -o json \\ | tr -d \u0026#34;\\n\u0026#34; | sed \u0026#34;s/\\\u0026#34;finalizers\\\u0026#34;: \\[[^]]\\+\\]/\\\u0026#34;finalizers\\\u0026#34;: []/\u0026#34; \\ | kubectl replace --raw /api/v1/namespaces/longhorn-system/finalize -f -\rCopy\rError: rendered manifests contain a resource that already exists. During re-installing of Longhorn, I realized some of the custom resources definition were not removed.\n# Check if there are any crd ending with longhorn.io kubectl get crd # Delete each of those kubectl patch crd/engineimages.longhorn.io -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl patch crd/instancemanagers.longhorn.io -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge\rCopy\rNode does not appear in Longhorn You may check on the taint value if any of the master nodes are not in Longhorn.\nkubectl describe node master3\rCopy\r# Sample result\r...\rTaints: node-role.kubernetes.io/master:NoSchedule\r...\r# Remove the taintness from master3\rkubectl taint node master3 node-role.kubernetes.io/master:NoSchedule-\rCopy\r","date":"2021-06-06","permalink":"https://seehiong.github.io/posts/2021/06/pi-cluster-with-longhorn/","summary":"I documented my journey setting up a Raspberry Pi Cluster with Longhorn for simplified cloud-native persistent block storage. The process took 30 ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Helm","Calio","Storage","Longhorn"],"title":"Pi Cluster with Longhorn"},{"content":"By provisioning a Kubernetes PI Cluster with Ansible, you can easily spin off a Raspberry PI cluster\nKubernetes Cluster with Ansible (Total Setup Time: 50 mins)\nIn this guide, I will configure a Kubernetes Cluster using Ansible\r. This guide follows closely to the Raspbernetes Cluster Installation\r. I will be using 3x Raspberry Pi 4 Model B 8GB as the master nodes and 1x Raspberry Pi 3 Model B as the only worker node.\nYou need to install ansible\r, flash\rand kubectl\r. I installed both Ansible and Flash onto my elementary OS\rhost by:\n#Installing Ansible sudo apt update sudo apt install software-properties-common sudo add-apt-repository --yes --update ppa:ansible/ansible sudo apt install ansible #Installing Flash curl -LO https://github.com/hypriot/flash/releases/download/2.7.0/flash cdmod +x flash sudo mv flash /usr/local/bin/flash sudo apt-get install -y pv curl python-pip unzip hdparm sudo pip install awscli\rCopy\rPreparation (20 mins)\nFirstly, downloads the Ubuntu 20.04 image and unzips it:\ncurl -L \u0026#34;http://cdimage.ubuntu.com/releases/20.04.2/release/ubuntu-20.04.2-preinstalled-server-arm64+raspi.img.xz\u0026#34; -o ~/Downloads/ubuntu-20.04.2-preinstalled-server-arm64+raspi.img.xz unxz -T 0 ~/Downloads/ubuntu-20.04.2-preinstalled-server-arm64+raspi.img.xz\rCopy\rSecondly, clones the Raspbernetes repository\rand makes the necessary changes (hostname, users.name, ssh_authorized_keys, etho0.addresses and gateway4) to the cloud-init file. I have created 4x cloud-config.yml for each of the 64GB SD card:\ngit clone https://github.com/raspbernetes/k8s-cluster-installation.git cd k8s-cluster-installation vi setup/cloud-config.yml #Create the 4x SD cards ubuntu image using Flash sudo flash --userdata cloud-config.yml ~/Downloads/ubuntu-20.04.2-preinstalled-server-arm64+raspi.img\rCopy\rThirdly, ensures that Ansible inventory is changed according to hostname, username and IP addresses for each SD card.\nvi ansible/inventory\rCopy\r#My setup as reference [masters] master1 hostname=master1 ansible_host=192.168.100.180 ansible_user=ubuntu master2 hostname=master2 ansible_host=192.168.100.181 ansible_user=ubuntu master3 hostname=master3 ansible_host=192.168.100.182 ansible_user=ubuntu [workers] worker1 hostname=worker1 ansible_host=192.168.100.188 ansible_user=ubuntu #Ensures that you are able to ping all nodes env ANSIBLE_CONFIG=ansible/ansible.cfg ansible all -m ping\rCopy\rFourthly, prior to the cluster setup, I ssh into each of the nodes and run the following.\nssh ubuntu@master1 sudo -i systemctl stop apt-daily.timer systemctl disable apt-daily.timer systemctl mask apt-daily.service systemctl daemon-reload reboot\rCopy\rThese are some of my customizations:\n# group_vars/all.yml 26: keepalived_vip: \u0026#39;192.168.100.200\u0026#39; 32: cri_plugin: docker 38: cni_plugin: \u0026#39;calico\u0026#39; # group_var/masters.yml 16: cluster_kube_proxy_enabled:true 23: cni_plugin: \u0026#39;calico\u0026#39; # roles/cluster/defaults/main/main.yml 101: docker: \u0026#39;unix:///run/containerd/containerd.sock\u0026#39; # roles/clusters/templates/kubeadm-config.yaml.j2 25: criSocket: unix:///run/containerd/containerd.sock # roles/clusters/templates/kubeadm-join.yaml.j2 26: criSocket: unix:///run/containerd/containerd.sock # roles/cni/defaults/main.yml 3: cni_bgp_peer_address: 192.168.100.1 # roles/common/tasks/main.yml (added to line 1) - name: Bugfix for Ubuntu 20.04 set_fact: ansible_os_family: \u0026#34;debian\u0026#34; # roles/container-runtime/containerd/defaults/main.yml 9: containerd_use_docker_repo: true # roles/container-runtime/containerd/defaults/main.yml (added to line 1) # Instructions: https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository - name: Add Docker‚Äôs official GPG key apt_key: url: https://download.docker.com/linux/ubuntu/gpg state: present - name: Add apt repository for Docker apt_repository: repo: deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu focal stable state: present regiser: docker_repository until: docker_repository is success # roles/container-runtime/defaults/main.yml 2: cri_plugin: \u0026#39;docker\u0026#39; # roles/container-runtime/docker/defaults/main.yml 12: #- docker-ce 13: #- docker-ce-cli 14: #- containerd.io 15: - docker.io # roles/container-runtime/vars/main.yml 23: docker: \u0026#39;unix:///run/containerd/containerd.sock\u0026#39; # roles/haproxy/templates/haproxy.cfg.j2 22: server {{hostvars[host][\u0026#39;hostname\u0026#39;] }} {{hostvars[host][\u0026#39;ansible_host\u0026#39;] }}:6433 check\rCopy\rInstallation (30 mins)\nFirst, you may start the Kubernetes cluster setup by running Ansible playbook\r:\nenv ANSIBLE_CONFIG=ansible/ansible.cfg ansible-playbook ansible/playbooks/all.yml\rCopy\rSecond, when installation completes, you may copy contents of ansible/playbooks/output/k8s-config.yaml to your local machine:\n# Create a kube folder in host machine mkdir ~/.kube # Paste copied content into config file vi ~/.kube/config kubectl get nodes\rCopy\rFinally, the Kubernetes PI Cluster with Ansible is ready for ‚Ä¶","date":"2021-05-29","permalink":"https://seehiong.github.io/posts/2021/05/k8s-pi-cluster-with-ansible/","summary":"I documented my journey configuring a Kubernetes Cluster on a Raspberry Pi using Ansible, totaling 50 minutes. I deployed three Raspberry Pi 4 Model B ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Ansible","Raspbernetes"],"title":"K8s Pi Cluster with Ansible"},{"content":"By having a Highly Available Kubernetes Pi Cluster, you will have full control over your production grade environment on-premise\nHA Kubernetes Pi Cluster (Part II) (Total Setup Time: 20 mins)\nIn this follow up guide, I will configure the HA Kubernetes Cluster onto the previous external etcd setup\r.\nInstalling Docker (5 mins)\nFirstly, installs docker container runtimes\rto all master nodes:\nsudo apt install docker.io sudo usermod -aG docker ubuntu su - ubuntu sudo systemctl enable docker.service\rCopy\rSecondly, sets up the docker daemon:\n# Run su commnad as root sudo su - # Set up the Docker daemon cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } EOF\rCopy\rThirdly, enables and restarts Docker:\nsystemctl daemon-reload systemctl enable docker systemctl restart docker\rCopy\rInstalling kudeadm (10 mins)\nStarts by adding Kubernetes repository to all master nodes:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF\rCopy\rNext, installs kubeadm\r:\nsudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl sudo reboot\rCopy\rInitializing Kubernetes Master Nodes (5 mins)\nFirst, prepares the certs by following kubeadm HA setup\r:\nsudo mkdir -p /etc/kubernetes/pki/etcd/ sudo cp /srv/etcd-certs/cacert.pem /etc/kubernetes/pki/etcd/ sudo cp /srv/etcd-certs/etcd.pem /etc/kubernetes/pki sudo cp /srv/etcd-certs/etcd-key.pem /etc/kubernetes/pki\rCopy\rSecond, inserts the following into kubeadm-config.yaml:\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: stable controlPlaneEndpoint: \u0026#34;cluster-endpoint:6443\u0026#34; etcd: external: endpoints: - https://192.168.100.119:2379 - https://192.168.100.173:2379 - https://192.168.100.100:2379 caFile: /etc/kubernetes/pki/etcd/cacert.pem certFile: /etc/kubernetes/pki/etcd.pem keyFile: /etc/kubernetes/pki/etcd-key.pem\rCopy\rThis is my /etc/hosts for all master nodes:\n# cluster-endpoint points to first master node 127.0.0.1 localhost 192.168.100.119 master1 192.168.100.173 master2 192.168.100.100 master3 192.168.100.119 cluster-endpoint\rCopy\rThird, initializes the master node by:\nsudo kubeadm init --config kubeadm-config.yaml --upload-certs mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\rCopy\rFourth, installs Calio\rfrom the list of networking addons\r:\ncurl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o calico-etcd.yaml\rCopy\rFifth, prepares TLS for calico by following config options\r:\ncat /srv/etcd-certs/cacert.pem | base64 -w 0 cat /srv/etcd-certs/etcd.pem | base64 -w 0 sudo cat /srv/etcd-certs/etcd-key.pem | base64 -w 0\rCopy\r# Searches for the Secret section in calico.yaml and paste cat output from the cacert.pem, etcd.pem and etcd-key.pem apiVersion: v1 kind: Secret type: Opaque metadata: name: calico-etcd-secrets namespace: kube-system data: # Populate the following files with etcd TLS configuration if desired, but leave blank if # not using TLS for etcd. # This self-hosted install expects three files with the following names. The values # should be base64 encoded strings of the entire contents of each file. etcd-key: LS0tLS1CRUdJTiB...VZBVEUgS0VZLS0tLS0= etcd-cert: LS0tLS1...ElGSUNBVEUtLS0tLQ== etcd-ca: LS0tLS1CRUdJTiBD...JRklDQVRFLS0tLS0= --- # Source: calico/templates/calico-config.yaml # This ConfigMap is used to configure a self-hosted Calico installation. kind: ConfigMap apiVersion: v1 metadata: name: calico-config namespace: kube-system data: # Configure this with the location of your etcd cluster. etcd_endpoints: \u0026#34;https://cluster-endpoint:2379\u0026#34; # If you\u0026#39;re using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: \u0026#34;/calico-secrets/etcd-ca\u0026#34; etcd_cert: \u0026#34;/calico-secrets/etcd-cert\u0026#34; etcd_key: \u0026#34;/calico-secrets/etcd-key\u0026#34;\rCopy\rkubectl apply -f calico.yaml watch kubectl get pods --all-namespaces kubectl get nodes -o wide\rCopy\rLastly, runs this on the other 2 master nodes:\nsudo kubeadm join cluster-endpoint:6443 --token puuck4.kjgqjif1xyy2397f \\ --discovery-token-ca-cert-hash sha256:50bb31b6f26ee6e98228f098fb9f50eaf7f1db67a011c90d6c764f7331cb90e1 \\ --control-plane --certificate-key 28ad20c49a7c6b8d06b2faa305fdf0c98d9763b932b929deb4704d57ec8ff959\rCopy\rThis is my modified /etc/hosts:\n# cluster-endpoint points to the loadbalancer IP 127.0.0.1 localhost 192.168.100.119 master1 192.168.100.173 master2 192.168.100.100 master3 192.168.100.200 cluster-endpoint\rCopy\rAnd that\u0026rsquo;s all to it! You now own a ‚Ä¶","date":"2020-08-17","permalink":"https://seehiong.github.io/posts/2020/08/ha-k8s-pi-cluster-ii/","summary":"In my journey to establish a Highly Available Kubernetes Pi Cluster, I\u0026rsquo;ve successfully configured the cluster following an external etcd setup. ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","HA","Docker","Kubeadm","Calio"],"title":"HA K8s Pi Cluster (II)"},{"content":"By having a Highly Available Kubernetes Pi Cluster, you will have full control over your production grade environment on-premise\nHA Kubernetes Pi Cluster (Part I) (Total Setup Time: 25 mins)\nOn this special day, I will like to wish all Singaporeans and Singapore a Happy 55th National Day\r!\nWith the newly purchase 2x Raspberry Pi Model B 8GB and 64GB SD card to my collection, I will setup a Highly Available Kubernetes Pi Cluster. In this guide, I will setup an external etcd\rkey-value store. In the next article\r, I will continue with the HA configuration.\nPreparing OS (10 mins)\nFirst, I am using Ubuntu Server (64-bit)\ras my OS. After burning the image onto my 64GB SD card, create an empty file ssh in d:/boot. This section is required for each master nodes.\nSecond, change the default password ubuntu for the default ubuntu user. Upgrade the OS:\nsudo apt update sudo apt upgrade\rCopy\rThird, change the hostname by running:\nsudo vi /etc/hostname sudo vi /etc/hosts\rCopy\rFourth, letting iptables to see bridged traffic\r:\n# Checks if br_netfilter is loaded lsmod | grep br_netfilter # Loads its explicitly sudo modprobe br_netfilter # Sees bridged traffic cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system\rCopy\rFifth, enable memory cgroup, by adding the following to /boot/firmware/cmdline.txt:\ncgroup=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1\rCopy\rFinally, add the following to /boot/firmware/usercfg.txt for disabling WiFi and Bluetooth\r:\ndtoverlay=disable-wifi dtoverlay=disable-bt # Memory group should be 1 after reboot sudo reboot grep mem /proc/cgroups | awk \u0026#39;{ print $4 }\u0026#39;\rCopy\rCreating Virtual IP (5 mins)\nFirst, install keepalived\rreferencing from LVS-NAT-Keepalived\rfor all master nodes.\n#Installs keepalived sudo apt install keepalived # Configures keepalived sudo vi /etc/keepalived/keepalived.conf\rCopy\r#VRRP Instances definitions #state MASTER for first master, BACKUP for other master nodes vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 150 authentication { auth_type PASS auth_pass seehiong } virtual_ipaddress { 192.168.100.200 } } # Virtual Servers definitions virtual_server 192.168.100.200 6443 { delay_loop 6 lb_algo rr lb_kind NAT protocol TCP real_server 192.168.100.119 6443 { weight 1 TCP_CHECK { connect_timeout 3 connect_port 6443 } } real_server 192.168.100.173 6443 { weight 1 TCP_CHECK { connect_timeout 3 connect_port 6443 } } real_server 192.168.100.100 6443 { weight 1 TCP_CHECK { connect_timeout 3 connect_port 6443 } } }\rCopy\r# Restarts keepalived sudo systemctl restart keepalived\rCopy\rSecond, test the connection, which will fail at this point in time:\nnc -v 192.168.100.200 6443 # Expected result nc: connect to 192.168.100.200 port 6443 (tcp) failed: Connection refused\rCopy\rPreparing certs for etcd (5 mins)\nFirst, by following openssl CA\r, configure openssl and create root cert:\nsudo su # Openssl configuration vi /usr/lib/ssl/openssl.cnf\rCopy\r[ CA_default ] dir = /root/ca mkdir /root/ca cd /root/ca mkdir newcerts certs crl private requests touch index.txt echo \u0026#39;1234\u0026#39; \u0026gt; serial # Root certificate openssl genrsa -aes256 -out private/cakey.pem 4096 openssl req -new -x509 -key private/cakey.pem -out cacert.pem -days 3650 -set_serial 0 -subj \u0026#39;/C=SG/ST=SG/O=seehiong/CN=master1\u0026#39;\rCopy\rSecond, create certs for all master nodes:\n# Create master nodes\u0026#39; certificate cd /root/ca/requests/ openssl genrsa -out etcd-key.pem openssl req -new -key etcd-key.pem -out etcd.csr -subj \u0026#39;/C=SG/ST=SG/O=seehiong/CN=master1,master2,master3,localhost,cluster-endpoint\u0026#39; openssl ca -in etcd.csr -out etcd.pem \\ -extfile \u0026lt;(printf \u0026#34;subjectAltName=IP:192.168.100.119,IP:192.168.100.173,IP:192.168.100.100,IP:192.168.100.200,IP:127.0.0.1,\\ DNS:master1,DNS:master2,DNS:master3,DNS:localhost,DNS:cluster-endpoint\u0026#34;) openssl genrsa -out peer-etcd-key.pem openssl req -new -key peer-etcd-key.pem -out peer-etcd.csr -subj \u0026#39;/C=SG/ST=SG/O=seehiong/CN=192.168.100.119,192.168.100.173,192.168.100.100,192.168.100.200\u0026#39; openssl ca -in peer-etcd.csr -out peer-etcd.pem \\ -extfile \u0026lt;(printf \u0026#34;subjectAltName=IP:192.168.100.119,IP:192.168.100.173,IP:192.168.100.100,IP:192.168.100.200,IP:127.0.0.1,\\ DNS:master1,DNS:master2,DNS:master3,DNS:localhost,DNS:cluster-endpoint\u0026#34;) rm etcd.csr peer-etcd.csr mv etcd-key.pem peer-etcd-key.pem /root/ca/private/ mv etcd.pem peer-etcd.pem /root/ca/certs/ # Protects /root/ca folder chmod -R 600 /root/ca\rCopy\rThird, copies all certs to /srv/etcd-certs/ for all master nodes.\n# Copies certs to master1 cp /root/ca/cacert.pem /root/ca/private/etcd-key.pem /root/ca/private/peer-etcd-key.pem \\ /root/ca/certs/etcd.pem /root/ca/certs/peer-etcd.pem /srv/etcd-certs/ # Copies certs to other master nodes scp /root/ca/cacert.pem /root/ca/private/etcd-key.pem /root/ca/private/peer-etcd-key.pem \\ /root/ca/certs/etcd.pem ‚Ä¶","date":"2020-08-09","permalink":"https://seehiong.github.io/posts/2020/08/ha-k8s-pi-cluster-i/","summary":"In this special guide, I celebrate Singapore\u0026rsquo;s 55th National Day by setting up a Highly Available Kubernetes Pi Cluster using 2x Raspberry Pi ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","HA","KeepedAlive","Etcd"],"title":"HA K8s Pi Cluster (I)"},{"content":"With your own Private Registry for Kubernetes Cluster, you can have full control over the docker registry and improve overall performance\nPrivate Registry on Kubernetes Cluster (Total Setup Time: 10 mins)\nDocker Registry\ris the official implementation for storing and distributing Docker images.\nPreparing Private Registry (5mins)\nFirst, create the self-signed certificate\r:\nmkdir -p certs openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \\ -x509 -days 365 -out certs/domain.crt\rCopy\rSecond, install the certificate in the master node and all of your leaf nodes:\nsudo mkdir -p /etc/docker/certs.d/myregistrydomain.com:5000 sudo cp certs/domain.crt /etc/docker/certs.d/myregistrydomain.com:5000/ca.crt sudo cp certs/domain.crt /usr/local/share/ca-certificates/myregistrydomain.com.crt sudo update-ca-certificates sudo systemctl restart docker\rCopy\rThird, start and deploy registry\r:\ndocker run -d \\ --restart=always \\ --name registry \\ -v \u0026#34;$(pwd)\u0026#34;/certs:/certs \\ -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -p 443:443 \\ registry:2\rCopy\rUsing Private Registry (5 mins)\nFirst, following up with my previous Jenkins Maven Agents\r, I pushed them to the private registry:\ndocker tag seehiong/jenkins-agent:1.0 myregistrydomain.com/my-jenkins-agent docker push myregistrydomain.com/my-jenkins-agent docker pull myregistrydomain.com/my-jenkins-agent\rCopy\rSecond, navigate to Jenkins \u0026gt; Configure Clouds, and change the docker image to the private registry:\nDocker image: myregistrydomain.com/my-jenkins-agent\rCopy\rAnd that\u0026rsquo;s it, the Private Registry on Kubernetes Cluster is setup properly for subsequent usage\nTroubleshooting Get https://myregistrydomain.com/v2/: read: connection reset by peer For the leaf nodes to connect to myregistrydomain.com, you need to setup Hosts file:\nsudo vi /etc/hosts\rCopy\r192.168.100.100 myregistrydomain.com\rCopy\rGet https://myregistrydomain.com/v2/: x509: certificate signed by unknown authority For the docker version I am using, I need to trust the cert at the OS level:\nsudo cp certs/domain.crt /usr/local/share/ca-certificates/myregistrydomain.com.crt sudo update-ca-certificates sudo systemctl restart docker\rCopy\r","date":"2020-08-07","permalink":"https://seehiong.github.io/posts/2020/08/private-registry-for-k8s-cluster/","summary":"I\u0026rsquo;ve successfully set up my Private Registry for the Kubernetes Cluster on Raspberry Pi. With this, I have full control over the Docker ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Registry"],"title":"Private Registry for K8s Cluster"},{"content":"By running OpenWrt Router on Raspberry Pi 3, you will be able to customize the device to suit your application usage\nOpenWrt Router for Raspberry (Total Setup Time: 20 mins)\nIn this guide, I will config OpenWrt\ras router on Raspberry Pi 3. The Wireless LAN (wlan0) is connected to home network while the LAN interface (eth0) is connected to the Kubernetes Cluster.\nPreparing OpenWrt (10 mins)\nFirst, you may choose the Raspberry Pi image\rthat matches your Pi. This is the downloaded image\rfor my 4GB SD card on the Raspberry Pi 3 Model B.\nSecond, insert the SD card and boot up your Raspberry Pi. The default root password is empty.\nThird, from System \u0026gt; Administrator and change your router password.\nInstalling OpenWrt (5 mins)\nFirst, navigate to System \u0026gt; LED configuration and customize the Raspberry Pi LED to your preferences.\nSecond, from Network \u0026gt; Wireless, click on the Scan button and Join Network.\nThird, moving on to Network \u0026gt; Interfaces, click on the Edit button for the LAN for configuring the interface.\nFourth, navigate to Network \u0026gt; DHCP and DNS \u0026gt; Static Leases, click on the Add button to add the Kubernetes master node.\nLastly, save all your settings and perform a reboot. You may go to System \u0026gt; Backup / Flash Firmware for a local backup copy.\nReconfiguring Kubernetes Cluster (5 mins)\nBecause of the change in IP address, all my previous Kubernetes cluster\rsettings are gone!\nFirst, with thanks to the post\rfrom valerius257\r, these are the steps for restoring:\nsystemctl stop kubelet docker cd /etc/ sudo mv kubernetes kubernetes-backup sudo mv /var/lib/kubelet /var/lib/kubelet-backup sudo mkdir -p kubernetes sudo cp -r kubernetes-backup/pki kubernetes sudo rm kubernetes/pki/{apiserver.*,etcd/peer.*} systemctl start docker sudo kubeadm init --ignore-preflight-errors=DirAvailable--var-lib-etcd\rCopy\rSecond, start using the cluster by following the commands:\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # Join each worker nodes as root sudo kubeadm join 192.168.100.100:6443 --token iu0gjx.lc07ou5eruk5d5sf \\ --discovery-token-ca-cert-hash sha256:752ee2428cc151770a4f7cf45a9595f8608440988f853bd4ee6c0a1a174b310b\rCopy\rThird, re-initializes all previous worker nodes.\n# Performs a reset on each node sudo kubeadm reset # Re-joins the master node on each node sudo kubeadm join 192.168.100.100:6443 --token 6o88ge.uxq1g9hk4kmd9rhx \\ --discovery-token-ca-cert-hash sha256:1dc568d05f2be9260091f758207f236b87282f55ee72b3cb09dfb1639ee28c4c # From master node, checks if the worker node joins successfully kubectl get nodes\rCopy\rBecause 1 node(s) had taint {node-role.kubernetes.io/master: }, my Jenkins\rfails to work. This error is fixed by running this command:\nkubectl taint nodes master1k8s node-role.kubernetes.io/master:NoSchedule\rCopy\rTroubleshooting Missing Wireless Menu for OpenWrt on Raspberry Pi 2 This is the downloaded image\rfor my Raspberry Pi 2 Model B+. I tried using my USB Wi-Pi (OYR-COMFAST88) and this /etc/config/wireless setting, but failed.\nconfig wifi-device \u0026#39;radio0\u0026#39; option type \u0026#39;mac80211\u0026#39; option channel \u0026#39;11\u0026#39; option hwmode \u0026#39;11g\u0026#39; option path \u0026#39;platform/soc/3f300000.mmc/mmc_host/mmc1/mmc1:0001/mmc1:0001:1\u0026#39; option htmode \u0026#39;HT20\u0026#39; option disabled \u0026#39;1\u0026#39; config wifi-iface \u0026#39;default_radio0\u0026#39; option device \u0026#39;radio0\u0026#39; option network \u0026#39;lan\u0026#39; option mode \u0026#39;ap\u0026#39; option ssid \u0026#39;OpenWrt\u0026#39; option encryption \u0026#39;none\u0026#39;\rCopy\r","date":"2020-08-02","permalink":"https://seehiong.github.io/posts/2020/08/openwrt-router-on-pi-3/","summary":"I\u0026rsquo;ve successfully configured OpenWrt Router on my Raspberry Pi 3, enhancing customization for my application needs. The process involves ‚Ä¶","tags":["Raspberry Pi","OpenWrt","Router"],"title":"OpenWrt Router on Pi 3"},{"content":"By creating Jenkins Maven Agent for Kubernetes Cluster, you can improve build time of your maven builds\nJenkins Maven Agent on Kubernetes (Total Setup Time: 15 mins)\nFollowing up on the previous post\r, I will create a Jenkins Maven Agent for Kubernetes. By configuring a local maven m2 repository, you can save previous time on your builds.\nConfiguring Jenkins (1 min)\nFirst, navigate to Jenkins \u0026gt; Configure Clouds, and click on the Add Pod Template. I named the pod template as maven, with its usage set as:\nNote\rOnly build jobs with label expressions matching this node\rSecond, add a container with the following settings:\nName: jnlp\rDcoker image: docjoube/jenkins-agent:1.0\rworking directory: /home/jenkins/agent\rCopy\rThird, add a volume, with the following persistent volume claim:\nClaim Name: maven-agent-pvc\rMount path: /home/jenkins/.m2\rCopy\rFourth, insert the maven volume claims to the maven-pv.yaml:\napiVersion: v1 kind: PersistentVolume metadata: name: maven-agent-pv labels: type: local spec: storageClassName: manual capacity: storage: 2Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/mnt/hdd/master1k8s/app/maven/data\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: maven-agent-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 2Gi\rCopy\rLastly, apply the maven-agent persistent volume and claims:\n# Create the persistent volume and claim kubectl apply -f maven-pv.yaml # Create the data folder and change ownership sudo mkdir -p /mnt/hdd/master1k8s/app/maven/data sudo chown -R 1000:1000 /mnt/hdd/master1k8s/app/maven/data\rCopy\rTesting Jenkins Maven Agent (14 mins)\nFirst, modify Jenkinsfile to use maven agent and commit the change. You may refer to kubernetes plugin\rfor more examples.\npipeline { agent none stages { stage(\u0026#39;Build\u0026#39;) { agent { label \u0026#39;maven\u0026#39; } steps { checkout scm sh \u0026#39;./mvnw -DskipTests clean package\u0026#39; } } } }\rCopy\rSecond, Jenkins automatically triggers a build.\nYou may notice that Jenkins master provisions a new maven-agent for this build.\nLast, this is the build console output:\nFor subsequent builds, maven-agent uses the local maven m2 repository and hence significantly reducing the build time.\nTroubleshooting persistentvolumeclaim \u0026ldquo;maven-agent-pvc\u0026rdquo; not found When the maven persistent volume is not created before any maven build, this error will occur. You can clear this error by creating the claim first.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling \u0026lt;unknown\u0026gt; default-scheduler persistentvolumeclaim \u0026#34;maven-agent-pvc\u0026#34; not found Warning FailedScheduling \u0026lt;unknown\u0026gt; default-scheduler persistentvolumeclaim \u0026#34;maven-agent-pvc\u0026#34; not found\rCopy\rNo such file or directory Because Jenkins user needs access rights to the maven persistent volume mount path, remember to change the folder ownership.\nsudo chown -R 1000:1000 /mnt/hdd/master1k8s/app/maven/data\rCopy\rWaiting for next available executor You can fix this error by following closely to this console output and removing the default values (/bin/sh -c and cat) from Jenkins \u0026gt; Configure Clouds.\n","date":"2020-07-31","permalink":"https://seehiong.github.io/posts/2020/07/jenkins-maven-agent/","summary":"I\u0026rsquo;ve successfully created a Jenkins Maven Agent for my Kubernetes Cluster, significantly improving Maven build times. Configuring Jenkins ‚Ä¶","tags":["Raspberry Pi","K8s","Jenkins","Maven"],"title":"Jenkins Maven Agent"},{"content":"With Jenkins Pipeline for Kubernetes Cluster, you can create a continuous integration environment for your project\nJenkins Pipeline on Kubernetes Cluster (Total Setup Time: 10 mins)\nIn this guide, I will create my own declarative Jenkins pipeline\r. With this, I can build, package and run my Spring Boot\rHello-World application.\nConfiguring Jenkins and Gitea (1 min)\nFirst, by following my previous post\r, I will install Git.\n# Get the Jenkins pod kubectl get pods # Get a shell in container kubectl exec --stdin --tty jenkins-7c5ffc6f55-c8fmj -- /bin/bash # Search for Git which git # Install Git if not present apt-get install git\rCopy\rSecond, navigate to Jenkins \u0026gt; Global Tool Configuration. Check if Git setup correctly.\nThird, navigate to Gitea \u0026gt; Site Administration Tool Configuration. Create a new organization.\nTesting Jenkins Pipeline (9 mins)\nWith the Gitea webhooks in place, Jenkins master will trigger a build on each Git commit. You may configure Jenkins to send email alert when the build fails.\nFirst, this is my Jenkinsfile:\npipeline { agent any stages { stage(\u0026#39;Checkout\u0026#39;) { steps { checkout scm } } stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;./mvnw clean verify\u0026#39; } } } }\rCopy\rSecond, when Apache Maven\rbuilds, it downloads all dependency jars.\nLast, this is my final build success result:\nThat\u0026rsquo;s all to it! My Jenkins Pipeline for Kubernetes Cluster works in sync with each Git commits. You may follow my next guide\rto improve on the maven build time.\nTroubleshooting There\u0026rsquo;s no such executable git in PATH You can clear this error by installing Git in the Jenkins pod.\nFor a permanent solution, re-build the jenkins docker image from the previous post\r. This is the modified Dockerfile:\nFROM balenalib/raspberrypi4-64-debian-openjdk:11-bullseye ENV JENKINS_HOME /var/jenkins_home ENV JENKINS_SLAVE_AGENT_PORT 50000 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends curl git RUN curl -fL -o /opt/jenkins.war http://updates.jenkins-ci.org/download/war/2.235.3/jenkins.war VOLUME ${JENKINS_HOME} WORKDIR ${JENKINS_HOME} EXPOSE 8080 ${JENKINS_SLAVE_AGENT_PORT} CMD [\u0026#34;/bin/bash\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;java -jar /opt/jenkins.war\u0026#34;]\rCopy\rBuild the image and tag it:\ndocker build -t seehiong/jenkins:2.0 .\rCopy\r[Gitea] Notifying branch build status: PENDING Build started This error happens when Jenkins cannot notify Gitea. You can fix this error by creating a new organization at Gitea.\nJenkins pipeline notifies Gitea on each build status:\n./mvnw: Permission denied My initial build failed because of the file permission issue.\n+ ./mvnw clean verify /var/jenkins_home/workspace/seehiong_hello-world_master@tmp/durable-c269e2ef/script.sh: 1: ./mvnw: Permission denied\rCopy\rYou can fix this issue by changing mode for mvnw and push the changes to gitea:\nchmod +x mvnw git update-index --chmod=+x mvnw\rCopy\r","date":"2020-07-31","permalink":"https://seehiong.github.io/posts/2020/07/jenkins-pipeline-for-k8s-cluster/","summary":"I effortlessly set up a Jenkins Pipeline for my Kubernetes Cluster, enabling seamless continuous integration for my projects. Following my previous ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Jenkins","Gitea"],"title":"Jenkins Pipeline for K8s Cluster"},{"content":"By integrating Jenkins and Gitea with webhook, you have full control over your own self-hosted continuous integration (CI) environment\nIntegration between Jenkins and Gitea (Total Setup Time: 8 mins)\nFollowing up with my previous posts, I will integrate Jenkins\rand Gitea\rin this guide.\nInstalling Gitea Plugin on Jenkins (2 mins)\nNavigate to Manage Jenkins \u0026gt; Manage Plugins, click on the Available tab. Search for Gitea, check on it and start installation.\nNext, restart Jenkins upon installation completes.\nConfiguring Gitea (2 mins)\nFirstly, following the initial setup\r, register a new Jenkins user.\nSecondly, create a new Hello-World repository under seehiong account.\nThirdly, add the newly created Jenkins user as the Collaborator.\nConfiguring Jenkins (2 mins)\nFirst, navigate to Manage Jenkins \u0026gt; Configure System, enter the following at the Gitea Servers section:\nName: Gitea\rServer URL: http://192.168.100.51:30080/\rCopy\rSecond, select on the Gitea Organization, and enter the item name as seehiong:\nThird, select on the Gitea server and add seehiong (Folder Credentials Provider):\nLast, enter owner as seehiong and save configuration:\nJenkins will start the scan and this is the sample result:\nOnce the Jenkinsfile is added to the repository, my repo appears under the status selection:\nConfiguring and Testing Webhook in Gitea (Optional) (2 mins)\nIn this section, I will add Webhook to Jenkins so that it will automatically trigger a build each time we commits to the repo.\nFor my setup, the target URL is set as follows:\nFirst, save the Webhook:\nSecond, click on the Test Delivery button to test the connection:\nThird, after a while you will see that the Test Deliver is success:\nFinally, I complete the setup for integrating Jenkins and Gitea. In the next post\r, I will describe on the CI workflow for our Spring Boot application. Stay tuned!\n","date":"2020-07-26","permalink":"https://seehiong.github.io/posts/2020/07/integrating-jenkins-and-gitea/","summary":"I seamlessly integrated Jenkins and Gitea, establishing full control over my self-hosted continuous integration (CI) environment. I installed the ‚Ä¶","tags":["Raspberry Pi","Jenkins","Gitea"],"title":"Integrating Jenkins and Gitea"},{"content":"With Helm as the package manager for Kubernetes Cluster on Raspberry, you can find, share and and use software built for Kubernetes\nHelm on Kubernetes Cluster (Total Setup Time: 10 mins)\nHelm\ris the package manager for Kubernetes. In this guide, I will install helm and setup ingress nginx controller with metallb as the layer 2 load balancer.\nInstalling Helm (3 mins)\nBecause I am installing Helm for Kubernetes Cluster runnning on a mixture of Raspberry Pi 4 and 3, I will download the Helm linux arm64\rvariant:\nwget https://get.helm.sh/helm-v3.3.0-rc.1-linux-arm64.tar.gz tar -xzvf helm-v3.3.0-rc.1-linux-arm64.tar.gz sudo mv linux-arm64/helm /usr/local/bin/helm\rCopy\rAfer Helm 3 is setup, you may check the version:\nhelm version # Helm output version.BuildInfo{Version:\u0026#34;v3.3.0-rc.1\u0026#34;, GitCommit:\u0026#34;5c2dfaad847df2ac8f289d278186d048f446c70c\u0026#34;, GitTreeState:\u0026#34;dirty\u0026#34;, GoVersion:\u0026#34;go1.14.4\u0026#34;}\rCopy\rInstalling NGINX Ingress Controller (1 min)\nFor external to access our Kubernetes cluster, I will install NGINX Ingress\rusing helm.\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm install my-nginx ingress-nginx/ingress-nginx\rCopy\rSetting up Load Balancer (1 min)\nMetallB\rhooks into our Kubernetes cluster and I will use it as a layer 2 network load-balancer.\nsudo mkdir -p /mnt/hdd/master1k8s/app/metallb cd /mnt/hdd/master1k8s/app/metallb wget https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml -O metallb-namespace.yaml kubectl apply -f metallb-namespace.yaml wget https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml -O metallb.yaml kubectl apply -f metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026#34;$(openssl rand -base64 128)\u0026#34;\rCopy\rThis is my setting for the layer 2\rconfiguration:\nsudo vi metallb-config.yaml\rCopy\rapiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.100.200-192.168.100.250\rCopy\rkubectl apply -f metallb-config.yaml\rCopy\rAfter the metallb is setup, I am able to get external IP addresses:\nkubectl --namespace default get services -o wide -w my-nginx-ingress-nginx-controller\rCopy\rThis is the sample result:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR my-nginx-ingress-nginx-controller LoadBalancer 10.104.36.43 192.168.100.200 80:31710/TCP,443:30063/TCP 11h app.kubernetes.io/component=controller,app.kubernetes.io/instance=my-nginx,app.kubernetes.io/name=ingress-nginx\rCopy\rRe-visiting Gitea setup (5 mins)\nWith the metallb usage\r, I change my Gitea service (you may refer to the previous Gitea post\r) to the following:\napiVersion: v1 kind: Service metadata: name: gitea annotations: metallb.universe.tf/allow-shared-ip: home-net spec: ports: - port: 3000 targetPort: 3000 name: gitea-http - port: 2222 targetPort: 2222 nodePort: 32222 name: gitea-ssh selector: app: gitea type: LoadBalancer loadBalancerIP: 192.168.100.250\rCopy\rIn this guide, I complete the Helm for Kubernetes Cluster setup and demonstrate how easy this package manager is, for my existing Kubernetes Cluster on Raspberry Pi.\n","date":"2020-07-24","permalink":"https://seehiong.github.io/posts/2020/07/helm-for-k8s-cluster/","summary":"I\u0026rsquo;ve successfully set up Helm as the package manager for my Raspberry Pi Kubernetes Cluster, enabling easy discovery, sharing, and use of ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Helm","Nginx","NIC","Metallb"],"title":"Helm for K8s Cluster"},{"content":"With Jenkins Agent for Kubernetes Cluster, you can orchestrate your build, test and scale deployment pipelines automatically\nJenkins on Kubernetes Cluster (Part II) (Total Setup Time: 25 mins)\nIn this guide, I will setup Jenkins agents for previous post\r. With these agents, you can expand your Kubernetes cluster capabilities to handle additional loads.\nPreparation (15 mins)\nFirst, you need to create the jenkins agent docker image for raspberry Pi cluster. You may start by creating a new Dockerfile and insert the following:\n# Source # https://hub.docker.com/r/jenkins/slave/dockerfile FROM balenalib/raspberrypi4-64-debian-openjdk:11-bullseye ARG VERSION=4.7 ARG user=jenkins ARG group=jenkins ARG uid=1000 ARG gid=1000 # Modified for debian-bullseye RUN groupadd -g ${gid} ${group} RUN useradd -d /home/${user} -u ${uid} -g ${group} -m ${user} LABEL Description=\u0026#34;This is a base image, which provides the Jenkins agent executable (slave.jar)\u0026#34; Vendor=\u0026#34;Jenkins project\u0026#34; Version=\u0026#34;${VERSION}\u0026#34; ARG AGENT_WORKDIR=/home/${user}/agent # Modified for debian-bullseye RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl bash git git-lfs openssh-client openssl procps \\ \u0026amp;\u0026amp; curl --create-dirs -fsSLo /usr/share/jenkins/agent.jar https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/${VERSION}/remoting-${VERSION}.jar \\ \u0026amp;\u0026amp; chmod 755 /usr/share/jenkins \\ \u0026amp;\u0026amp; chmod 644 /usr/share/jenkins/agent.jar \\ \u0026amp;\u0026amp; ln -sf /usr/share/jenkins/agent.jar /usr/share/jenkins/slave.jar \\ \u0026amp;\u0026amp; apt-get remove -y curl USER ${user} ENV AGENT_WORKDIR=${AGENT_WORKDIR} RUN mkdir /home/${user}/.jenkins \u0026amp;\u0026amp; mkdir -p ${AGENT_WORKDIR} VOLUME /home/${user}/.jenkins VOLUME ${AGENT_WORKDIR} WORKDIR /home/${user} # Source # https://hub.docker.com/r/jenkins/inbound-agent/dockerfile USER ${user} COPY jenkins-agent /usr/share/jenkins/jenkins-agent USER root COPY jenkins-agent /usr/local/bin/jenkins-agent RUN chmod +x /usr/local/bin/jenkins-agent \\ \u0026amp;\u0026amp; ln -s /usr/local/bin/jenkins-agent /usr/local/bin/jenkins-slave USER ${user} ENTRYPOINT [\u0026#34;jenkins-agent\u0026#34;]\rCopy\rSecond, create a local copy of jenkins-agent\rfile. The above COPY command will copy it into the docker image.\nThird, build\rthe jenkins-agent image and tag it:\ndocker build -t seehiong/jenkins-agent:1.0 .\rCopy\rVerify the built status by running docker images:\n# REPOSITORY TAG IMAGE ID CREATED SIZE # seehiong/jenkins-agent 1.0 3e31e8033460 2 minutes ago 432MB\rCopy\rTo check the java version, use docker run\r:\ndocker run -it seehiong/jenkins-agent:1.0 /bin/bash java -version\rCopy\rLastly, we are ready to configure Jenkins for Kubernetes Cluster for scalability, with this Jenkins agent image!\nConfiguring Jenkins Agent for Kubernetes Cluster (5 mins)\nFirst, log into Jenkins and navigate to Jenkins \u0026gt; Configure Global Security. Set TCP port for inbound agents to the defualt port 50000. Since remoting 3.0 is using JNLP4-connect\r, verify that the Inbound TCP Agent Protocol/4 option is marked.\nSecond, navigate to Jenkins \u0026gt; Configure Clouds, add a new cloud and select Kubernetes. You may test the connection by clicking on the Test Connection button.\nNext, check that the Jenkins URL and tunnel is set correctly. You can find the IP address by running this:\nkubectl get svc # Sample output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE jenkins NodePort 10.99.201.197 \u0026lt;none\u0026gt; 8080:30080/TCP,50000:32502/TCP 8d\rCopy\rThis is the my cloud configuration:\nThird, click on the pod templates to add a pod template. Next, click on the add container to add a new jnlp container. This is my pod template setting:\nCreating the First Jenkins Job (5 mins)\nFirst, click on the New Item, enter item name as First Job and select Freestyle project. From the Build tab, enter any abitrary command. I use sleep 1m to execute in shell. Click on the save button.\nSecond, following the above steps, I created 3 jobs, to sleep for 1 minute, 45 seconds and 30 seconds respectively.\nLastlty, from the main page, you can click on the schedule a build for each job. You will notice that the jenkins-agents are initially suspended:\nAfter some pod initialization processes, it will execute the jobs, similar to this:\nYou may check the logs from jenkins master by running:\n# Get pod name kubectl get pods kubectl logs jenkins-7c5ffc6f55-c8fmj\rCopy\rThat\u0026rsquo;s all folks! You have configured a working Jenkins on Kubernetes Cluster with scalability capabilities. Stay tuned to our subsequent posts on configuring Jenkins pipeline for our builds.\nTroubleshooting Nodes stay suspended The important point to note is that, as opposed to many site on the internet, the container name must be named as jnlp. Even though it is stated correctly here\r, the screen shot on the page itself states otherwise.\n","date":"2020-07-19","permalink":"https://seehiong.github.io/posts/2020/07/jenkins-for-k8s-cluster-ii/","summary":"I\u0026rsquo;ve successfully set up Jenkins Agents for my Kubernetes Cluster, enhancing its capabilities for automated build, test, and scalable deployment ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Jenkins"],"title":"Jenkins for K8s Cluster (II)"},{"content":"With Jenkins for Kubernetes Cluster, you can orchestrate your build, test and deployment pipelines\nJenkins on Kubernetes Cluster (Part I) (Total Setup Time: 40 mins)\nJenkins\ris the leading open source automation server. It provides hundreds of plugins for supporting the building, deploying and automating of any project.\nPreparation (25 min)\nIn this guide, I am going to build the docker image for Jenkins on Kubernetes Cluster on Pi\r. You may use the base image\rfrom Balena\rin any Docker environment. In addition, you can find more details about the Balena base images here\r.\nFirst, create a /mnt/hdd/master1k8s/docker/Dockerfile and insert the following. If you are interested in the Jenkins war file, please visit here\r:\nFROM balenalib/raspberrypi4-64-debian-openjdk:11-bullseye ENV JENKINS_HOME /var/jenkins_home ENV JENKINS_SLAVE_AGENT_PORT 50000 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends curl RUN curl -fL -o /opt/jenkins.war http://updates.jenkins-ci.org/download/war/2.235.2/jenkins.war VOLUME ${JENKINS_HOME} WORKDIR ${JENKINS_HOME} EXPOSE 8080 ${JENKINS_SLAVE_AGENT_PORT} CMD [\u0026#34;/bin/bash\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;java -jar /opt/jenkins.war\u0026#34;]\rCopy\rSecond, build\rthe image and tag it:\ndocker build -t seehiong/jenkins:1.0 .\rCopy\rYou may verify the built status by running docker images:\n# REPOSITORY TAG IMAGE ID CREATED SIZE # seehiong/jenkins 1.0 9f4f4930275a 1 hours ago 506MB\rCopy\rLast, with this Jenkins image created, we are ready to deploy Jenkins for Kubernetes Cluster.\nInstalling Jenkins (15 mins)\nAs usual, create the required folders and kubernetes deployment file:\nsudo mkdir -p /mnt/hdd/master1k8s/app/jenkins/home sudo vi /mnt/hdd/master1k8s/app/jenkins/jenkins-deployment.yaml\rCopy\rFirst, insert the following into jenkins-deployment.yaml:\napiVersion: v1 kind: Service metadata: name: jenkins spec: ports: - port: 8080 name: jenkins-http targetPort: 8080 nodePort: 30080 - port: 50000 name: jenkins-slave targetPort: 50000 selector: app: jenkins type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: jenkins spec: replicas: 1 selector: matchLabels: app: jenkins strategy: type: Recreate template: metadata: labels: app: jenkins spec: containers: - name: jenkins image: seehiong/jenkins:1.0 ports: - containerPort: 8080 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home volumes: - name: jenkins-home hostPath: path: /mnt/hdd/master1k8s/app/jenkins/home type: Directory nodeSelector: jenkins-data-storage: \u0026#34;true\u0026#34;\rCopy\rSecond, apply the Jenkins deployment to the Kubernetes cluster and label the node:\nkubectl apply -f /mnt/hdd/master1k8s/app/jenkins/jenkins-deployment.yaml kubectl label nodes master1k8s jenkins-data-storage=true\rCopy\rThird, verify if Jenkins is deployed successfully:\nkubectl describe deployment jenkins\rCopy\rThat\u0026rsquo;s all to it! You may access Jenkins from your eth0 address, nodePort 30080. You may install the recommended plugins:\nFinally, you may create the user and log in. Your Jenkins web page will look similar to this:\nYou may refer to the next post\ron the detailed steps for setting up Jenkins Agents for Kubernetes.\nTroubleshooting exec user process caused \u0026ldquo;exec format error\u0026rdquo; Since my Kubernetes Cluster runs on raspberry Pi ARM architecture, most of the publicly available Genkins images failed to work. As a result, I decided to build the docker image by myself.\nError looking up service account default/jenkins: serviceaccount \u0026ldquo;jenkins\u0026rdquo; not found Because of missing serviceaccount, my Jenkins did not run:\nkubectl get rs kubectl describe rs jenkins-69f669b7c4\rCopy\rCreate a jenkins-rolebinding.yaml and insert the following:\napiVersion: v1 kind: ServiceAccount metadata: namespace: default name: jenkins --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: jenkins rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;create\u0026#34;,\u0026#34;delete\u0026#34;,\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods/exec\u0026#34;] verbs: [\u0026#34;create\u0026#34;,\u0026#34;delete\u0026#34;,\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods/log\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: namespace: default name: jenkins roleRef: kind: Role name: jenkins apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: jenkins\rCopy\r","date":"2020-07-12","permalink":"https://seehiong.github.io/posts/2020/07/jenkins-for-k8s-cluster-i/","summary":"I\u0026rsquo;ve successfully set up Jenkins on my Kubernetes Cluster, streamlining build, test, and deployment pipelines. Following a Docker image creation ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Jenkins"],"title":"Jenkins for K8s Cluster (I)"},{"content":"Having Gitea on Kubernetes Pi cluster, you will have full control over your personal Git repositories\nGitea for Kubernetes Cluster on Pi (Total Setup Time: 40 mins)\nSimilar to the previous post on Gitea for MicroK8s Cluster\r, I will be setting up Git in the newly created Kubernetes Cluster\r.\nSetup MySQL (15 mins)\nFirst, I download the mysql-server docker\rimage:\ndocker pull mysql/mysql-server:latest docker images\rCopy\rSecond, with reference from here\r, I created the required path on my external HDD:\nsudo mkdir -p /mnt/hdd/master1k8s/app/mysql/data sudo vi /mnt/hdd/master1k8s/app/mysql/mysql-pv.yaml\rCopy\rAdd the following to mysql-pv.yaml file:\napiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv labels: type: local spec: storageClassName: manual capacity: storage: 20Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/mnt/hdd/master1k8s/app/mysql/data\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 20Gi\rCopy\rNext, apply the above yaml file to create mysql persistent volumes\rwith this command:\nkubectl apply -f /mnt/hdd/master1k8s/app/mysql/mysql-pv.yaml\rCopy\rThird, add the following to /mnt/hdd/master1k8s/app/mysql/mysql-deployment.yaml:\napiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql/mysql-server:latest imagePullPolicy: IfNotPresent name: mysql env: # Use secret in real usage - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pvc\rCopy\rNext, apply the above yaml file to create mysql service and deployment:\nkubectl apply -f /mnt/hdd/master1k8s/app/mysql/mysql-deployment.yaml\rCopy\rLastly, you may use the following to check on your deployment:\nkubectl describe deployment mysql kubectl get pods kubectl logs mysql-56c7cb9d76-w4snt\rCopy\rPreparing MySQL (5 mins)\nTo prepare mysql for Gitea, I need to get the pod name:\nkubectl get pods\rCopy\rFirst, ssh into mysql pod:\nkubectl exec -it mysql-56c7cb9d76-w4snt -- /bin/bash\rCopy\rSecond, connect to mysql (default is password):\nmysql -u root -p\rCopy\rLast, prepare the database\r(user=gitea, password=gitea, database=giteadb):\nCREATE USER \u0026#39;gitea\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;gitea\u0026#39;; CREATE DATABASE giteadb; GRANT ALL PRIVILEGES ON giteadb.* TO \u0026#39;gitea\u0026#39;; FLUSH PRIVILEGES;\rCopy\rPreparing Gitea with Docker (15 mins)\nYou may start to install Gitea with docker by preparing the docker-compose.yml:\nmkdir -p /mnt/hdd/master1k8s/docker sudo vi /mnt/hdd/master1k8s/docker/docker-compose.yml\rCopy\rNext, with reference from here\r, add the following to docker-compose.yml:\nversion: \u0026#34;2\u0026#34; networks: gitea: external: false services: server: image: gitea/gitea:latest environment: - USER_UID=1000 - USER_GID=1000 - DB_TYPE=mysql - DB_HOST=mysql:3306 - DB_NAME=giteadb - DB_USER=gitea - DB_PASSWD=gitea restart: always networks: - gitea volumes: - ./gitea:/data - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro ports: - \u0026#34;3000:3000\u0026#34; - \u0026#34;222:22\u0026#34;\rCopy\rYou will get Gitea running with these docker-compose\rcommands:\n# Builds, creates, starts containers docker-compose up -d # Lists containers docker-compose ps # Displays log output docker-compose logs # Stops, removes containers docker-compose down\rCopy\rServing Gitea (5 mins)\nTo access Gitea on Kubernetes Pi Cluster from my laptop, I need to expose it to external using NodePort\r:\nsudo mkdir -p /mnt/hdd/master1k8s/app/gitea/data sudo vi /mnt/hdd/master1k8s/app/gitea/gitea-deployment.yaml\rCopy\rNext, insert the following into gitea-deployment.yaml:\napiVersion: v1 kind: Service metadata: name: gitea spec: ports: - port: 3000 targetPort: 3000 nodePort: 30000 name: gitea-http - port: 2222 targetPort: 2222 nodePort: 32222 name: gitea-ssh selector: app: gitea type: NodePort --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: gitea spec: replicas: 1 selector: matchLabels: app: gitea strategy: type: Recreate template: metadata: labels: app: gitea spec: containers: - image: gitea/gitea:latest imagePullPolicy: IfNotPresent name: gitea ports: - containerPort: 3000 name: gitea-http - containerPort: 22 name: gitea-ssh volumeMounts: - name: gitea-data mountPath: /data volumes: - name: gitea-data hostPath: path: /mnt/hdd/master1k8s/app/gitea type: Directory nodeSelector: git-data-storage: \u0026#34;true\u0026#34;\rCopy\rApply the above yaml file to create Gitea service and deployment:\nkubectl apply -f /mnt/hdd/master1k8s/app/gitea/gitea-deployment.yaml\rCopy\rSince ‚Ä¶","date":"2020-07-10","permalink":"https://seehiong.github.io/posts/2020/07/gitea-for-k8s-cluster/","summary":"In my recent endeavor, I spent 40 minutes setting up Gitea on my Kubernetes Pi cluster, granting me absolute control over personal Git repositories. I ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","MicroK8s","Gitea","MySQL","Docker"],"title":"Gitea for K8s Cluster"},{"content":"With Kubernetes Cluster on Raspberry Pi, you may orchestrate and manage your Docker containers with full control\nKubernetes Cluster (Total Setup Time: 70 mins)\nSimilar to the previous MicroK8s cluster\rsetup, I am using Ubuntu Server 20.04 LTS (64-bit)\ras my OS. Having a Kubernetes Cluster on Raspberry Pi, you will have more control over how the cluster configured.\nAfter burning the image onto my 64GB SD card, an empty file named ssh is created at d:/boot for a headless setup.\nPowering Up (5 mins)\nAfter powering up the Pi, update the OS to the latest by:\nsudo apt update sudo apt upgrade\rCopy\rChange its hostname by running:\nsudo nano /etc/hostname sudo nano /etc/hosts\rCopy\rEnable memory cgroup, by opening the file:\nsudo nano /boot/firmware/cmdline.txt\rCopy\rNext, add the following at the end of the line:\ncgroup=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1\rCopy\rAdd the following /boot/firmware/usercfg.txt to disable WiFi and Bluetooth\r:\ndtoverlay=disable-wifi dtoverlay=disable-bt\rCopy\rFor iptables to see bridged traffic, perform these:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system\rCopy\rPerform a reboot:\nsudo reboot\rCopy\rInstalling Docker (5 mins)\nFirst, referencing the previous post on external storage\r, storage setup are summarized in these steps:\n# Get UUID of external HDD sudo blkid # Create the required folder sudo mkdir /mnt/hdd sudo nano /etc/fstab # Add line to /etc/fstab UUID=b89ca96d-5ff9-403e-8c7c-82a1a49e5f0d /mnt/hdd ext4 defaults 0 1 # Test mount without reboot sudo mount -a\rCopy\rSecond, from the previous preparation section on docker\rand gitea\r, these commands set docker up:\n# Install docker-compose sudo apt install docker-compose # Check version docker-compose version # Add ubuntu user to docker group sudo usermod -aG docker ubuntu sudo mkdir -p /mnt/hdd/master1k8s/docker/images\rCopy\rThird, add the \u0026ndash;data-root option to the ExecStart:\nsudo nano /lib/systemd/system/docker.service\rCopy\rFourth, add the \u0026ndash;data-root option to the ExecStart, similar to this:\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --data-root=/mnt/hdd/master1k8s/docker/images\rCopy\rLast, to setup docker daemon\r, run this command:\n# Run su commnad as root sudo su - # Set up the Docker daemon cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } EOF # Restart Docker systemctl daemon-reload systemctl restart docker systemctl enable docker\rCopy\rInstalling kudeadm (25 mins)\nAdd Kubernetes repository:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF\rCopy\rNext, install kubeadm\rand perform a reboot:\nsudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl sudo reboot\rCopy\rCreating Kubernetes Cluster (20 mins)\nCreate the kubernetes cluster by running:\n# Pass the container runtime sockets: sudo kubeadm init --control-plane-endpoint master1k8s --service-cidr 10.96.0.0/12 # Reset the cluster creation when things went wrong and try init again sudo kubeadm reset\rCopy\r# [init] Using Kubernetes version: v1.18.5 # [preflight] Running pre-flight checks # [preflight] Pulling images required for setting up a Kubernetes cluster # [preflight] This might take a minute or two, depending on the speed of your internet connection # [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; # [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; # [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; # [kubelet-start] Starting the kubelet # [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; # [certs] Generating \u0026#34;ca\u0026#34; certificate and key # [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key # [certs] apiserver serving cert is signed for DNS names [master1k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master1k8s] and IPs [10.96.0.1 192.168.100.51] # [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key # [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key # [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key # [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key # [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key # [certs] etcd/server serving cert is signed for DNS names [master1k8s localhost] and IPs [192.168.100.51 127.0.0.1 ::1] # ‚Ä¶","date":"2020-07-04","permalink":"https://seehiong.github.io/posts/2020/07/kubernetes-cluster-on-pi/","summary":"I recently spent 70 minutes setting up a Kubernetes Cluster on Raspberry Pi using Ubuntu Server 20.04 LTS. After burning the OS image and configuring ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","Docker","Kubeadm","Calio"],"title":"Kubernetes Cluster on Pi"},{"content":"With Gitea for Raspberry Pi cluster, you can have your own self-hosted Git Service\nGitea for Raspberry Pi (Total Setup Time: 45 mins)\nGitea\ris a painless self-hosted Git service. By hosting Gitea locally, our team is able to save cost and you also have more control over your server.\nPreparation (5 min)\nIf you are following my MicroK8s cluster\rsetup, for each kubectl command, you need to append with microk8s. With this section, you can simply use kubectl. First, install kubectl:\nsudo snap install kubectl --classic\rCopy\rSecond, save microk8s configuration to kubectl:\nmicrok8s kubectl config view --raw \u0026gt; ~/.kube/config\rCopy\rAnd you are done! You can continue to use either microk8s kubectl or simply kubectl:\nkubectl get nodes\rCopy\rThere are many ways to install docker-compose\r. By far, this is the most convenient way for my setup:\nsudo apt install docker-compose\rCopy\rWith this command, docker-compose \u0026ndash;version, I get:\ndocker-compose version 1.25.0, build unknown\rCopy\rSetup MySQL (15 mins)\nFirstly, I download the mysql-server docker\rimage:\ndocker pull mysql/mysql-server:latest docker images docker save mysql/mysql-server \u0026gt; mysql-server.tar\rCopy\rSecondly, inject the image into MicroK8s cache:\nmicrok8s ctr image import mysql-server.tar microk8s ctr images ls\rCopy\rThirdly, with reference from here\r, I created the required path on my external HDD:\nsudo mkdir -p /mnt/hdd/microk8s/application/mysql/data sudo nano /mnt/hdd/microk8s/application/mysql/mysql-pv.yaml\rCopy\rAdd the following to mysql-pv.yaml file:\napiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 20Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/mnt/hdd/microk8s/application/mysql/data\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 20Gi\rCopy\rNext, apply the above yaml file to create mysql persistent volumes\rwith this command:\nkubectl apply -f /mnt/hdd/microk8s/application/mysql/mysql-pv.yaml\rCopy\rFourthly, add the following to /mnt/hdd/microk8s/application/mysql/mysql-deployment.yaml:\napiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql/mysql-server:latest name: mysql env: # Use secret in real usage - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim\rCopy\rNext, apply the above yaml file to create mysql service and deployment:\nkubectl apply -f /mnt/hdd/microk8s/application/mysql/mysql-deployment.yaml\rCopy\rLastly, you may use the following to check on your deployment:\nkubectl describe deployment mysql kubectl delete deployment,svc mysql # to delete development and service, if needed kubectl delete pvc mysql-pv-claim # to delete PersistenceVolumeClaim, if needed kubectl delete pv mysql-pv-volume # to delete PersistenceVolume, if needed\rCopy\rPreparing MySQL (5 mins)\nTo prepare mysql for Gitea, I need to get the pod name:\nkubectl get pods\rCopy\rFirst, ssh into mysql pod:\nkubectl exec -it mysql-5f57bdc6c5-rbfwn -- /bin/bash\rCopy\rSecond, connect to mysql:\nmysql -u root -p\rCopy\rLastly, prepare the database\r:\nSET old_passwords=0; CREATE USER \u0026#39;gitea\u0026#39; IDENTIFIED BY \u0026#39;gitea\u0026#39;; -- create database user CREATE DATABASE giteadb CHARACTER SET \u0026#39;utf8mb4\u0026#39; COLLATE \u0026#39;utf8mb4_unicode_ci\u0026#39;; -- create database GRANT ALL PRIVILEGES ON giteadb.* TO \u0026#39;gitea\u0026#39;; -- grant privileges to database user FLUSH PRIVILEGES;\rCopy\rPreparing Gitea with Docker (15 mins)\nYou may start to install Gitea with docker by preparing the docker-compose.yml:\nmkdir -p /mnt/hdd/docker/deployment/gitea cd /mnt/hdd/docker/deployment touch docker-compose.yml\rCopy\rNext, with reference from here\r, add the following to docker-compose.yml:\nversion: \u0026#34;2\u0026#34; networks: gitea: external: false services: server: image: gitea/gitea:latest environment: - USER_UID=1000 - USER_GID=1000 - DB_TYPE=mysql - DB_HOST=mysql:3306 - DB_NAME=giteadb - DB_USER=gitea - DB_PASSWD=gitea\trestart: always networks: - gitea volumes: - ./gitea:/data - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro ports: - \u0026#34;3000:3000\u0026#34; - \u0026#34;222:22\u0026#34;\rCopy\rYou will get Gitea running with this command:\ndocker-compose up -d docker-compose ps # to show if Gitea is started docker-compose logs # to show Gitea logs docker-compose down # to shutdown Gitea\rCopy\rNext, similar to mysql, you need to inject this into MicroK8s cache:\ndocker images docker save gitea/gitea \u0026gt; gitea.tar ‚Ä¶","date":"2020-06-29","permalink":"https://seehiong.github.io/posts/2020/06/gitea-for-microk8s-cluster/","summary":"I dedicated 45 minutes to set up Gitea on my Raspberry Pi cluster using MicroK8s. Gitea, a self-hosted Git service, grants my team cost savings and ‚Ä¶","tags":["Raspberry Pi","Cluster","MicroK8s","Gitea","MySQL","Docker"],"title":"Gitea for MicroK8s Cluster"},{"content":"With Docker on Raspberry Pi cluster, you can run any containerized applications on your Pi Cluster\nDocker on Raspberry (Total Setup Time: 15 mins)\nTo install Docker\rfor Raspberry Pi Cluster, add ubuntu user to the docker group:\nsudo apt install docker.io sudo usermod -aG docker ubuntu su - ubuntu # open a new shell with updated membership for the user\rCopy\rConfiguring Docker (5 mins)\nFirstly, to configure docker daemon\rusing external storage, open this system file\r:\nsudo nano /lib/systemd/system/docker.service\rCopy\rNext, add the \u0026ndash;data-root option to the ExecStart, similar to this:\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --data-root=/mnt/hdd/docker/docker-data\rCopy\rYou may check and restart the docker status by:\nsystemctl status docker sudo systemctl stop docker sudo systemctl daemon-reload sudo systemctl start docker\rCopy\rFinally, this is my docker info:\nTest Docker (5 mins)\nYou may test the docker installation\rby running:\ndocker run hello-world\rCopy\rThis is the my first hello-world docker run:\nUse local images for MicroK8s (5 mins)\nThe hello-world image is only known to Docker and is invisible to MicroK8s. To export the previously built image, inject it to MicroK8s:\ndocker save hello-world \u0026gt; hello-world-image.tar microk8s ctr image import hello-world-image.tar\rCopy\rYou may search for the hello-world image with this command, microk8s ctr images ls. This is my result:\nTroubleshooting Job for docker.service failed because the control process exited with error code If your docker is unable to start, check on the details:\njournalctl -u docker\rCopy\rdocker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. On further checks with journalctl, I notice that there are some errors:\nfailed to start daemon: remote I/O error\rfailed to start daemon: error initialising graphdriver: driver not supported\rfailed to mount overlay; invalid argument storage-driver=overlay2\rCopy\rIt seems like for my case, the errors are cleared when this file is removed:\nsudo rm /etc/docker/daemon.json\rCopy\rstandard_init_linux.go:211: exec user process caused \u0026ldquo;permission denied\u0026rdquo; When docker runs with the above error, the error log shows:\nlevel=warning msg=\u0026#34;auplink flush failed: \u0026#34; error=\u0026#34;exec: \\\u0026#34;auplink\\\u0026#34;: executable file not found in $PATH\u0026#34; method=Unmount storage-driver=aufs\rCopy\rFollowing dockerd options\r, I tried adding below option to /lib/systemd/system/docker.service. But this fails too:\n--storage-driver=overlay2\rCopy\rFinally, I decided to format my existing ntfs HDD to ext4 using sudo mkfs.ext4 /dev/sda1. By adding the following to /etc/fstab:\nUUID=[UUID] /mnt/hdd ext4 defaults 0 2\rCopy\rVoila! Docker on Raspberry Pi Cluster works perfectly magically!\nWARNING: No swap limit support I removed this warning by editing /boot/firmware/cmdline.txt and adding the below option to the end of the line:\nswapaccount=1\rCopy\rThis is my final Raspberry Pi firmware boot-up setting:\nnet.ifnames=0 dwc_otg.lpm_enable=0 console=serial0,115200 console=tty1 root=LABEL=writable rootfstype=ext4 elevator=deadline rootwait fixrtc cgroup_enable=memory cgroup_memory=1 swapaccount=1\rCopy\r","date":"2020-06-21","permalink":"https://seehiong.github.io/posts/2020/06/docker-for-microk8s-cluster/","summary":"Setting up Docker on my Raspberry Pi Cluster took just 15 minutes. After installing Docker, I added the ubuntu user to the Docker group. Configuring ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","MicroK8s","Docker"],"title":"Docker for MicroK8s Cluster"},{"content":"GNU Health Embedded on Raspberry Pi 3, your own personal one-node EMR (Electronics Medical Record)\nGNU Health on Pi 3 (Total Setup Time: 45 mins)\nAs Singapore moves into its phase 2 of reopening from 19 Jun, personal hygiene and a healthy lifestyle is still of paramount importance in fighting the COVID-19 coronavirus pandemic\r.\nGNU Health embedded is a full server, runs on openSUSE\rand has its own database. It allows storing of information locally and has a demo DB to play with locally. Today, I would like to test drive the GNU Health Embedded on Raspberry Pi 3.\nPi OS Preparation (30 mins)\nThe GNU Health\rHMIS (Hospital Management Information System) and LIMS (Lab Information Management System) for Raspberry Pi 3 can be downloaded from here\r. After downloading the image, I write this to my 16GB SD card using Balena Etcher\r.\nPowering Up (5 mins)\nI attached USB keyboard and mouse, connected to an external monitor via HDMI. On boot up select openSUSE and the OS will load up. The default username is gnuhealth and the password is freedom.\nThis is how the openSUSE OS desktop looks like:\nGNU Health - The Free / Libre Hospital and Health Information System (10 mins)\nBy clicking on the GNU Health icon (located at the bottom left of the screen), the GNU Health HMIS login page is displayed. The default username is admin and the password is gnusolidario.\nIf you are already using GNU Health, this article\rmay come handy and provide some sort of insights for your day-to-day usage.\nSome usage examples are the Families screen, where you can add your family, company employees.\nFamily Members is the screen where you can add family members or employees to the family.\nTroubleshooting Configuring openSUSE System Settings If there is a need to access openSUSE OS, the default password for root user is test. You may refer to GNU Health on openSUSE\rfor more details.\n","date":"2020-06-21","permalink":"https://seehiong.github.io/posts/2020/06/gnu-health/","summary":"Exploring GNU Health Embedded on Raspberry Pi 3 for a personal Electronic Medical Record took me 45 minutes. Downloading the image from GNU Health, I ‚Ä¶","tags":["Raspberry Pi","GNU","Health"],"title":"GNU Health"},{"content":"Expand storage capacity by using external storage for your Raspberry Pi Cluster\nExternal Storage for Pi Cluster (Total Setup Time: 35 mins)\nWith the MicroK8s cluster\rin place, I decided to expand the External Storage for Raspberry Pi cluster. For this to work, I will use my external 640GB USB hard disk and configure MicroK8s default storage.\nMounting External Storage (5 mins)\nFirst, I plug my external 640GB USB hard disk into the Raspberry Pi 4 USB3 port (next to the LAN). For displaying information about the available block devices\r, issue this command and take note of the UUID:\nsudo blkid\rCopy\rSince I would like to mount the hard disk automatically to the /mnt/hdd folder, modify fstab\r:\nsudo mkdir /mnt/hdd sudo nano /etc/fstab\rCopy\rAs my external storage is a ntfs hard disk, replace the UUID and add the following to /etc/fstab:\nUUID=[UUID] /mnt/hdd ntfs defaults,auto,users,rw,nofail,umask=000,x-systemd.device-timeout=30 0 0\rCopy\rTo test mount\rthe new entry before a reboot, run this:\nsudo mount -a\rCopy\rAnd that\u0026rsquo;s it! After a reboot, the hard disk will be recognised by the Raspberry Pi.\nExpanding Storage for MicroK8s (25 mins)\nBecause of the errors I encountered, highlighted in the Troubleshooting section, I decided to perform a reset\rto my existing cluster:\nmicrok8s reset --destroy-storage\rCopy\rAfter preparing the required folders and I opened the fstab for edit:\nsudo mkdir -p /var/snap/microk8s/common/default-storage sudo mkdir -p /mnt/hdd/microk8s/common/default-storage sudo nano /etc/fstab\rCopy\rFor the external storage to perform a bind mount permanently, add these to the /etc/fstab:\n/mnt/hdd/microk8s/common/default-storage /var/snap/microk8s/common/default-storage none bind,user 0 2\rCopy\rAnd I re-enabled all the previous addons which I had before:\nmicrok8s enable dashboard dns registry\rCopy\rAgain, to access dashboard without a token, add \u0026ndash;enable-skip-login option to the spec section (microk8s edit uses the vim editor\r):\nsudo microk8s kubectl -n kube-system edit deploy kubernetes-dashboard -o yaml\rCopy\rspec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: kubernetes-dashboard spec: containers: - args: - --auto-generate-certificates - --namespace=kube-system - --enable-skip-login\rCopy\rFor serving the dashboard in the background (the skip button will appear only after a reboot or by restarting the MicroK8s):\nmicrok8s kubectl proxy --accept-hosts=.\\* --address=0.0.0.0 \u0026amp;\rCopy\rFinally, for viewing dashboard, hit the skip button:\nhttp://192.168.xx.xx:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\rCopy\rAdding leaf nodes for MicroK8s (5 mins)\nIn the previous post\r, each of the leaf nodes is added by running these:\n# on the master node sudo microk8s add-node # copy output and run on leaf node microk8s join 192.168.xx.xx:25000/TphlaAAVchZxMFGpFmrNVFLALxcvNFyn # check if the nodes are added successfully microk8s kubectl get nodes\rCopy\rTroubleshooting After microK8s is started and dashboard is served, no endpoints are available for dashboard: { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;no endpoints available for service \\\u0026#34;kubernetes-dashboard\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ServiceUnavailable\u0026#34;, \u0026#34;code\u0026#34;: 503 }\rCopy\rFor my case, when running the below commands, it shows that all pods are having unknown status:\nmicrok8s kubectl get pods -o wide --all-namespaces\rCopy\rWhen I tried removing addons, MicroK8s also suggests to perform microk8s disable storage:destroy-storage for deleting any storage.\nmicrok8s disable dashboard dns registry\rCopy\r","date":"2020-06-19","permalink":"https://seehiong.github.io/posts/2020/06/external-storage/","summary":"Expanding my Raspberry Pi Cluster\u0026rsquo;s storage with an external 640GB USB hard disk took 35 minutes. After mounting the external storage and ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","MicroK8s","Storage"],"title":"External Storage"},{"content":"Plasma Bigscreen is a completely open UI stack for your personal TV box\nPlasma Bigscreen on Raspberry Pi 4 Model B 8GB (Total Setup Time: 30 mins)\nPlasma Bigscreen\rwill transform your regular TV into a smart one. With built-in Mycroft AI voice assistant, you can use voice command to control. So, even though it is still in beta stage, with the arrival of the microHDMI cable, I am eager to test drive Plasma Bigscreen on Raspberry Pi 4 Model B 8GB!\nPi OS Preparation (15 mins)\nThe latest beta image for the Raspberry Pi 4 can be downloaded from here\r.\nAnd, I have used Win32DiskImager\rto write the image file to my 16GB SD card.\nPowering Up Plasma Bigscreen (15 mins)\nAfter booting up and going through few of the Plasma Bigscreen pages and it directs me to home.mycroft.ai\rto activate Mycroft:\nAfter registering an account with Mycroft and having added the device, the next step is to setup mycroft:\nAnd I tried using apple earpod\rbut it failed to response to my voice. In the end, I performed a hard reboot. This is the screen that eventually greeted me:\nSince Plasma Bigscreen on Raspberry Pi 4 Model B 8GB is still at beta stage, I decided to wait out. At the same time, should I be able to grab hold of a USB microphone, I will definitely test this out again. Stay tuned!\nTroubleshooting Unable to control using TV remote and USB keyboard and mouse The default username is mycroft and password is mycroft. In attempt to resolve the issue, I tried to update the OS with the following commands, but the system hangs thereafter (I had to re-image the SD card again):\nsudo apt update sudo apt upgrade\rCopy\rThe sha256checksum is verified using MD5 \u0026amp; SHA CheckSum Utility\rand against the issue reported\r:\n3988a209b48fafb4e753474ca21c576aad81cf28af5dd1bc88a9c3d1e885b358\rCopy\rOn second attempt, I used the official Raspberry Pi Imager\rto burn directly without compressing it. I did not create the empty ssh file my second attempt.\n","date":"2020-06-14","permalink":"https://seehiong.github.io/posts/2020/06/plasma-bigscreen/","summary":"Excited to explore, I set up Plasma Bigscreen on my Raspberry Pi 4 Model B 8GB in just 30 minutes. Downloading the beta image and using ‚Ä¶","tags":["Raspberry Pi","Plasma","Bigscreen"],"title":"Plasma Bigscreen"},{"content":"Adding few low-cost Raspberry Pi nodes to improve your MicroK8s performance\nMicroK8s Cluster on Raspberry Pi 4 Model B 8GB (Part II) (Total Setup Time: 45 mins)\nFollowing up from the previous post\r, for this second part, I will be adding new Raspberry Pi nodes to the MicroK8s Cluster. However, other than Raspberry Pi, you can also re-purpose some of your older unused laptop or PC and add them to your MicroK8s cluster.\nCreating new Pi nodes (40 mins)\nI will be using 2 of my older Pi 3B and using the same Ubuntu Server (64-bit)\ras my OS. The default username is ubuntu and password is ubuntu. Update the OS to the latest and perform all of the following steps:\nsudo apt update sudo apt upgrade sudo snap install microk8s --classic sudo usermod -a -G microk8s ubuntu sudo chown -f -R ubuntu ~/.kube sudo sed -i \u0026#39;$ s/$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1/\u0026#39; /boot/firmware/cmdline.txt\rCopy\rFinally, change its hostname by running:\nsudo nano /etc/hostname sudo nano /etc/hosts\rCopy\rFor now, I will repeat the above process for the 2nd raspberry pi. Will plan to use Cluster SSH\rwhen I am ready to grow my cluster.\nMaster and Leaf nodes for your MicroK8s cluster (5 mins)\nOn the appointed master node (typically the one which is more powerful in terms of processing power), run the following command for each node to be added:\nsudo microk8s add-node\rCopy\rFor each of the 2 newly created nodes, copy the output from the master node. The command will be similar to this:\nmicrok8s join 192.168.xx.xx:25000/HWvrBvNWjIcOYAoSMfpxpwSbMkNHIfAX\rCopy\rTo check if the nodes are joined successfully, from the master node run this command:\nmicrok8s kubectl get node\rCopy\rThat\u0026rsquo;s it! With this MicroK8s Cluster on Raspberry Pi 4 Model B 8GB in place, I am able to try out some of the cool things Kubernetes has to offer, and at the same time, to slowly offload our CI/CP pipelines to the cluster. Stay tuned on the next few topics on the different use cases of this lightweight cluster!\nTroubleshooting Joining as a node on the master node itself If you have accidentally added as a node on the master node itself, simply leave the cluster:\nsudo microk8s add-node microk8s join 192.168.xx.xx:25000/HWvrBvNWjIcOYAoSMfpxpwSbMkNHIfAX # joining as a leaf node to ownself microk8s leave # just leave!\rCopy\r","date":"2020-06-09","permalink":"https://seehiong.github.io/posts/2020/06/microk8s-on-pi-4-8gb-ii/","summary":"In the second part of my MicroK8s adventure on Raspberry Pi 4 Model B 8GB, I spent 45 minutes adding low-cost Raspberry Pi nodes to enhance cluster ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","MicroK8s"],"title":"MicroK8s on Pi 4 8GB (II)"},{"content":"Making your Raspberry Pi works for your MicroK8s cluster\nMicroK8s Cluster on Raspberry Pi 4 Model B 8GB (Part I) (Total Setup Time: 40 mins)\nI am very excited to receive my Raspberry Pi 4 today. Since I do not have the microHDMI cable, I decided to go for a headless install. With this new Pi 8GB, I plan to check out on the MicroK8s\r, a lightweight upstream K8s. This tutorial shows my steps for setting up the MicroK8s Cluster on Raspberry Pi 4 Model B 8GB.\nRaspberry Pi OS Preparation (15 mins)\nDue to the 64-bit requirements for MicroK8s, I am using Ubuntu Server (64-bit)\ras my OS. Since this is going to be a headless install, after burning the image onto my 32GB SD card, I created an empty file named ssh at d:/boot (you may notice that I am working on a Windows PC).\nPowering Up (10 mins)\nAfter powering the Raspberry Pi up with the USB-C power adapter, it can be accessed via PuTTy\r:\nThe default username is ubuntu and password is ubuntu, be sure to change password as always. Update the OS to the latest by:\nsudo apt update sudo apt upgrade\rCopy\rChange its hostname by running:\nsudo nano /etc/hostname sudo nano /etc/hosts\rCopy\rEnable memory cgroup, by opening the file:\nsudo nano /boot/firmware/cmdline.txt\rCopy\rNext, add the following at the end of the line:\ncgroup_enable=memory cgroup_memory=1\rCopy\rReboot after the above modification and check that memory group is enabled (\u0026ldquo;1\u0026rdquo;):\ngrep mem /proc/cgroups | awk \u0026#39;{ print $4 }\u0026#39;\rCopy\rInstalling MicroK8s on Raspberry Pi (10 mins)\nAfter the reboot, install microK8s:\nsudo snap install microk8s --classic\rCopy\rAdd the ubuntu user to the microk8s group:\nsudo usermod -a -G microk8s ubuntu sudo chown -f -R ubuntu ~/.kube\rCopy\rEnsure that the ubuntu node is ready (without setting the cgroup memory, it will always report as not ready):\nsu - ubuntu microk8s kubectl get nodes\rCopy\rUsing MicroK8s (5 mins)\nFor checking MicroK8s status and ensure that all services are up:\nmicrok8s status --wait-ready\rCopy\rIf you wish to turn on any extra services that you want (microk8s enable \u0026ndash;help for more options), for example enabling dashboard, dns and registry:\nmicrok8s enable dashboard dns registry\rCopy\rIn order to access dashboard without a token, add \u0026ndash;enable-skip-login option to the spec section (microk8s edit uses the vim editor\r):\nsudo microk8s kubectl -n kube-system edit deploy kubernetes-dashboard -o yaml\rCopy\rspec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: kubernetes-dashboard spec: containers: - args: - --auto-generate-certificates - --namespace=kube-system - --enable-skip-login\rCopy\rNext, to serve the dashboard in the background (the skip button will appear only after a reboot or by running microk8s stop, followed by microk8s start):\nmicrok8s kubectl proxy --accept-hosts=.\\* --address=0.0.0.0 \u0026amp;\rCopy\rFinally, for viewing dashboard from my windows laptop (the grey text below denotes IP of your Pi). Hit the skip button to view the dashboard:\nhttp://192.168.xx.xx:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\rCopy\rTo add leaf nodes to the MicroK8s Cluster on Raspberry Pi 4 Model B 8GB, please check this\rout.\nTroubleshooting microk8s is not available on stable for this architecture (armhf) Since MicroK8s is only available on 64-bit architecture (amd64, arm64, ppc64el), I have switched to using 64-bit OS instead.\nFinding IP address of the Pi Mac addresses are assigned by device manufacturers and you can find those from here\r. To find out the IP, you can use the following command:\n# You may try e4-5f-01 too arp -a | findstr dc-a6-32 Copy\rDepending on your network configuration, if you are still unable to figure out the IP, you can use nmap\rto scan your network (replacing the IP accordingly):\nnmap -sn 192.168.xx.0/24\rCopy\rBelow is the sample nmap output:\nMAC Address: DC:A6:32:xx:xx:xx (Raspberry Pi Trading) Nmap scan report for 192.168.xx.xx\rCopy\r","date":"2020-06-06","permalink":"https://seehiong.github.io/posts/2020/06/microk8s-on-pi-4-8gb-i/","summary":"I recently spent 40 minutes setting up MicroK8s on my new Raspberry Pi 4 Model B 8GB. Opting for a headless install due to a lack of a microHDMI ‚Ä¶","tags":["Raspberry Pi","K8s","Cluster","MicroK8s","Nmap"],"title":"MicroK8s on Pi 4 8GB (I)"},{"content":"Your own personal NAS server using OpenMediaVault on Raspberry Pi\nOpenMediaVault on Pi (Total Setup Time: 90 mins)\nWith my old Raspberry Pi 3 Mobel B V1.2 laying around, I plan to re-purpose it into a NAS server. OpenMediaVault (\rOMV\r) is a network attached storage (NAS) solution based on Debian Linux. So, let\u0026rsquo;s start trying openmediavault on Raspberry Pi out.\nThis tutorial lists my steps for setting OMV, referencing from Installing OMV5 On a Raspberry Pi\r.\nPi OS Preparation (15 mins)\nFor the OS, I choose Raspberry Pi OS (32-bit) Lite\r. Next, after downloading, you may check the file\u0026rsquo;s integrity using MD5 \u0026amp; SHA CheckSum Utility\r.\nTo write the raspberry Pi OS (previously called Raspbian) image or other operating system onto my 16GB SD card, I use Win32DiskImager\r. Alternatively you may use the official Raspberry Pi Imager\r.\nPowering Up (10 mins)\nAfter powering the Pi up, you can patch the OS to the latest by running the commands:\nsudo apt update sudo apt upgrade\rCopy\rYou may change the password by:\nsudo passwd\rCopy\rNext, configure the keyboard and perform a reboot:\nsudo dpkg-reconfigure keyboard-configuration sudo reboot\rCopy\rInstalling openmediavault (45 mins)\nNow, the Pi is ready for openmediavault installation. With thanks to the OMV developers, I install OMV by using the commands:\nwget -O - https://github.com/OpenMediaVault-Plugin-Developers/installScript/raw/master/install | sudo bash\rCopy\rThe above scripts\rwill install openmediavault, omv-extras, and flashmemory and will take quite a while. A reboot is performed once installation is completed.\nConfiguring openmediavault (20 mins)\nTo access the openmediavault on Raspberry Pi, you can either use the Pi\u0026rsquo;s IP address or by its default hostname (\rhttp://raspberrypi\r)\nFrom the left menu, navigate to System \u0026gt; General Settings \u0026gt; Web Administrator Password and change the default password.\nNext, I change the hostname by navigating to System \u0026gt; Network \u0026gt; General and enters the new hostname. After clicking on the Save button, wait for the dialog to appear and apply changes.\nAlternatively, you can change the hostname by issuing the commands and replace the default raspberrypi to the new hostname:\nsudo nano /etc/hostname sudo nano /etc/hosts sudo reboot\rCopy\rNext, I attached my 600GB hard disk to the Pi. With the official Pi 2.5A power adapter, I tried attaching a higher capacity hard disk but the Pi was not able to handle the load and OMV cannot detect the HDD at all.\nAdding a User To add a user, navigate to Access Rights Management \u0026gt; User and click on Add button. Enters the user information, click on Save button and apply the changes:\nAdding a File System For adding a file system, navigate to Storage \u0026gt; File systems, select on the external HDD (/dev/sda1). Next, click on the mount button and apply the changes:\nAdding a Shared Folder To add a shared folder, navigate to Access Rights Management \u0026gt; Shared Folders and click on the Add button. After entering the shared folder name and selected the HDD, click on Save button and apply the changes.\nEnabling SMB/CIFS Service Next, to enable the SMB/CIFS service, navigate to Services \u0026gt; SMB/CIFS \u0026gt; Settings, toggle ON Enable radio button, click on the Save button and apply the changes.\nAdding File Share Finally, to add a file share, navigate to Services \u0026gt; SMB/CIFS \u0026gt; Shares, and click on the Add button. Select the shared folder, choose Guest allowed, click on Save button and apply the changes.\nAnd that\u0026rsquo;s it! By following through this long post till here, you have finally created and own your personal openmediavault on Raspberry Pi!\nTroubleshooting OpenMediaVault Failed to start Wait for Network to be Configured It shows enx{mac address} when running this command:\nifconfig -a\rCopy\rTo solve this, add the following to end of the line at /boot/cmdline.txt:\nnet.ifnames=0\rCopy\rTo fix the networking issue, I go for the following changes to /etc/network/interfaces\n# source-directory /etc/network/interfaces.d auto lo iface lo inet loopback auto eth0 iface eth0 inet dhcp\rCopy\rAfter the above is done and Pi reboot is completed, this following command will show my new hostname:\nhostname --all-fqdns\rCopy\r","date":"2020-06-05","permalink":"https://seehiong.github.io/posts/2020/06/nas-server/","summary":"Repurposing my Raspberry Pi 3B into a NAS server using OpenMediaVault (OMV) was a 90-minute project. I opted for Raspberry Pi OS Lite, ensuring its ‚Ä¶","tags":["Raspberry Pi","NAS","OpenMediaVault","Storage","SMB","CIFS"],"title":"NAS Server"}]