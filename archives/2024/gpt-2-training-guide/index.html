<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
    




    







<script defer language="javascript" type="text/javascript" src="/js/bundle.min.b989f0fdae3971a3effd81d6d0b27b3cb1023ff0b601ff43e2118b0d9d82ddb9.js"></script>





  
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-5PNPM4HWQ0"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-5PNPM4HWQ0');
        }
      </script>

    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    <link rel="icon" href=/favicon.png>

    
    





  





  
  
  


<!-- Open Graph image and Twitter Card metadata -->

<title itemprop="name">Tech Blog - GPT-2 Training Guide</title>
<meta property="og:title" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Training&#32;Guide />
<meta name="twitter:title" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Training&#32;Guide />
<meta itemprop="name" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Training&#32;Guide />
<meta name="application-name" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Training&#32;Guide />
<meta property="og:site_name" content="See Hiong&#39;s Blog" />


<meta name="description" content="" />
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />


<base href="https://seehiong.github.io/archives/2024/gpt-2-training-guide/" />
<link rel="canonical" href="https://seehiong.github.io/archives/2024/gpt-2-training-guide/" itemprop="url" />
<meta name="url" content="https://seehiong.github.io/archives/2024/gpt-2-training-guide/" />
<meta name="twitter:url" content="https://seehiong.github.io/archives/2024/gpt-2-training-guide/" />
<meta property="og:url" content="https://seehiong.github.io/archives/2024/gpt-2-training-guide/" />


<meta property="og:updated_time" content="2024-10-31T10:00:00&#43;08:00" />


<link rel="sitemap" type="application/xml" title="Sitemap" href='https://seehiong.github.io/sitemap.xml' />

<meta name="robots" content="index,follow" />
<meta name="googlebot" content="index,follow" />



<meta property="fb:admins" content="" />


<meta name="apple-mobile-web-app-title" content="See Hiong&#39;s Blog" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black" />






<meta name="generator" content="Hugo 0.134.1">


    
    

<link type="text/css" rel="stylesheet" href="/css/bundle.min.95cc0677a25232434df79a1cf0ee059633a97da88b4235a87f2a69bea2c1e24d.css">


    
    <style>
    body {
        --sidebar-bg-color: #202020;
        --sidebar-img-border-color: #515151;
        --sidebar-p-color: #909090;
        --sidebar-h1-color: #FFF;
        --sidebar-a-color: #FFF;
        --sidebar-socials-color: #FFF;
        --text-color: #222;
        --bkg-color: #FAF9F6;
        --post-title-color: #303030;
        --list-color: #5a5a5a;
        --link-color: #268bd2;
        --date-color: #515151;
        --table-border-color: #E5E5E5;
        --table-stripe-color: #F9F9F9;
        --code-color: #000;
        --code-background-color: #E5E5E5;
        --code-block-color: #fff;
        --code-block-background-color: #272822;
        --moon-sun-color: #FFF;
        --moon-sun-background-color: #515151;
    }
    body.dark-theme {
        --text-color: #eee;
        --bkg-color: #121212;
        --post-title-color: #DBE2E9;
        --list-color: #9d9d9d;
        --link-color: #268bd2;
        --date-color: #9a9a9a;
        --table-border-color: #515151;
        --table-stripe-color: #202020;
        --code-color: #fff;
        --code-background-color: #515151;
        --code-block-color: #fff;
        --code-block-background-color: #272822;
    }
    body {
        background-color: var(--bkg-color);
    }
</style>

</head>

    <body class="dark-theme">
        <div class="wrapper">
            <aside class="sidebar">
    <div class="container sidebar-sticky">
        <div class="light-dark" align="right">
    <button class="btn-light-dark" title="Toggle light/dark mode">
        <svg class="moon" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M6 .278a.768.768 0 0 1 .08.858a7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277c.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316a.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71C0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"/>
        </svg>
        <svg class="sun" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M8 12a4 4 0 1 0 0-8a4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"/>
        </svg>
    </button>
</div>

        <div class="sidebar-about">
    <h1 class="brand">
        
        
            <a href="https://seehiong.github.io/">
                <h1>Tech Blog</h1>
            </a>
        
    </h1>
    <p class="lead">
    Chronicling my personal expedition of technology exploration and experimentation, facilitated by a home-based self-configured laboratory
    </p>
</div>

        <link href="/pagefind/pagefind-ui.css" rel="stylesheet">
<style>
    :root {
        --pagefind-ui-primary: #eeeeee;
        --pagefind-ui-text: #eeeeee;
        --pagefind-ui-background: #152028;
        --pagefind-ui-border: #152028;
        --pagefind-ui-tag: #152028;
    }
</style>
<script src="/pagefind/pagefind-ui.js"></script>
<div style="max-width: 100%; margin: clamp(2px, 1vw, 10px) auto;">
    <div id="search"></div>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search", showSubResults: true, showImages: false });
        });
    </script>
</div>
        <nav>
    <ul class="sidebar-nav">

        
        
        
        
            

            
                
                
                    <li class="heading">
                        <a href="/2025/">2025</a>
                    </li>
                    
                        <li class="sub-heading">
                            Recent
                        </li>
                        
                            <li class="bullet">
                                <a href="https://seehiong.github.io/2025/jmc-java-performance-profiling-simplified/">JMC: Java Performance Profiling Simplified</a>
                            </li>
                        
                            <li class="bullet">
                                <a href="https://seehiong.github.io/2025/building-a-vuejs-frontend-for-combinatorial-optimization-problem/">Building a Vue.js Frontend for Combinatorial Optimization Problems</a>
                            </li>
                        
                    
                
            
                
                
            
                
                
            
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
        
        
            

            
                
                
            
                
                
            
                
                
                    <li class="heading">
                        <a href="/archives/">Archives</a>
                    </li>
                    
                
            
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
        
        
            

            
                
                
            
                
                
                    <li class="heading">
                        <a href="/about/">About</a>
                    </li>
                    
                
            
                
                
            
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
        

    </ul>
</nav>

        
    <a target="_blank" class="social" title="GitHub" href="https://github.com/seehiong">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="-2 -2 24 24">
            <path fill="currentColor" d="M18.88 1.099C18.147.366 17.265 0 16.233 0H3.746C2.714 0 1.832.366 1.099 1.099C.366 1.832 0 2.714 0 3.746v12.487c0 1.032.366 1.914 1.099 2.647c.733.733 1.615 1.099 2.647 1.099H6.66c.19 0 .333-.007.429-.02a.504.504 0 0 0 .286-.169c.095-.1.143-.245.143-.435l-.007-.885c-.004-.564-.006-1.01-.006-1.34l-.3.052c-.19.035-.43.05-.721.046a5.555 5.555 0 0 1-.904-.091a2.026 2.026 0 0 1-.872-.39a1.651 1.651 0 0 1-.572-.8l-.13-.3a3.25 3.25 0 0 0-.41-.663c-.186-.243-.375-.407-.566-.494l-.09-.065a.956.956 0 0 1-.17-.156a.723.723 0 0 1-.117-.182c-.026-.061-.004-.111.065-.15c.07-.04.195-.059.378-.059l.26.04c.173.034.388.138.643.311a2.1 2.1 0 0 1 .631.677c.2.355.44.626.722.813c.282.186.566.28.852.28c.286 0 .533-.022.742-.065a2.59 2.59 0 0 0 .585-.196c.078-.58.29-1.028.637-1.34a8.907 8.907 0 0 1-1.333-.234a5.314 5.314 0 0 1-1.223-.507a3.5 3.5 0 0 1-1.047-.872c-.277-.347-.505-.802-.683-1.365c-.177-.564-.266-1.215-.266-1.952c0-1.049.342-1.942 1.027-2.68c-.32-.788-.29-1.673.091-2.652c.252-.079.625-.02 1.119.175c.494.195.856.362 1.086.5c.23.14.414.257.553.352a9.233 9.233 0 0 1 2.497-.338c.859 0 1.691.113 2.498.338l.494-.312a6.997 6.997 0 0 1 1.197-.572c.46-.174.81-.221 1.054-.143c.39.98.424 1.864.103 2.653c.685.737 1.028 1.63 1.028 2.68c0 .737-.089 1.39-.267 1.957c-.177.568-.407 1.023-.689 1.366a3.65 3.65 0 0 1-1.053.865c-.42.234-.828.403-1.223.507a8.9 8.9 0 0 1-1.333.235c.45.39.676 1.005.676 1.846v3.11c0 .147.021.266.065.357a.36.36 0 0 0 .208.189c.096.034.18.056.254.064c.074.01.18.013.318.013h2.914c1.032 0 1.914-.366 2.647-1.099c.732-.732 1.099-1.615 1.099-2.647V3.746c0-1.032-.367-1.914-1.1-2.647z"/>
        </svg>
    </a>



    <a target="_blank" class="social" title="LinkedIn" href="https://www.linkedin.com/in/seehiong">
        <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1.2em" viewBox="0 0 448 512">
            <path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5c0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7c-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5c67.2 0 79.7 44.3 79.7 101.9V416z"/>
        </svg>
    </a>















    <a target="_blank" class="social" title="RSS Feed" href="/2025/index.xml">
        <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1.2em" viewBox="0 0 1280.000000 1280.000000">
            <g transform="translate(0.000000,1280.000000) scale(0.100000,-0.100000)" fill="currentColor">
                <path d="M2295 11929 c-284 -12 -642 -45 -707 -65 -17 -5 -18 -63 -18 -1039 0 -569 4 -1036 8 -1039 5 -3 74 6 153 19 510 86 1168 95 1789 25 1348 -153 2602 -677 3670 -1531 385 -308 820 -744 1126 -1129 842 -1060 1362 -2313 1514 -3650 70 -621 61 -1279 -25 -1789 -13 -79 -22 -148 -19 -153 3 -4 471 -8 1039 -8 l1035 0 5 23 c51 225 85 942 67 1419 -23 605 -77 1044 -198 1617 -294 1400 -927 2734 -1823 3846 -1043 1295 -2364 2259 -3909 2854 -1158 447 -2451 656 -3707 600z"/>
                <path d="M2255 7845 c-269 -25 -620 -81 -667 -106 -17 -9 -18 -55 -18 -899 0 -706 3 -890 13 -890 6 0 66 18 132 41 130 44 288 79 467 105 154 21 577 30 749 15 1207 -107 2267 -823 2814 -1902 166 -327 268 -637 330 -1001 38 -227 48 -384 42 -662 -8 -348 -44 -590 -126 -831 -23 -66 -41 -126 -41 -132 0 -10 184 -13 890 -13 844 0 890 1 899 18 27 50 88 452 110 725 14 162 14 624 1 782 -59 703 -233 1323 -545 1945 -481 956 -1313 1788 -2270 2268 -620 310 -1239 483 -1940 542 -165 14 -669 10 -840 -5z"/>
                <path d="M2519 3815 c-391 -66 -725 -336 -868 -703 -79 -201 -96 -462 -45 -677 83 -344 338 -641 666 -774 116 -47 205 -69 330 -80 412 -39 811 153 1040 500 193 292 240 648 128 981 -135 403 -492 699 -914 757 -100 14 -241 12 -337 -4z"/>
            </g>
        </svg>
    </a>





        <p class="footnote">
powered by <a target="_blank" href="https://gohugo.io">Hugo</a> | themed with <a target="_blank" href="https://github.com/lukeorth/poison">poison</a>
    <br>
    &copy; 2025 See Hiong. All rights reserved.
</p>

  </div>
</aside>

            <main class="content container">
                <div class="post" data-pagefind-body>
  <div class="info">
  <h1 class="post-title">
    <a href="https://seehiong.github.io/archives/2024/gpt-2-training-guide/">GPT-2 Training Guide</a>
  </h1>

  <div class="headline" data-pagefind-ignore>
    <div>
      
      
      <time datetime=" 2024-10-31T10:00:00&#43;0800" class="post-date">
        October 31, 2024
      </time>
      
      <span> - </span>
      <span class="reading-time">
        
          
        

        <span>18 mins read</span>
      </span>
    </div>

    
    <ul class="tags">
      
      <li class="tag-AI">
        <a href="https://seehiong.github.io/tags/ai">AI</a>
      </li>
      
      <li class="tag-GPT">
        <a href="https://seehiong.github.io/tags/gpt">GPT</a>
      </li>
      
      <li class="tag-PyTorch">
        <a href="https://seehiong.github.io/tags/pytorch">PyTorch</a>
      </li>
      
      <li class="tag-LLM">
        <a href="https://seehiong.github.io/tags/llm">LLM</a>
      </li>
      
      <li class="tag-nanoGPT">
        <a href="https://seehiong.github.io/tags/nanogpt">nanoGPT</a>
      </li>
      
      <li class="tag-MLflow">
        <a href="https://seehiong.github.io/tags/mlflow">MLflow</a>
      </li>
      
    </ul>
    
  </div>

  
  
  <p class="author">
    Author: <i>See Hiong</i>
  </p>
  

  

  
</div>

  <p>In this post, I’ll build on my <a href="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/" target="_blank">previous post</a>, where we set up GPT-2. Following Andrej Karpathy’s instructional video, I’ll walk through each step for training GPT-2 on a small dataset—Tiny Shakespeare. This post is a documentation of my learning journey with GPT-2, closely following Karpathy&rsquo;s approach.</p>
<h2 id="training">Training</h2>
<p>We&rsquo;ll use the Tiny Shakespeare dataset to get started:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;input.txt&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> text[:<span style="color:#ae81ff">1000</span>]
</span></span><span style="display:flex;"><span>print(data[:<span style="color:#ae81ff">100</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sample output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># First Citizen:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Before we proceed any further, hear me speak.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># All:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Speak, speak.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># First Citizen:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You</span>
</span></span></code></pre></div><p>To verify the data size, we can use a word count tool in WSL:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wc input.txt
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 40000  202651 1115394 input.txt</span>
</span></span></code></pre></div><ol>
<li>Encoding the Dataset with tiktoken</li>
</ol>
<p>Using tiktoken to encode the dataset (GPT-2&rsquo;s tokenizer), we can observe that 198 represents the newline character:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tiktoken
</span></span><span style="display:flex;"><span>enc <span style="color:#f92672">=</span> tiktoken<span style="color:#f92672">.</span>get_encoding(<span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>encode(data)
</span></span><span style="display:flex;"><span>print(tokens[:<span style="color:#ae81ff">24</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]</span>
</span></span></code></pre></div><p>To break this into sequences, we convert the encoded data into B x T tensors for batching.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>buf <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(tokens[:<span style="color:#ae81ff">24</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> buf[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">6</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> buf[<span style="color:#ae81ff">1</span>:]<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">6</span>)
</span></span><span style="display:flex;"><span>print(x) <span style="color:#75715e"># input tensor</span>
</span></span><span style="display:flex;"><span>print(y) <span style="color:#75715e"># label tensor</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[ 5962, 22307,    25,   198,  8421,   356],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [ 5120,   597,  2252,    11,  3285,   502],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [ 2740,    13,   198,   198,  3237,    25],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [  198,  5248,   461,    11,  2740,    13]])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[22307,    25,   198,  8421,   356,  5120],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [  597,  2252,    11,  3285,   502,  2740],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [   13,   198,   198,  3237,    25,   198],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [ 5248,   461,    11,  2740,    13,   198]])</span>
</span></span></code></pre></div><ol start="2">
<li>Adding a Loss Function</li>
</ol>
<p>Let&rsquo;s define a loss function in the custom GPT model:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, idx, targets<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># idx is of shape (B, T)</span>
</span></span><span style="display:flex;"><span>        B, T <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> T <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Cannot forward sequence of length </span><span style="color:#e6db74">{</span>t<span style="color:#e6db74">}</span><span style="color:#e6db74">, block size is only </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># forward the token and position embeddings</span>
</span></span><span style="display:flex;"><span>        pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, T, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>idx<span style="color:#f92672">.</span>device) <span style="color:#75715e"># shape (t)</span>
</span></span><span style="display:flex;"><span>        pos_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wpe(pos) <span style="color:#75715e"># position embeddings of shape (t, n_embd)</span>
</span></span><span style="display:flex;"><span>        tok_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte(idx) <span style="color:#75715e"># token embeddings of shape (b, t, n_embd)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tok_emb <span style="color:#f92672">+</span> pos_emb
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># forward the blocks of the transformer</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> block(x)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># forward the final layernorm and the classifier</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>ln_f(x)        
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x) <span style="color:#75715e"># (B, T, vocab_size)</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> targets <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(logits<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, logits<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)), targets<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits, loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cpu&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available():
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cuda&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">elif</span> hasattr(torch<span style="color:#f92672">.</span>backends, <span style="color:#e6db74">&#34;mps&#34;</span>) <span style="color:#f92672">and</span> torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>mps<span style="color:#f92672">.</span>is_available():
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;mps&#34;</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;using device: </span><span style="color:#e6db74">{</span>device<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get a data batch</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tiktoken
</span></span><span style="display:flex;"><span>enc <span style="color:#f92672">=</span> tiktoken<span style="color:#f92672">.</span>get_encoding(<span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;input.txt&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>text <span style="color:#f92672">=</span> text[:<span style="color:#ae81ff">1000</span>]
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>encode(text)
</span></span><span style="display:flex;"><span>B, T <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>buf <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(tokens[:B<span style="color:#f92672">*</span>T <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>buf <span style="color:#f92672">=</span> buf<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># move buf to same device</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> buf[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>view(B, T)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> buf[<span style="color:#ae81ff">1</span>:]<span style="color:#f92672">.</span>view(B, T)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get logits</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT(GPTConfig())
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(loss)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># using device: cuda</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor(11.0591, device=&#39;cuda:0&#39;, grad_fn=&lt;NllLossBackward0&gt;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># expected loss at initialization</span>
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">50257</span>)
</span></span><span style="display:flex;"><span>print(input)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 10.82490511970208</span>
</span></span></code></pre></div><ol start="3">
<li>Optimizing the Model</li>
</ol>
<p>We&rsquo;ll use the AdamW optimizer, which is typically effective for initial GPT training:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-4</span>) <span style="color:#75715e"># good learning rate for most at the beginning</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad() <span style="color:#75715e"># to always start zero gradient</span>
</span></span><span style="display:flex;"><span>    logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward() <span style="color:#75715e"># deposits or add the gradient </span>
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step() <span style="color:#75715e"># update parameters, decrease loss</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">, loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 0, loss: 11.059085845947266</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 1, loss: 6.672627925872803</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 2, loss: 4.326003074645996</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 47, loss: 0.003014578018337488</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 48, loss: 0.002937569282948971</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 49, loss: 0.002866392722353339</span>
</span></span></code></pre></div><ol start="4">
<li>Adding a Simple DataLoader</li>
</ol>
<p>A lightweight data loader simplifies the batching process by iterating over the encoded data:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tiktoken
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DataLoaderLite</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, B, T):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>B <span style="color:#f92672">=</span> B
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>T <span style="color:#f92672">=</span> T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;input.txt&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>            text <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>        enc <span style="color:#f92672">=</span> tiktoken<span style="color:#f92672">.</span>get_encoding(<span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>encode(text)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(tokens)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;loaded </span><span style="color:#e6db74">{</span>len(self<span style="color:#f92672">.</span>tokens)<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;1 epoch = </span><span style="color:#e6db74">{</span>len(self<span style="color:#f92672">.</span>tokens) <span style="color:#f92672">//</span> (B <span style="color:#f92672">*</span> T)<span style="color:#e6db74">}</span><span style="color:#e6db74"> batches&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">next_batch</span>(self):
</span></span><span style="display:flex;"><span>        B, T <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>B, self<span style="color:#f92672">.</span>T
</span></span><span style="display:flex;"><span>        buf <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>tokens[self<span style="color:#f92672">.</span>current_position : self<span style="color:#f92672">.</span>current_position<span style="color:#f92672">+</span>B<span style="color:#f92672">*</span>T<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> buf[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>view(B, T) <span style="color:#75715e"># inputs</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> buf[<span style="color:#ae81ff">1</span>:]<span style="color:#f92672">.</span>view(B, T) <span style="color:#75715e"># targets</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># advance the position in the tensor</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">+=</span> B <span style="color:#f92672">*</span> T
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if loading the next batch would be out of bounds, reset</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">+</span> (B <span style="color:#f92672">*</span> T <span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">&gt;</span> len(self<span style="color:#f92672">.</span>tokens):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x, y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoaderLite(B<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, T<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get logits</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT(GPTConfig())
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-4</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># move tensor to device</span>
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">, loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># loaded 338025 tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1 epoch = 2640 batches</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 0, loss: 11.003260612487793</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 1, loss: 9.66711139678955</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 2, loss: 8.685151100158691    </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 47, loss: 5.960447311401367</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 48, loss: 6.783339500427246</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 49, loss: 6.529984474182129</span>
</span></span></code></pre></div><ol start="5">
<li>Using Pretrained GPT-2 Parameters</li>
</ol>
<p>Leveraging pretrained weights can enhance performance. Here, we validate that GPT-2&rsquo;s top and bottom layers share weights:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> GPT2LMHeadModel
</span></span><span style="display:flex;"><span>model_hf <span style="color:#f92672">=</span> GPT2LMHeadModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;gpt2&#34;</span>) <span style="color:#75715e">#124M</span>
</span></span><span style="display:flex;"><span>sd_hf <span style="color:#f92672">=</span> model_hf<span style="color:#f92672">.</span>state_dict()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(sd_hf[<span style="color:#e6db74">&#34;lm_head.weight&#34;</span>]<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>print(sd_hf[<span style="color:#e6db74">&#34;transformer.wte.weight&#34;</span>]<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> (sd_hf[<span style="color:#e6db74">&#34;lm_head.weight&#34;</span>] <span style="color:#f92672">==</span> sd_hf[<span style="color:#e6db74">&#34;transformer.wte.weight&#34;</span>])<span style="color:#f92672">.</span>all()
</span></span><span style="display:flex;"><span>print(result)
</span></span><span style="display:flex;"><span>print(sd_hf[<span style="color:#e6db74">&#34;transformer.wte.weight&#34;</span>]<span style="color:#f92672">.</span>data_ptr)
</span></span><span style="display:flex;"><span>print(sd_hf[<span style="color:#e6db74">&#34;transformer.wte.weight&#34;</span>]<span style="color:#f92672">.</span>data_ptr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([50257, 768])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([50257, 768])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor(True)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &lt;built-in method data_ptr of Tensor object at 0x00000254ECD982D0&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &lt;built-in method data_ptr of Tensor object at 0x00000254ECD982D0&gt;</span>
</span></span></code></pre></div><ol start="6">
<li>Implementing Weight Sharing in Our Model</li>
</ol>
<p>To enable weight sharing, we set the embedding and head layers to use the same weights:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transformer <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict(dict(
</span></span><span style="display:flex;"><span>            wte <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(config<span style="color:#f92672">.</span>vocab_size, config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>            wpe <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(config<span style="color:#f92672">.</span>block_size, config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>            h <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([Block(config) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(config<span style="color:#f92672">.</span>n_layer)]),
</span></span><span style="display:flex;"><span>            ln_f <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>        ))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>vocab_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># weight sharing scheme</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span></code></pre></div><ol start="7">
<li>Initializing Parameters</li>
</ol>
<p>Proper initialization of parameters, such as in self-attention blocks, is crucial to stable training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CausalSelfAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> config<span style="color:#f92672">.</span>n_embd <span style="color:#f92672">%</span> config<span style="color:#f92672">.</span>n_head <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># key, query, value projections for all heads, but in a batch</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_attn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># output projection</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj<span style="color:#f92672">.</span>NANOGPT_SCALE_INIT <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># regularization</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_head <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>n_head
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_embd <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>n_embd
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(<span style="color:#e6db74">&#34;bias&#34;</span>, torch<span style="color:#f92672">.</span>tril(torch<span style="color:#f92672">.</span>ones(config<span style="color:#f92672">.</span>block_size, config<span style="color:#f92672">.</span>block_size))
</span></span><span style="display:flex;"><span>                                    <span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, config<span style="color:#f92672">.</span>block_size, config<span style="color:#f92672">.</span>block_size))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B, T, C <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size() <span style="color:#75715e"># batch size, sequence length, embedding dimensionality (n_embd)</span>
</span></span><span style="display:flex;"><span>        qkv  <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_attn(x)
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> qkv<span style="color:#f92672">.</span>split(self<span style="color:#f92672">.</span>n_embd, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> v<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># attention (materializes the large (T,T) matrix for all the queries and keys)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> (q <span style="color:#f92672">@</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(k<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)))
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> att<span style="color:#f92672">.</span>masked_fill(self<span style="color:#f92672">.</span>bias[:,:,:T,:T] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(att, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> att <span style="color:#f92672">@</span> v <span style="color:#75715e"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, T, C) <span style="color:#75715e"># re-assemble all head outputs side by side</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># output projection</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(y)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MLP</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_fc    <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gelu    <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tanh&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj  <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj<span style="color:#f92672">.</span>NANOGPT_SCALE_INIT <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_fc(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gelu(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># init all weights</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(self<span style="color:#f92672">.</span>_init_weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_init_weights</span>(self, module):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, nn<span style="color:#f92672">.</span>Linear):
</span></span><span style="display:flex;"><span>            std <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.02</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> hasattr(module, <span style="color:#e6db74">&#39;NANOGPT_SCALE_INIT&#39;</span>):
</span></span><span style="display:flex;"><span>                std <span style="color:#f92672">*=</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>n_layer) <span style="color:#f92672">**</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>normal_(module<span style="color:#f92672">.</span>weight, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, std<span style="color:#f92672">=</span>std)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> module<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>zeros_(module<span style="color:#f92672">.</span>bias)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> isinstance(module, nn<span style="color:#f92672">.</span>Embedding):
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>normal_(module<span style="color:#f92672">.</span>weight, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">1337</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available():
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">1337</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoaderLite(B<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, T<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get logits</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT(GPTConfig())
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-4</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># move tensor to device</span>
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">, loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># loaded 338025 tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1 epoch = 2640 batches</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 0, loss: 10.960028648376465</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 1, loss: 9.68770694732666</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 2, loss: 9.082900047302246</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 48, loss: 6.953257083892822</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 49, loss: 6.799217224121094</span>
</span></span></code></pre></div><hr>
<h2 id="training-performance-insights">Training Performance Insights</h2>
<h3 id="understanding-the-hardware">Understanding the Hardware</h3>
<p>The Jetson Orin NX 16GB boasts 100 TOPS (INT8) and 32 tensor cores, making it ideal for inferencing over training. Thus, for training, I will leverage my 6GB RTX A2000, which delivers a single-precision performance of 8 TFLOPS and 104 tensor cores.</p>
<ol>
<li>Data type inspection: Using torch.float32 by default</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> code; code<span style="color:#f92672">.</span>interact(local<span style="color:#f92672">=</span>locals())
</span></span><span style="display:flex;"><span><span style="color:#75715e"># enter below in the interactive prompt for inspection</span>
</span></span><span style="display:flex;"><span>logits<span style="color:#f92672">.</span>dtype
</span></span></code></pre></div><ol start="2">
<li>To clear cache:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># empty cache</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> gc
</span></span><span style="display:flex;"><span>gc<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>empty_cache()
</span></span></code></pre></div><ol start="3">
<li>To watch NVIDIA memory usage in WSL:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>watch -n 0.1 nvidia-smi
</span></span></code></pre></div><h3 id="booast-training-performance">Booast Training Performance</h3>
<ol>
<li><strong>Automatic Mixed Precision Training</strong>: Leverage torch.autocast for efficient mixed precision training</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoaderLite(B<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, T<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>set_float32_matmul_precision(<span style="color:#e6db74">&#39;high&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get logits</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT(GPTConfig())
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-4</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
</span></span><span style="display:flex;"><span>    t0 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>autocast(device_type<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>bfloat16):
</span></span><span style="display:flex;"><span>        logits, loss <span style="color:#f92672">=</span> model(x, y)     
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize() <span style="color:#75715e"># wait for GPU to finish work</span>
</span></span><span style="display:flex;"><span>    t1 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    dt <span style="color:#f92672">=</span> (t1 <span style="color:#f92672">-</span> t0)<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>    tokens_per_sec <span style="color:#f92672">=</span> (train_loader<span style="color:#f92672">.</span>B <span style="color:#f92672">*</span> train_loader<span style="color:#f92672">.</span>T) <span style="color:#f92672">/</span> (t1 <span style="color:#f92672">-</span> t0)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> | loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74"> | norm: </span><span style="color:#e6db74">{</span>norm<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | dt: </span><span style="color:#e6db74">{</span>dt<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">ms, tok/sec: </span><span style="color:#e6db74">{</span>tokens_per_sec<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><ol start="2">
<li><strong>Compile Optimization</strong>: Use torch.compile to accelerate code execution</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># get logits</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT(GPTConfig())
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch._dynamo
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>_dynamo<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>suppress_errors <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>compile(model)
</span></span></code></pre></div><ol start="3">
<li><strong>Flash Attention</strong>: Integrate flash attention to enhance your model&rsquo;s attention mechanism</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CausalSelfAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B, T, C <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size() <span style="color:#75715e"># batch size, sequence length, embedding dimensionality (n_embd)</span>
</span></span><span style="display:flex;"><span>        qkv  <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_attn(x)
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> qkv<span style="color:#f92672">.</span>split(self<span style="color:#f92672">.</span>n_embd, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> v<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># attention (materializes the large (T,T) matrix for all the queries and keys)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(&#39;-inf&#39;))</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># att = F.softmax(att, dim=-1)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>scaled_dot_product_attention(q, k, v, is_causal<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, T, C) <span style="color:#75715e"># re-assemble all head outputs side by side</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># output projection</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(y)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y
</span></span></code></pre></div><ol start="4">
<li><strong>Utilize Efficient Numbers</strong>: Optimize computations by using nice numbers</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># get logits</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT(GPTConfig(vocab_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50304</span>))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># loaded 338025 tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1 epoch = 20 batches</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 0 | loss: 11.026554107666016 | norm: 27.6531 | dt: 4550.37ms, tok/sec: 3600.59</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 1 | loss: 9.354239463806152 | norm: 6.2953 | dt: 7496.54ms, tok/sec: 2185.54</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 2 | loss: 8.961758613586426 | norm: 2.2035 | dt: 7598.34ms, tok/sec: 2156.26</span>
</span></span></code></pre></div><ol start="5">
<li><strong>Gradient Clipping</strong>: Clip the global norm of gradients to 1.0 to stabilize training</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-4</span>, betas<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.9</span>, <span style="color:#ae81ff">0.95</span>), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-8</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
</span></span><span style="display:flex;"><span>    t0 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>autocast(device_type<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>bfloat16):
</span></span><span style="display:flex;"><span>        logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    norm <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize() <span style="color:#75715e"># wait for GPU to finish work</span>
</span></span><span style="display:flex;"><span>    t1 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    dt <span style="color:#f92672">=</span> t1 <span style="color:#f92672">-</span> t0 <span style="color:#75715e"># time difference in seconds</span>
</span></span><span style="display:flex;"><span>    tokens_per_sec <span style="color:#f92672">=</span> (train_loader<span style="color:#f92672">.</span>B <span style="color:#f92672">*</span> train_loader<span style="color:#f92672">.</span>T) <span style="color:#f92672">/</span> (t1 <span style="color:#f92672">-</span> t0)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> | loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74"> | norm: </span><span style="color:#e6db74">{</span>norm<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | dt: </span><span style="color:#e6db74">{</span>dt<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">ms, tok/sec: </span><span style="color:#e6db74">{</span>tokens_per_sec<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 0 | loss: 6.1148786544799805 | norm: 5.3749 | dt: 5243.35ms, tok/sec: 3124.72</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 1 | loss: 5.990167617797852 | norm: 2.5040 | dt: 5465.20ms, tok/sec: 2997.88</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step: 2 | loss: 6.073593616485596 | norm: 1.6870 | dt: 7355.99ms, tok/sec: 2227.30</span>
</span></span></code></pre></div><ol start="6">
<li><strong>Learning Rate Setting</strong>: Adjust and set an appropriate learning rate for your model</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>max_lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">6e-4</span>
</span></span><span style="display:flex;"><span>min_lr <span style="color:#f92672">=</span> max_lr <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>warmup_steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>max_steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_lr</span>(it):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># linear warmup for warmup_iters steps</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> it <span style="color:#f92672">&lt;</span> warmup_steps:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> max_lr <span style="color:#f92672">*</span> (it<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> warmup_steps
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># if it &gt; lr_decay_iters, return min learning rate</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> it <span style="color:#f92672">&gt;</span> max_steps:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> min_lr
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># in between, use cosine decay down to min learning rate</span>
</span></span><span style="display:flex;"><span>    decay_ratio <span style="color:#f92672">=</span> (it <span style="color:#f92672">-</span> warmup_steps) <span style="color:#f92672">/</span> (max_steps <span style="color:#f92672">-</span> warmup_steps)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> decay_ratio <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    coeff <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> math<span style="color:#f92672">.</span>cos(math<span style="color:#f92672">.</span>pi <span style="color:#f92672">*</span> decay_ratio)) <span style="color:#75715e"># coeff starts at 1 and goes to 0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> min_lr <span style="color:#f92672">+</span> coeff <span style="color:#f92672">*</span> (max_lr <span style="color:#f92672">-</span> min_lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-4</span>, betas<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.9</span>, <span style="color:#ae81ff">0.95</span>), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-8</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(max_steps):
</span></span><span style="display:flex;"><span>    t0 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>autocast(device_type<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>bfloat16):
</span></span><span style="display:flex;"><span>        logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    norm <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># determine and set the learning rate for this iteration</span>
</span></span><span style="display:flex;"><span>    lr <span style="color:#f92672">=</span> get_lr(step)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> param_group <span style="color:#f92672">in</span> optimizer<span style="color:#f92672">.</span>param_groups:
</span></span><span style="display:flex;"><span>        param_group[<span style="color:#e6db74">&#39;lr&#39;</span>] <span style="color:#f92672">=</span> lr
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize() <span style="color:#75715e"># wait for GPU to finish work</span>
</span></span><span style="display:flex;"><span>    t1 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    dt <span style="color:#f92672">=</span> t1 <span style="color:#f92672">-</span> t0 <span style="color:#75715e"># time difference in seconds</span>
</span></span><span style="display:flex;"><span>    tokens_per_sec <span style="color:#f92672">=</span> (train_loader<span style="color:#f92672">.</span>B <span style="color:#f92672">*</span> train_loader<span style="color:#f92672">.</span>T) <span style="color:#f92672">/</span> (t1 <span style="color:#f92672">-</span> t0)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>step<span style="color:#e6db74">:</span><span style="color:#e6db74">4d</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74"> | norm: </span><span style="color:#e6db74">{</span>norm<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | dt: </span><span style="color:#e6db74">{</span>dt<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">ms, tok/sec: </span><span style="color:#e6db74">{</span>tokens_per_sec<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    0 | loss: 10.976452 | lr: 0.0001 | norm: 29.3385 | dt: 5638.74ms, tok/sec: 2905.61</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    1 | loss: 9.567224 | lr: 0.0001 | norm: 11.2004 | dt: 7997.90ms, tok/sec: 2048.54</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    2 | loss: 9.393660 | lr: 0.0002 | norm: 11.4306 | dt: 7960.79ms, tok/sec: 2058.09</span>
</span></span></code></pre></div><ol start="7">
<li><strong>Weight Decay</strong>: Apply weight decay to regularize your model and prevent overfitting</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">configure_optimizers</span>(self, weight_decay, learning_rate, device):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># start with all of the candidate parameters (that require grad)</span>
</span></span><span style="display:flex;"><span>        param_dict <span style="color:#f92672">=</span> {pn: p <span style="color:#66d9ef">for</span> pn, p <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters()}
</span></span><span style="display:flex;"><span>        param_dict <span style="color:#f92672">=</span> {pn: p <span style="color:#66d9ef">for</span> pn, p <span style="color:#f92672">in</span> param_dict<span style="color:#f92672">.</span>items() <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>requires_grad}
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don&#39;t.</span>
</span></span><span style="display:flex;"><span>        decay_params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> n, p <span style="color:#f92672">in</span> param_dict<span style="color:#f92672">.</span>items() <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>dim() <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>        nodecay_params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> n, p <span style="color:#f92672">in</span> param_dict<span style="color:#f92672">.</span>items() <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>dim() <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>        optim_groups <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>            {<span style="color:#e6db74">&#39;params&#39;</span>: decay_params, <span style="color:#e6db74">&#39;weight_decay&#39;</span>: weight_decay},
</span></span><span style="display:flex;"><span>            {<span style="color:#e6db74">&#39;params&#39;</span>: nodecay_params, <span style="color:#e6db74">&#39;weight_decay&#39;</span>: <span style="color:#ae81ff">0.0</span>}
</span></span><span style="display:flex;"><span>        ]
</span></span><span style="display:flex;"><span>        num_decay_params <span style="color:#f92672">=</span> sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> decay_params)
</span></span><span style="display:flex;"><span>        num_nodecay_params <span style="color:#f92672">=</span> sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> nodecay_params)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;num decayed parameter tensors: </span><span style="color:#e6db74">{</span>len(decay_params)<span style="color:#e6db74">}</span><span style="color:#e6db74">, with </span><span style="color:#e6db74">{</span>num_decay_params<span style="color:#e6db74">:</span><span style="color:#e6db74">,</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> parameters&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;num non-decayed parameter tensors: </span><span style="color:#e6db74">{</span>len(nodecay_params)<span style="color:#e6db74">}</span><span style="color:#e6db74">, with </span><span style="color:#e6db74">{</span>num_nodecay_params<span style="color:#e6db74">:</span><span style="color:#e6db74">,</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> parameters&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Create AdamW optimizer and use the fused version if it is available</span>
</span></span><span style="display:flex;"><span>        fused_available <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;fused&#39;</span> <span style="color:#f92672">in</span> inspect<span style="color:#f92672">.</span>signature(torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW)<span style="color:#f92672">.</span>parameters
</span></span><span style="display:flex;"><span>        use_fused <span style="color:#f92672">=</span> fused_available <span style="color:#f92672">and</span> <span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#f92672">in</span> device
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;using fused AdamW: </span><span style="color:#e6db74">{</span>use_fused<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(optim_groups, lr<span style="color:#f92672">=</span>learning_rate, betas<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.9</span>, <span style="color:#ae81ff">0.95</span>), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-8</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> optimizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>configure_optimizers(weight_decay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">6e-4</span>, device<span style="color:#f92672">=</span>device)
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># num decayed parameter tensors: 50, with 124,354,560 parameters</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># num non-decayed parameter tensors: 98, with 121,344 parameters</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># using fused AdamW: True</span>
</span></span></code></pre></div><ol start="8">
<li><strong>Simulated Batch Accumulation</strong>: Effectively using smaller batch sizes when memory is limited by accumulating gradients</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(max_steps):
</span></span><span style="display:flex;"><span>    t0 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    loss_accum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> micro_step <span style="color:#f92672">in</span> range(grad_accum_steps):
</span></span><span style="display:flex;"><span>        x, y <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>        x, y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>autocast(device_type<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>bfloat16):
</span></span><span style="display:flex;"><span>            logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># we have to scale the loss to account for gradient accumulation,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># because the gradients just add on each successive backward(),</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># addition of gradients corresponds to a SUM in the objective, but</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># instead of a SUM we want MEAN. Scale the loss here so it comes out right</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss <span style="color:#f92672">/</span> grad_accum_steps
</span></span><span style="display:flex;"><span>        loss_accum <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    norm <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># determine and set the learning rate for this iteration</span>
</span></span><span style="display:flex;"><span>    lr <span style="color:#f92672">=</span> get_lr(step)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> param_group <span style="color:#f92672">in</span> optimizer<span style="color:#f92672">.</span>param_groups:
</span></span><span style="display:flex;"><span>        param_group[<span style="color:#e6db74">&#39;lr&#39;</span>] <span style="color:#f92672">=</span> lr
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize() <span style="color:#75715e"># wait for GPU to finish work</span>
</span></span><span style="display:flex;"><span>    t1 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    dt <span style="color:#f92672">=</span> t1 <span style="color:#f92672">-</span> t0 <span style="color:#75715e"># time difference in seconds</span>
</span></span><span style="display:flex;"><span>    tokens_processed <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>B <span style="color:#f92672">*</span> train_loader<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> grad_accum_steps
</span></span><span style="display:flex;"><span>    tokens_per_sec <span style="color:#f92672">=</span> tokens_processed <span style="color:#f92672">/</span> dt
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>step<span style="color:#e6db74">:</span><span style="color:#e6db74">4d</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | loss: </span><span style="color:#e6db74">{</span>loss_accum<span style="color:#f92672">.</span>item()<span style="color:#e6db74">:</span><span style="color:#e6db74">.6f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | lr: </span><span style="color:#e6db74">{</span>lr<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | norm: </span><span style="color:#e6db74">{</span>norm<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | dt: </span><span style="color:#e6db74">{</span>dt<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">ms, tok/sec: </span><span style="color:#e6db74">{</span>tokens_per_sec<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output for micro batch size, B = 4</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># using device: cuda</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># total desired batch size: 524288</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># =&gt; calculated gradient accumulation steps: 128</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># loaded 338025 tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1 epoch = 82 batches</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># num decayed parameter tensors: 50, with 124,354,560 parameters</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># num non-decayed parameter tensors: 98, with 121,344 parameters</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># using fused AdamW: True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    0 | loss: 10.939045 | lr: 0.0001 | norm: 27.0274 | dt: 40582.00ms | tok/sec: 12919.22</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    1 | loss: 9.647425 | lr: 0.0001 | norm: 9.5036 | dt: 37164.44ms | tok/sec: 14107.25</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    2 | loss: 9.226212 | lr: 0.0002 | norm: 5.7323 | dt: 37072.59ms | tok/sec: 14142.20</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:   48 | loss: 5.837780 | lr: 0.0001 | norm: 0.1336 | dt: 36719.26ms | tok/sec: 14278.28</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:   49 | loss: 5.839324 | lr: 0.0001 | norm: 0.1318 | dt: 36749.97ms | tok/sec: 14266.35</span>
</span></span></code></pre></div><p>The code at this stage is extracted from <a href="https://raw.githubusercontent.com/karpathy/build-nanogpt/01be6b358941cb1c7561a56353423eba1cc7fe80/train_gpt2.py" target="_blank">train_gpt2.py</a>.</p>
<hr>
<h2 id="nanogpt">nanoGPT</h2>
<p><a href="https://github.com/karpathy/nanoGPT" target="_blank">nanoGPT</a> is an efficient, streamlined repository for training and fine-tuning medium-sized GPT models.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Clone the repository</span>
</span></span><span style="display:flex;"><span>git clone https://github.com/karpathy/nanoGPT.git
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Install required packages</span>
</span></span><span style="display:flex;"><span>pip install torch numpy transformers datasets tiktoken wandb tqdm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Prepare the dataset</span>
</span></span><span style="display:flex;"><span>python data/shakespeare_char/prepare.py
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># length of dataset in characters: 1,115,394</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># all the unique characters:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  !$&amp;&#39;,-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># vocab size: 65</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># train has 1,003,854 tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># val has 111,540 tokens</span>
</span></span></code></pre></div><h3 id="training-a-baby-gpt-model">Training a Baby GPT Model</h3>
<p>The training process for the baby GPT model on my setup:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python train.py config/train_shakespeare_char.py
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Sample Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iter 0: loss 4.2659, time 16.77s, mfu -100.00%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iter 10: loss 3.2412, time 0.75s, mfu 0.02%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iter 20: loss 2.7942, time 0.99s, mfu 0.07%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step 1750: train loss 1.1015, val loss 1.4632</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># saving checkpoint to out-shakespeare-char</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iter 1750: loss 1.1848, time 14.39s, mfu 0.36%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iter 4990: loss 0.8243, time 1.03s, mfu 0.36%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step 5000: train loss 0.6277, val loss 1.6922</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iter 5000: loss 0.8236, time 14.28s, mfu 0.36%</span>
</span></span></code></pre></div><p><img src="/images/train-llm2/train-llm2-baby-gpt-model.png" alt="train-llm2-baby-gpt-model"></p>
<p>Below are the modifications I applied to the repository:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>diff --git a/train.py b/train.py
</span></span><span style="display:flex;"><span>index 951bda9..b4b386c <span style="color:#ae81ff">100644</span>
</span></span><span style="display:flex;"><span>--- a/train.py
</span></span><span style="display:flex;"><span>+++ b/train.py
</span></span><span style="display:flex;"><span>@@ -205,6 +205,8 @@ checkpoint <span style="color:#f92672">=</span> None <span style="color:#75715e"># free up memory</span>
</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">if</span> compile:
</span></span><span style="display:flex;"><span>     print<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;compiling the model... (takes a ~minute)&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>     unoptimized_model <span style="color:#f92672">=</span> model
</span></span><span style="display:flex;"><span>+    import torch._dynamo
</span></span><span style="display:flex;"><span>+    torch._dynamo.config.suppress_errors <span style="color:#f92672">=</span> True
</span></span><span style="display:flex;"><span>     model <span style="color:#f92672">=</span> torch.compile<span style="color:#f92672">(</span>model<span style="color:#f92672">)</span> <span style="color:#75715e"># requires PyTorch 2.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> <span style="color:#75715e"># wrap model into DDP container</span>
</span></span><span style="display:flex;"><span>@@ -315,8 +317,6 @@ <span style="color:#66d9ef">while</span> True:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>     <span style="color:#75715e"># timing and logging</span>
</span></span><span style="display:flex;"><span>     t1 <span style="color:#f92672">=</span> time.time<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>-    dt <span style="color:#f92672">=</span> t1 - t0
</span></span><span style="display:flex;"><span>-    t0 <span style="color:#f92672">=</span> t1
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">if</span> iter_num % log_interval <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> and master_process:
</span></span><span style="display:flex;"><span>         <span style="color:#75715e"># get loss as float. note: this is a CPU-GPU sync point</span>
</span></span><span style="display:flex;"><span>         <span style="color:#75715e"># scale up to undo the division above, approximating the true total loss (exact would have been a sum)</span>
</span></span><span style="display:flex;"><span>@@ -324,7 +324,9 @@ <span style="color:#66d9ef">while</span> True:
</span></span><span style="display:flex;"><span>         <span style="color:#66d9ef">if</span> local_iter_num &gt;<span style="color:#f92672">=</span> 5: <span style="color:#75715e"># let the training loop settle a bit</span>
</span></span><span style="display:flex;"><span>             mfu <span style="color:#f92672">=</span> raw_model.estimate_mfu<span style="color:#f92672">(</span>batch_size * gradient_accumulation_steps, dt<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>             running_mfu <span style="color:#f92672">=</span> mfu <span style="color:#66d9ef">if</span> running_mfu <span style="color:#f92672">==</span> -1.0 <span style="color:#66d9ef">else</span> 0.9*running_mfu + 0.1*mfu
</span></span><span style="display:flex;"><span>-        print<span style="color:#f92672">(</span>f<span style="color:#e6db74">&#34;iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>+        dt <span style="color:#f92672">=</span> t1 - t0
</span></span><span style="display:flex;"><span>+        t0 <span style="color:#f92672">=</span> t1
</span></span><span style="display:flex;"><span>+        print<span style="color:#f92672">(</span>f<span style="color:#e6db74">&#34;iter {iter_num}: loss {lossf:.4f}, time {dt:.2f}s, mfu {running_mfu*100:.2f}%&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>     iter_num <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>     local_iter_num <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span></code></pre></div><h3 id="sampling-with-baby-gpt">Sampling with Baby GPT</h3>
<p>I ran a sampling script to test the model:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python sample.py --out_dir<span style="color:#f92672">=</span>out-shakespeare-char
</span></span></code></pre></div><p><img src="/images/train-llm2/train-llm2-sample-baby-gpt-model.png" alt="train-llm2-sample-baby-gpt-model"></p>
<h2 id="optional---integrating-with-mlflow">Optional - Integrating with MLflow</h2>
<p>For a detailed overview of setting up MLflow alongside Kubeflow, refer to my <a href="https://seehiong.github.io/archives/2024/integrating-mlflow-and-kubeflow-on-talos/" target="_blank">MLflow integration post</a>. Here’s an example of how I publish training metrics to MLflow:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># optimize!</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>configure_optimizers(weight_decay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">6e-4</span>, device<span style="color:#f92672">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> mlflow
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os 
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;MLFLOW_TRACKING_USERNAME&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;user&#39;</span> 
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;MLFLOW_TRACKING_PASSWORD&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;39VpDZdVLr&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mlflow<span style="color:#f92672">.</span>set_tracking_uri(uri<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://192.168.68.220&#34;</span>)
</span></span><span style="display:flex;"><span>mlflow<span style="color:#f92672">.</span>set_experiment(<span style="color:#e6db74">&#34;nanoGPT-shakespeare&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mlflow<span style="color:#f92672">.</span>log_param(<span style="color:#e6db74">&#34;Micro Batch Size&#34;</span>, B)
</span></span><span style="display:flex;"><span>mlflow<span style="color:#f92672">.</span>log_param(<span style="color:#e6db74">&#34;Block Size&#34;</span>, T)
</span></span><span style="display:flex;"><span>mlflow<span style="color:#f92672">.</span>log_param(<span style="color:#e6db74">&#34;Batch Size&#34;</span>, total_batch_size)
</span></span><span style="display:flex;"><span>mlflow<span style="color:#f92672">.</span>log_param(<span style="color:#e6db74">&#34;Gradient Accumulation Steps&#34;</span>, grad_accum_steps)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(max_steps):
</span></span><span style="display:flex;"><span>    t0 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    loss_accum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> micro_step <span style="color:#f92672">in</span> range(grad_accum_steps):
</span></span><span style="display:flex;"><span>        x, y <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>        x, y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        scaler <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>amp<span style="color:#f92672">.</span>GradScaler(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>autocast(device_type<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>bfloat16):
</span></span><span style="display:flex;"><span>            logits, loss <span style="color:#f92672">=</span> model(x, y)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># we have to scale the loss to account for gradient accumulation,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># because the gradients just add on each successive backward(),</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># addition of gradients corresponds to a SUM in the objective, but</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># instead of a SUM we want MEAN. Scale the loss here so it comes out right</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss <span style="color:#f92672">/</span> grad_accum_steps
</span></span><span style="display:flex;"><span>        loss_accum <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    norm <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># determine and set the learning rate for this iteration</span>
</span></span><span style="display:flex;"><span>    lr <span style="color:#f92672">=</span> get_lr(step)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> param_group <span style="color:#f92672">in</span> optimizer<span style="color:#f92672">.</span>param_groups:
</span></span><span style="display:flex;"><span>        param_group[<span style="color:#e6db74">&#39;lr&#39;</span>] <span style="color:#f92672">=</span> lr
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize() <span style="color:#75715e"># wait for GPU to finish work</span>
</span></span><span style="display:flex;"><span>    t1 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    dt <span style="color:#f92672">=</span> t1 <span style="color:#f92672">-</span> t0 <span style="color:#75715e"># time difference in seconds</span>
</span></span><span style="display:flex;"><span>    tokens_processed <span style="color:#f92672">=</span> train_loader<span style="color:#f92672">.</span>B <span style="color:#f92672">*</span> train_loader<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> grad_accum_steps
</span></span><span style="display:flex;"><span>    tokens_per_sec <span style="color:#f92672">=</span> tokens_processed <span style="color:#f92672">/</span> dt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    mlflow<span style="color:#f92672">.</span>log_metric(<span style="color:#e6db74">&#34;Loss&#34;</span>, loss_accum<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>    mlflow<span style="color:#f92672">.</span>log_metric(<span style="color:#e6db74">&#34;Learning Rate&#34;</span>, lr)
</span></span><span style="display:flex;"><span>    mlflow<span style="color:#f92672">.</span>log_metric(<span style="color:#e6db74">&#34;Norm&#34;</span>, norm)
</span></span><span style="display:flex;"><span>    mlflow<span style="color:#f92672">.</span>log_metric(<span style="color:#e6db74">&#34;Time per Iteration&#34;</span>, dt)
</span></span><span style="display:flex;"><span>    mlflow<span style="color:#f92672">.</span>log_metric(<span style="color:#e6db74">&#34;Tokens per Second&#34;</span>, tokens_per_sec)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;step: </span><span style="color:#e6db74">{</span>step<span style="color:#e6db74">:</span><span style="color:#e6db74">4d</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | loss: </span><span style="color:#e6db74">{</span>loss_accum<span style="color:#f92672">.</span>item()<span style="color:#e6db74">:</span><span style="color:#e6db74">.6f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | lr: </span><span style="color:#e6db74">{</span>lr<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | norm: </span><span style="color:#e6db74">{</span>norm<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> | dt: </span><span style="color:#e6db74">{</span>dt<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">ms | tok/sec: </span><span style="color:#e6db74">{</span>tokens_per_sec<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>For comparison purpose, these are my settings for the 2 runs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1st run</span>
</span></span><span style="display:flex;"><span>total_batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">262144</span>
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>T <span style="color:#f92672">=</span> <span style="color:#ae81ff">512</span>
</span></span><span style="display:flex;"><span>warmup_steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># GPU usage around 4821MiB / 6138MiB</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    0 | loss: 10.940472 | lr: 0.0001 | norm: 26.5947 | dt: 23285.35ms | tok/sec: 11257.89</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    1 | loss: 9.651522 | lr: 0.0001 | norm: 9.2193 | dt: 14724.37ms | tok/sec: 17803.41</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    2 | loss: 9.175361 | lr: 0.0002 | norm: 4.7185 | dt: 14595.11ms | tok/sec: 17961.08</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:   48 | loss: 5.898010 | lr: 0.0001 | norm: 0.2110 | dt: 15174.01ms | tok/sec: 17275.85</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:   49 | loss: 5.887230 | lr: 0.0001 | norm: 0.2155 | dt: 15391.68ms | tok/sec: 17031.53</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2nd run</span>
</span></span><span style="display:flex;"><span>total_batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">262144</span>
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>
</span></span><span style="display:flex;"><span>T <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>
</span></span><span style="display:flex;"><span>warmup_steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># GPU usage around 4821MiB / 6138MiB</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    0 | loss: 10.936398 | lr: 0.0001 | norm: 26.1147 | dt: 22972.23ms | tok/sec: 11411.34</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    1 | loss: 9.640186 | lr: 0.0001 | norm: 8.7945 | dt: 14762.45ms | tok/sec: 17757.48</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:    2 | loss: 9.145591 | lr: 0.0002 | norm: 3.6486 | dt: 14683.03ms | tok/sec: 17853.53</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:   48 | loss: 5.813196 | lr: 0.0001 | norm: 0.3081 | dt: 15273.17ms | tok/sec: 17163.70</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># step:   49 | loss: 5.800667 | lr: 0.0001 | norm: 0.2381 | dt: 15022.11ms | tok/sec: 17450.55</span>
</span></span></code></pre></div><p><img src="/images/train-llm2/train-llm2-mlflow-compare-runs.png" alt="train-llm2-mlflow-compare-runs"></p>

  
  <hr>
<div class="footer">
    
	    
            <a class="previous-post" href="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/?ref=footer"><span style="font-weight:bold;">« Previous</span><br>GPT-2 Setup and Pretraining Guide</a>
        
	    
            <div class="next-post">
                <a href="https://seehiong.github.io/archives/2024/exploring-ai-with-raspberry-pi-5/?ref=footer"><span style="font-weight:bold;">Next »</span><br>Exploring AI with Raspberry Pi 5</a>
            </div>
        
    
</div>

  
  <br>
<div class="comments">
    <script>
        const getStoredTheme = () => {
            const themeFromLS = localStorage.getItem("theme");
            const themeFromHugo = document.body.classList.contains("dark-theme") ? "dark" : "light";
            const currentTheme = themeFromLS ? themeFromLS : themeFromHugo;
            return currentTheme;
        };

        const setGiscusTheme = (theme) => {
            const sendMessage = (message) => {
                const iframe = document.querySelector('iframe.giscus-frame');
                if (iframe) {
                    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
                }
            };
            sendMessage({ setConfig: { theme } });
        };

        document.addEventListener("DOMContentLoaded", () => {
            setGiscusTheme(getStoredTheme());

            const giscusAttributes = {
                "src": "https://giscus.app/client.js",
                "data-repo": "seehiong/seehiong.github.io",
                "data-repo-id": "R_kgDOJg7Puw",
                "data-category": "General",
                "data-category-id": "DIC_kwDOJg7Pu84Cbyhc",
                "data-mapping": "url",
                "data-strict": "0",
                "data-reactions-enabled": "1",
                "data-emit-metadata": "0",
                "data-input-position": "top",
                "data-theme": getStoredTheme(),
                "data-lang": "en",
                "data-loading": "lazy",
                "crossorigin": "anonymous",
                "async": "",
            };

            
            const giscusScript = document.createElement("script");
            Object.entries(giscusAttributes).forEach(
                ([key, value]) => giscusScript.setAttribute(key, value));
            document.querySelector(".comments").appendChild(giscusScript);

            
            const themeSwitcher = document.querySelector(".btn-light-dark");
            if (themeSwitcher) {
                themeSwitcher.addEventListener("click", () => {
                    const currentTheme = getStoredTheme();
                    document.documentElement.classList.toggle("dark", currentTheme);
                    setGiscusTheme(currentTheme);
                });
            }
        });
    </script>
</div>
</div>
            </main>
            
  
    <div class="article-toc ">
    <div class="toc-wrapper">
      <h4 id="contents"></h4>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#training">Training</a></li>
    <li><a href="#training-performance-insights">Training Performance Insights</a>
      <ul>
        <li><a href="#understanding-the-hardware">Understanding the Hardware</a></li>
        <li><a href="#booast-training-performance">Booast Training Performance</a></li>
      </ul>
    </li>
    <li><a href="#nanogpt">nanoGPT</a>
      <ul>
        <li><a href="#training-a-baby-gpt-model">Training a Baby GPT Model</a></li>
        <li><a href="#sampling-with-baby-gpt">Sampling with Baby GPT</a></li>
      </ul>
    </li>
    <li><a href="#optional---integrating-with-mlflow">Optional - Integrating with MLflow</a></li>
  </ul>
</nav>

      
    </div>
</div>
  

        </div>
    </body>
</html>


<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>
