<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
    




    







<script defer language="javascript" type="text/javascript" src="/js/bundle.min.b989f0fdae3971a3effd81d6d0b27b3cb1023ff0b601ff43e2118b0d9d82ddb9.js"></script>





  
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-5PNPM4HWQ0"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-5PNPM4HWQ0');
        }
      </script>

    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    <link rel="icon" href=/favicon.png>

    
    





  





  
  
  


<!-- Open Graph image and Twitter Card metadata -->

<title itemprop="name">Tech Blog - GPT-2 Setup and Pretraining Guide</title>
<meta property="og:title" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Setup&#32;and&#32;Pretraining&#32;Guide />
<meta name="twitter:title" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Setup&#32;and&#32;Pretraining&#32;Guide />
<meta itemprop="name" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Setup&#32;and&#32;Pretraining&#32;Guide />
<meta name="application-name" content=Tech&#32;Blog&#32;-&#32;GPT-2&#32;Setup&#32;and&#32;Pretraining&#32;Guide />
<meta property="og:site_name" content="See Hiong&#39;s Blog" />


<meta name="description" content="" />
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />


<base href="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/" />
<link rel="canonical" href="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/" itemprop="url" />
<meta name="url" content="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/" />
<meta name="twitter:url" content="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/" />
<meta property="og:url" content="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/" />


<meta property="og:updated_time" content="2024-10-28T20:00:00&#43;08:00" />


<link rel="sitemap" type="application/xml" title="Sitemap" href='https://seehiong.github.io/sitemap.xml' />

<meta name="robots" content="index,follow" />
<meta name="googlebot" content="index,follow" />



<meta property="fb:admins" content="" />


<meta name="apple-mobile-web-app-title" content="See Hiong&#39;s Blog" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black" />






<meta name="generator" content="Hugo 0.134.1">


    
    

<link type="text/css" rel="stylesheet" href="/css/bundle.min.95cc0677a25232434df79a1cf0ee059633a97da88b4235a87f2a69bea2c1e24d.css">


    
    <style>
    body {
        --sidebar-bg-color: #202020;
        --sidebar-img-border-color: #515151;
        --sidebar-p-color: #909090;
        --sidebar-h1-color: #FFF;
        --sidebar-a-color: #FFF;
        --sidebar-socials-color: #FFF;
        --text-color: #222;
        --bkg-color: #FAF9F6;
        --post-title-color: #303030;
        --list-color: #5a5a5a;
        --link-color: #268bd2;
        --date-color: #515151;
        --table-border-color: #E5E5E5;
        --table-stripe-color: #F9F9F9;
        --code-color: #000;
        --code-background-color: #E5E5E5;
        --code-block-color: #fff;
        --code-block-background-color: #272822;
        --moon-sun-color: #FFF;
        --moon-sun-background-color: #515151;
    }
    body.dark-theme {
        --text-color: #eee;
        --bkg-color: #121212;
        --post-title-color: #DBE2E9;
        --list-color: #9d9d9d;
        --link-color: #268bd2;
        --date-color: #9a9a9a;
        --table-border-color: #515151;
        --table-stripe-color: #202020;
        --code-color: #fff;
        --code-background-color: #515151;
        --code-block-color: #fff;
        --code-block-background-color: #272822;
    }
    body {
        background-color: var(--bkg-color);
    }
</style>

</head>

    <body class="dark-theme">
        <div class="wrapper">
            <aside class="sidebar">
    <div class="container sidebar-sticky">
        <div class="light-dark" align="right">
    <button class="btn-light-dark" title="Toggle light/dark mode">
        <svg class="moon" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M6 .278a.768.768 0 0 1 .08.858a7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277c.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316a.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71C0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"/>
        </svg>
        <svg class="sun" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M8 12a4 4 0 1 0 0-8a4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"/>
        </svg>
    </button>
</div>

        <div class="sidebar-about">
    <h1 class="brand">
        
        
            <a href="https://seehiong.github.io/">
                <h1>Tech Blog</h1>
            </a>
        
    </h1>
    <p class="lead">
    Chronicling my personal expedition of technology exploration and experimentation, facilitated by a home-based self-configured laboratory
    </p>
</div>

        <link href="/pagefind/pagefind-ui.css" rel="stylesheet">
<style>
    :root {
        --pagefind-ui-primary: #eeeeee;
        --pagefind-ui-text: #eeeeee;
        --pagefind-ui-background: #152028;
        --pagefind-ui-border: #152028;
        --pagefind-ui-tag: #152028;
    }
</style>
<script src="/pagefind/pagefind-ui.js"></script>
<div style="max-width: 100%; margin: clamp(2px, 1vw, 10px) auto;">
    <div id="search"></div>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search", showSubResults: true, showImages: false });
        });
    </script>
</div>
        <nav>
    <ul class="sidebar-nav">

        
        
        
        
            

            
                
                
                    <li class="heading">
                        <a href="/2025/">2025</a>
                    </li>
                    
                        <li class="sub-heading">
                            Recent
                        </li>
                        
                            <li class="bullet">
                                <a href="https://seehiong.github.io/2025/jmc-java-performance-profiling-simplified/">JMC: Java Performance Profiling Simplified</a>
                            </li>
                        
                            <li class="bullet">
                                <a href="https://seehiong.github.io/2025/building-a-vuejs-frontend-for-combinatorial-optimization-problem/">Building a Vue.js Frontend for Combinatorial Optimization Problems</a>
                            </li>
                        
                    
                
            
                
                
            
                
                
            
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
        
        
            

            
                
                
            
                
                
            
                
                
                    <li class="heading">
                        <a href="/archives/">Archives</a>
                    </li>
                    
                
            
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
        
        
            

            
                
                
            
                
                
                    <li class="heading">
                        <a href="/about/">About</a>
                    </li>
                    
                
            
                
                
            
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
                
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
                        
                
            
        

    </ul>
</nav>

        
    <a target="_blank" class="social" title="GitHub" href="https://github.com/seehiong">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="-2 -2 24 24">
            <path fill="currentColor" d="M18.88 1.099C18.147.366 17.265 0 16.233 0H3.746C2.714 0 1.832.366 1.099 1.099C.366 1.832 0 2.714 0 3.746v12.487c0 1.032.366 1.914 1.099 2.647c.733.733 1.615 1.099 2.647 1.099H6.66c.19 0 .333-.007.429-.02a.504.504 0 0 0 .286-.169c.095-.1.143-.245.143-.435l-.007-.885c-.004-.564-.006-1.01-.006-1.34l-.3.052c-.19.035-.43.05-.721.046a5.555 5.555 0 0 1-.904-.091a2.026 2.026 0 0 1-.872-.39a1.651 1.651 0 0 1-.572-.8l-.13-.3a3.25 3.25 0 0 0-.41-.663c-.186-.243-.375-.407-.566-.494l-.09-.065a.956.956 0 0 1-.17-.156a.723.723 0 0 1-.117-.182c-.026-.061-.004-.111.065-.15c.07-.04.195-.059.378-.059l.26.04c.173.034.388.138.643.311a2.1 2.1 0 0 1 .631.677c.2.355.44.626.722.813c.282.186.566.28.852.28c.286 0 .533-.022.742-.065a2.59 2.59 0 0 0 .585-.196c.078-.58.29-1.028.637-1.34a8.907 8.907 0 0 1-1.333-.234a5.314 5.314 0 0 1-1.223-.507a3.5 3.5 0 0 1-1.047-.872c-.277-.347-.505-.802-.683-1.365c-.177-.564-.266-1.215-.266-1.952c0-1.049.342-1.942 1.027-2.68c-.32-.788-.29-1.673.091-2.652c.252-.079.625-.02 1.119.175c.494.195.856.362 1.086.5c.23.14.414.257.553.352a9.233 9.233 0 0 1 2.497-.338c.859 0 1.691.113 2.498.338l.494-.312a6.997 6.997 0 0 1 1.197-.572c.46-.174.81-.221 1.054-.143c.39.98.424 1.864.103 2.653c.685.737 1.028 1.63 1.028 2.68c0 .737-.089 1.39-.267 1.957c-.177.568-.407 1.023-.689 1.366a3.65 3.65 0 0 1-1.053.865c-.42.234-.828.403-1.223.507a8.9 8.9 0 0 1-1.333.235c.45.39.676 1.005.676 1.846v3.11c0 .147.021.266.065.357a.36.36 0 0 0 .208.189c.096.034.18.056.254.064c.074.01.18.013.318.013h2.914c1.032 0 1.914-.366 2.647-1.099c.732-.732 1.099-1.615 1.099-2.647V3.746c0-1.032-.367-1.914-1.1-2.647z"/>
        </svg>
    </a>



    <a target="_blank" class="social" title="LinkedIn" href="https://www.linkedin.com/in/seehiong">
        <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1.2em" viewBox="0 0 448 512">
            <path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5c0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7c-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5c67.2 0 79.7 44.3 79.7 101.9V416z"/>
        </svg>
    </a>















    <a target="_blank" class="social" title="RSS Feed" href="/2025/index.xml">
        <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1.2em" viewBox="0 0 1280.000000 1280.000000">
            <g transform="translate(0.000000,1280.000000) scale(0.100000,-0.100000)" fill="currentColor">
                <path d="M2295 11929 c-284 -12 -642 -45 -707 -65 -17 -5 -18 -63 -18 -1039 0 -569 4 -1036 8 -1039 5 -3 74 6 153 19 510 86 1168 95 1789 25 1348 -153 2602 -677 3670 -1531 385 -308 820 -744 1126 -1129 842 -1060 1362 -2313 1514 -3650 70 -621 61 -1279 -25 -1789 -13 -79 -22 -148 -19 -153 3 -4 471 -8 1039 -8 l1035 0 5 23 c51 225 85 942 67 1419 -23 605 -77 1044 -198 1617 -294 1400 -927 2734 -1823 3846 -1043 1295 -2364 2259 -3909 2854 -1158 447 -2451 656 -3707 600z"/>
                <path d="M2255 7845 c-269 -25 -620 -81 -667 -106 -17 -9 -18 -55 -18 -899 0 -706 3 -890 13 -890 6 0 66 18 132 41 130 44 288 79 467 105 154 21 577 30 749 15 1207 -107 2267 -823 2814 -1902 166 -327 268 -637 330 -1001 38 -227 48 -384 42 -662 -8 -348 -44 -590 -126 -831 -23 -66 -41 -126 -41 -132 0 -10 184 -13 890 -13 844 0 890 1 899 18 27 50 88 452 110 725 14 162 14 624 1 782 -59 703 -233 1323 -545 1945 -481 956 -1313 1788 -2270 2268 -620 310 -1239 483 -1940 542 -165 14 -669 10 -840 -5z"/>
                <path d="M2519 3815 c-391 -66 -725 -336 -868 -703 -79 -201 -96 -462 -45 -677 83 -344 338 -641 666 -774 116 -47 205 -69 330 -80 412 -39 811 153 1040 500 193 292 240 648 128 981 -135 403 -492 699 -914 757 -100 14 -241 12 -337 -4z"/>
            </g>
        </svg>
    </a>





        <p class="footnote">
powered by <a target="_blank" href="https://gohugo.io">Hugo</a> | themed with <a target="_blank" href="https://github.com/lukeorth/poison">poison</a>
    <br>
    &copy; 2025 See Hiong. All rights reserved.
</p>

  </div>
</aside>

            <main class="content container">
                <div class="post" data-pagefind-body>
  <div class="info">
  <h1 class="post-title">
    <a href="https://seehiong.github.io/archives/2024/gpt-2-setup-and-pretraining-guide/">GPT-2 Setup and Pretraining Guide</a>
  </h1>

  <div class="headline" data-pagefind-ignore>
    <div>
      
      
      <time datetime=" 2024-10-28T20:00:00&#43;0800" class="post-date">
        October 28, 2024
      </time>
      
      <span> - </span>
      <span class="reading-time">
        
          
        

        <span>12 mins read</span>
      </span>
    </div>

    
    <ul class="tags">
      
      <li class="tag-AI">
        <a href="https://seehiong.github.io/tags/ai">AI</a>
      </li>
      
      <li class="tag-GPT">
        <a href="https://seehiong.github.io/tags/gpt">GPT</a>
      </li>
      
      <li class="tag-PyTorch">
        <a href="https://seehiong.github.io/tags/pytorch">PyTorch</a>
      </li>
      
      <li class="tag-LLM">
        <a href="https://seehiong.github.io/tags/llm">LLM</a>
      </li>
      
      <li class="tag-nanoGPT">
        <a href="https://seehiong.github.io/tags/nanogpt">nanoGPT</a>
      </li>
      
    </ul>
    
  </div>

  
  
  <p class="author">
    Author: <i>See Hiong</i>
  </p>
  

  

  
</div>

  <p>In this post, I’ll document my journey in learning how to reproduce GPT-2 from scratch using my 6GB NVIDIA RTX A2000 GPU. This is my first attempt at training a model from scratch, and I’m excited to learn from the experts and share my experiences here.</p>
<h2 id="the-basics">The Basics</h2>
<p>I began my journey with the video <a href="https://www.youtube.com/watch?v=UU1WVnMk4E8" target="_blank">Create a Large Language Model from Scratch with Python</a> by <a href="https://github.com/Infatoshi" target="_blank">Elliot Arledge</a>. This video covers the fundamentals of large language models (LLMs) and demonstrates how to build one from the ground up. Here, I’ve documented the foundational concepts I extracted from the initial stages of this video.</p>
<h3 id="pytorch-basic-examples">PyTorch Basic Examples</h3>
<p>As part of this journey, I’m learning <a href="https://pytorch.org/" target="_blank">PyTorch</a>, an optimized tensor library for deep learning on GPUs and CPUs. In PyTorch, <strong>tensors</strong> are specialized data structures similar to arrays and matrices, with additional capabilities that make them suitable for deep learning.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;cuda&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;mps&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>mps<span style="color:#f92672">.</span>is_available()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Using </span><span style="color:#e6db74">{</span>device<span style="color:#e6db74">}</span><span style="color:#e6db74"> device&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: Using cuda device</span>
</span></span></code></pre></div><p>The shape of a tensor in PyTorch refers to its dimensions — the number of elements along each axis. For example, a tensor with shape (2, 3, 4) means:</p>
<ul>
<li>2 elements along the first axis (depth)</li>
<li>3 elements along the second axis (height)</li>
<li>4 elements along the third axis (width)</li>
</ul>
<p>Here are some of the torch functions:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>randint <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">100</span>, (<span style="color:#ae81ff">6</span>,))
</span></span><span style="display:flex;"><span>print(randint)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: tensor([-21,   0, -39, -71, -64, -60])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([[<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">1.2</span>], [<span style="color:#ae81ff">2.2</span>, <span style="color:#ae81ff">3.1</span>], [<span style="color:#ae81ff">4.9</span>, <span style="color:#ae81ff">5.2</span>]])
</span></span><span style="display:flex;"><span>print(tensor)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[0.1000, 1.2000],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [2.2000, 3.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [4.9000, 5.2000]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>zeros <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>print(zeros)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., 0.]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ones <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>print(ones)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[1., 1., 1.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#        [1., 1., 1.]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>empty(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>print(input)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[-1.1287e+28,  6.1223e-41, -1.1247e+28],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [ 6.1223e-41,  1.6678e+19,  7.0976e+22]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>arange <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>print(arange)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([0, 1, 2, 3, 4])</span>
</span></span></code></pre></div><p><img src="/images/train-llm/train-llm-torch-basics.png" alt="train-llm-torch-basics"></p>
<h3 id="measuring-time-taken">Measuring Time Taken</h3>
<p>By using the <strong>%%time</strong> magic command at the beginning of a cell, I can measure how long the entire cell takes to run, which helps track and optimize execution time.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">%%</span>time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>start_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>zeros <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>end_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>elapsed_time <span style="color:#f92672">=</span> end_time <span style="color:#f92672">-</span> start_time
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>elapsed_time<span style="color:#e6db74">:</span><span style="color:#e6db74">.8f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: 0.00000000 seconds</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># CPU times: total: 0 ns</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Wall time: 0 ns</span>
</span></span></code></pre></div><h3 id="additional-pytorch-features">Additional PyTorch Features</h3>
<p>Here are some additional PyTorch functions I explored, which I’ll use later in the model-building process. I also explored <em>tril</em>, <em>triu</em>, and <em>masked_fill</em> for manipulating tensor data, and transpose for altering tensor dimensions. These will be helpful for matrix operations and attention mechanisms.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Returns a tensor where each row contains num_samples indices sampled from the multinomial distribution located in the corresponding row of tensor input</span>
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.9</span>])
</span></span><span style="display:flex;"><span>samples <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(input, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, replacement<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>print(samples)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Concatenates the given sequence of tensors in tensors in the given dimension</span>
</span></span><span style="display:flex;"><span>tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>])
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((tensor, torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">5</span>])), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>print(out)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([1, 2, 3, 4, 5])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns the lower triangular part of the matrix (2-D tensor), the other elements of the result tensor out are set to 0</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tril(torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>print(out)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[1., 0., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [1., 1., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [1., 1., 1., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [1., 1., 1., 1., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [1., 1., 1., 1., 1.]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns the upper triangular part of a matrix (2-D tensor), the other elements of the result tensor out are set to 0</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>triu(torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>print(out)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[1., 1., 1., 1., 1.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 1., 1., 1., 1.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., 1., 1., 1.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., 0., 1., 1.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., 0., 0., 1.]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fills elements of self tensor with value, -inf where mask is True</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>)<span style="color:#f92672">.</span>masked_fill(torch<span style="color:#f92672">.</span>tril(torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>)) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>print(out)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[0., -inf, -inf, -inf, -inf],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., -inf, -inf, -inf],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., 0., -inf, -inf],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., 0., 0., -inf],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [0., 0., 0., 0., 0.]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns a tensor that is a transposed version of input wheret the given dimensions dim0 and dim1 are swapped</span>
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> input<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>print(out<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>print(out)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([4, 3, 2])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[[0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.]],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [[0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.]],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [[0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.]],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [[0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#          [0., 0.]]])</span>
</span></span></code></pre></div><h3 id="linear-transformations">Linear Transformations</h3>
<p>The linear layer in PyTorch applies an affine transformation, represented as \( y = xA^T + b \), where \( y \) is the output, \( x \) is the input, \( A \) is the weight matrix and \( b \) is the bias vector.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>sample <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">10.</span>, <span style="color:#ae81ff">10.</span>, <span style="color:#ae81ff">10.</span>])
</span></span><span style="display:flex;"><span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>print(linear)
</span></span><span style="display:flex;"><span>print(linear(sample))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: tensor([10., 10., 10.])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([-0.8503, -5.8184,  1.0190], grad_fn=&lt;SqueezeBackward4&gt;)</span>
</span></span></code></pre></div><h3 id="softmax">Softmax</h3>
<p>The softmax function rescales an input tensor so its elements lie between 0 and 1 and sum to 1. Softmax is defined as:</p>
$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
$$<p>and \( e \) is a mathematical constant, where \( e \approx 2.71828 \).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>tensor1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>])
</span></span><span style="display:flex;"><span>softmax_output <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(tensor1, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>print(softmax_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: tensor([0.0900, 0.2447, 0.6652])</span>
</span></span></code></pre></div><h3 id="embedding">Embedding</h3>
<p>An embedding layer stores dense representations of a fixed dictionary of words or indices.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Initialize an embedding layer</span>
</span></span><span style="display:flex;"><span>vocab_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>embedding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab_size, embedding_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create some input indices</span>
</span></span><span style="display:flex;"><span>input_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>LongTensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>print(input_indices)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Apply the embedding layer</span>
</span></span><span style="display:flex;"><span>embedded_output <span style="color:#f92672">=</span> embedding(input_indices)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The output will be a tensor of shape (4, 6), where 4 is the number of inputs</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># and 6 is the dimensionality of the embedding vectors</span>
</span></span><span style="display:flex;"><span>print(embedded_output<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>Size([<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>])
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([[ 1.1766,  1.3491, -0.2536,  0.5023,  0.4930,  0.3043],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [ 0.3194,  1.2871,  0.5535,  0.7847, -0.1497,  0.6422],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [-1.0683,  1.6570,  0.3645, -1.2519,  2.5594, -1.0523],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         [ 1.4452, -0.2749,  0.7373,  0.4051, -0.4702, -1.2839]],</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#        grad_fn=&lt;EmbeddingBackward0&gt;)</span>
</span></span></code></pre></div><h3 id="activation-functions-sigmoid-and-gelu">Activation Functions: Sigmoid and GELU</h3>
<h4 id="sigmoid">Sigmoid</h4>
<p>Sigmoid squashes the input into a range between 0 and 1. It is defined as:</p>
$$ \sigma(x) = \frac{1}{1 + e^x} $$<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>m <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> m(input)
</span></span><span style="display:flex;"><span>print(input)
</span></span><span style="display:flex;"><span>print(output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([ 0.8731, -0.2994])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([0.7054, 0.4257])</span>
</span></span></code></pre></div><h4 id="gelu">GELU</h4>
<p>GELU (Gaussian Error Linear Units) provides smoother activation, which can enhance model performance. It is defined as:</p>
$$
\text{GELU}(x) = x \cdot \Phi(x)
$$<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>m <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU()
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> m(input)
</span></span><span style="display:flex;"><span>print(input)
</span></span><span style="display:flex;"><span>print(output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([-0.4450, -0.4593])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tensor([-0.1460, -0.1484])</span>
</span></span></code></pre></div><h3 id="broadcasting-semantics">Broadcasting semantics</h3>
<p>PyTorch operations often support NumPy-style broadcasting, where tensor arguments expand to equal sizes automatically without copying data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([[<span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">3</span>]])
</span></span><span style="display:flex;"><span>print(y<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>(x<span style="color:#f92672">+</span>y)<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([3])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([3, 1])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([3, 3])</span>
</span></span></code></pre></div><h3 id="standard-deviation">Standard Deviation</h3>
<p>The standard deviation \( (\sigma) \) of a dataset \( X = \{x_1, x_2, \ldots, x_n\} \) is calculated as:</p>
$$ \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2} $$<p>This value indicates the average deviation of the data points from the mean, providing insight into the dataset’s spread.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>])
</span></span><span style="display:flex;"><span>mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(data)
</span></span><span style="display:flex;"><span>print(mean)
</span></span><span style="display:flex;"><span>squared_diff <span style="color:#f92672">=</span> (data <span style="color:#f92672">-</span> mean) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>print(squared_diff)
</span></span><span style="display:flex;"><span>sum_squared_diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(squared_diff)
</span></span><span style="display:flex;"><span>print(sum_squared_diff)
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> len(data)
</span></span><span style="display:flex;"><span>variance <span style="color:#f92672">=</span> sum_squared_diff <span style="color:#f92672">/</span> n
</span></span><span style="display:flex;"><span>std_dev <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(variance)
</span></span><span style="display:flex;"><span>print(std_dev)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Mean: 5.0</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Square Differences: [ 9.  1.  1.  1.  0.  0.  4. 16.]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sum of Squared Differences: 32.0</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Standard Deviation: 2.0</span>
</span></span></code></pre></div><hr>
<h2 id="reproducing-gpt-2-from-scratch">Reproducing GPT-2 from Scratch</h2>
<p>After successfully navigating the transformer fundamentals, I took on another challenge with <a href="https://github.com/karpathy" target="_blank">Andrej Karpathy</a>’s four-hour tutorial <a href="https://www.youtube.com/watch?v=l8pRSuU81PU" target="_blank">Let&rsquo;s reproduce GPT-2(124M)</a>. This section guides you through the model-building and weights-initialization process, and contrasts with the original architecture presented in the landmark paper <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a>, which combines encoder and decoder components. GPT-2, however, only utilizes the decoder segment, making it unique in structure and application.</p>
<p><img src="/images/train-llm/The-Transformer-model-architecture.png" alt="The-Transformer-model-architecture"></p>
<h3 id="loading-pretrained-gpt-2-weights">Loading Pretrained GPT-2 Weights</h3>
<p>The initial part of the tutorial goes over setting up a pretrained GPT-2 model using the <em>transformers</em> library, allowing us to see how the model behaves with the pretrained weights. Here’s the example provided by Andrej Karpathy, to initiate GPT-2 and generate text based on an input prompt:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> GPT2LMHeadModel
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_hf <span style="color:#f92672">=</span> GPT2LMHeadModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;gpt2&#34;</span>) <span style="color:#75715e">#124M</span>
</span></span><span style="display:flex;"><span>sd_hf <span style="color:#f92672">=</span> model_hf<span style="color:#f92672">.</span>state_dict()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> sd_hf<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>    print(k, v<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>wpe_weight <span style="color:#f92672">=</span> sd_hf[<span style="color:#e6db74">&#34;transformer.wpe.weight&#34;</span>]<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)[:<span style="color:#ae81ff">20</span>]
</span></span><span style="display:flex;"><span>print(wpe_weight)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> pipeline, set_seed
</span></span><span style="display:flex;"><span>generator <span style="color:#f92672">=</span> pipeline(<span style="color:#e6db74">&#39;text-generation&#39;</span>, model<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span>set_seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>generator(<span style="color:#e6db74">&#34;Hello, I&#39;m a language model,&#34;</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, num_return_sequences<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><p>This helped in understanding how the weights are structured and provides a baseline for further model modifications.</p>
<p><img src="/images/train-llm/train-llm-pretrained-gpt-2.png" alt="train-llm-pretrained-gpt-2"></p>
<h3 id="building-gpt-2-from-scratch">Building GPT-2 from Scratch</h3>
<p>Following the analysis of pretrained weights, Andrej Karpathy started constructing the model from the ground up. The main class includes components like CausalSelfAttention, which handles the attention mechanism, and a simple MLP layer to process the embeddings.</p>
<p>Here’s a snippet of the core setup, where each block layer is defined, and the pretrained weights are loaded into the model for text generation:</p>
<ol>
<li>The initialization:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> dataclasses <span style="color:#f92672">import</span> dataclass
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.nn <span style="color:#f92672">import</span> functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span></code></pre></div><ol start="2">
<li>The main GPT class, using the pretrained gpt2 (124M) weights:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CausalSelfAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> config<span style="color:#f92672">.</span>n_embd <span style="color:#f92672">%</span> config<span style="color:#f92672">.</span>n_head <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># key, query, value projections for all heads, but in a batch</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_attn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># output projection</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># regularization</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_head <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>n_head
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_embd <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>n_embd
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(<span style="color:#e6db74">&#34;bias&#34;</span>, torch<span style="color:#f92672">.</span>tril(torch<span style="color:#f92672">.</span>ones(config<span style="color:#f92672">.</span>block_size, config<span style="color:#f92672">.</span>block_size))
</span></span><span style="display:flex;"><span>                                    <span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, config<span style="color:#f92672">.</span>block_size, config<span style="color:#f92672">.</span>block_size))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B, T, C <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size() <span style="color:#75715e"># batch size, sequence length, embedding dimensionality (n_embd)</span>
</span></span><span style="display:flex;"><span>        qkv  <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_attn(x)
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> qkv<span style="color:#f92672">.</span>split(self<span style="color:#f92672">.</span>n_embd, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> v<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> (q <span style="color:#f92672">@</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(k<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)))
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> att<span style="color:#f92672">.</span>masked_fill(self<span style="color:#f92672">.</span>bias[:,:,:T,:T] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(att, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> att <span style="color:#f92672">@</span> v <span style="color:#75715e"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, T, C) <span style="color:#75715e"># re-assemble all head outputs side by side</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># output projection</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(y)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MLP</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_fc    <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gelu    <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tanh&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj  <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_fc(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gelu(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Block</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln_1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> CausalSelfAttention(config)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mlp <span style="color:#f92672">=</span> MLP(config)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>attn(self<span style="color:#f92672">.</span>ln_1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>mlp(self<span style="color:#f92672">.</span>ln_2(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPTConfig</span>:
</span></span><span style="display:flex;"><span>    block_size: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>
</span></span><span style="display:flex;"><span>    vocab_size: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">50257</span>
</span></span><span style="display:flex;"><span>    n_layer: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>    n_head: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>    n_embd: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">768</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transformer <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict(dict(
</span></span><span style="display:flex;"><span>            wte <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(config<span style="color:#f92672">.</span>vocab_size, config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>            wpe <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(config<span style="color:#f92672">.</span>block_size, config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>            h <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([Block(config) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(config<span style="color:#f92672">.</span>n_layer)]),
</span></span><span style="display:flex;"><span>            ln_f <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>        ))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>vocab_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, idx):
</span></span><span style="display:flex;"><span>        B, T <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> T <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Cannot forward sequence of length </span><span style="color:#e6db74">{</span>t<span style="color:#e6db74">}</span><span style="color:#e6db74">, block size is only </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, T, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>idx<span style="color:#f92672">.</span>device) <span style="color:#75715e"># shape (t)</span>
</span></span><span style="display:flex;"><span>        pos_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wpe(pos) <span style="color:#75715e"># position embeddings of shape (t, n_embd)</span>
</span></span><span style="display:flex;"><span>        tok_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte(idx) <span style="color:#75715e"># token embeddings of shape (b, t, n_embd)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tok_emb <span style="color:#f92672">+</span> pos_emb
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> block(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>ln_f(x)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@classmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">from_pretrained</span>(cls, model_type):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> model_type <span style="color:#f92672">in</span> {<span style="color:#e6db74">&#39;gpt2&#39;</span>, <span style="color:#e6db74">&#39;gpt2-medium&#39;</span>, <span style="color:#e6db74">&#39;gpt2-large&#39;</span>, <span style="color:#e6db74">&#39;gpt2-xl&#39;</span>}
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> GPT2LMHeadModel
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;loading weights from pretrained gpt: </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> model_type)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># n_layer, n_head and n_embd are determined from model_type</span>
</span></span><span style="display:flex;"><span>        config_args <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;gpt2&#39;</span>:         dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>),  <span style="color:#75715e"># 124M params</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;gpt2-medium&#39;</span>:  dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">24</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>), <span style="color:#75715e"># 350M params</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;gpt2-large&#39;</span>:   dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">36</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">1280</span>), <span style="color:#75715e"># 774M params</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;gpt2-xl&#39;</span>:      dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">48</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">25</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">1600</span>), <span style="color:#75715e"># 1558M params</span>
</span></span><span style="display:flex;"><span>        }[model_type]
</span></span><span style="display:flex;"><span>        config_args[<span style="color:#e6db74">&#39;vocab_size&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">50257</span> <span style="color:#75715e"># always 50257 for GPT model checkpoints</span>
</span></span><span style="display:flex;"><span>        config_args[<span style="color:#e6db74">&#39;block_size&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span> <span style="color:#75715e"># always 1024 for GPT model checkpoints      </span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># create a from-scratch initialized minGPT model</span>
</span></span><span style="display:flex;"><span>        config <span style="color:#f92672">=</span> GPTConfig(<span style="color:#f92672">**</span>config_args)
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> GPT(config)
</span></span><span style="display:flex;"><span>        sd <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>state_dict()
</span></span><span style="display:flex;"><span>        sd_keys <span style="color:#f92672">=</span> sd<span style="color:#f92672">.</span>keys()
</span></span><span style="display:flex;"><span>        sd_keys <span style="color:#f92672">=</span> [k <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> sd_keys <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> k<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;.attn.bias&#39;</span>)] <span style="color:#75715e"># discard this mask / buffer, not a param</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># init a huggingface/transformers model</span>
</span></span><span style="display:flex;"><span>        model_hf <span style="color:#f92672">=</span> GPT2LMHeadModel<span style="color:#f92672">.</span>from_pretrained(model_type)
</span></span><span style="display:flex;"><span>        sd_hf <span style="color:#f92672">=</span> model_hf<span style="color:#f92672">.</span>state_dict()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># copy while ensuring all of the parameters are aligned and match in names and shapes</span>
</span></span><span style="display:flex;"><span>        sd_keys_hf <span style="color:#f92672">=</span> sd_hf<span style="color:#f92672">.</span>keys()
</span></span><span style="display:flex;"><span>        sd_keys_hf <span style="color:#f92672">=</span> [k <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> sd_keys_hf <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> k<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;.attn.masked_bias&#39;</span>)] <span style="color:#75715e"># ignore these, just a buffer</span>
</span></span><span style="display:flex;"><span>        sd_keys_hf <span style="color:#f92672">=</span> [k <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> sd_keys_hf <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> k<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;.attn.bias&#39;</span>)] <span style="color:#75715e"># same, just the mask (buffer)</span>
</span></span><span style="display:flex;"><span>        transposed <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;attn.c_attn.weight&#39;</span>, <span style="color:#e6db74">&#39;attn.c_proj.weight&#39;</span>, <span style="color:#e6db74">&#39;mlp.c_fc.weight&#39;</span>, <span style="color:#e6db74">&#39;mlp.c_proj.weight&#39;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># basically the openai checkpoints use a &#34;Conv1D&#34; module, but we only want to use a vanilla Linear</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># this means that we have to transpose these weights when we import them</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> len(sd_keys_hf) <span style="color:#f92672">==</span> len(sd_keys), <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;mismatched keys: </span><span style="color:#e6db74">{</span>len(sd_keys_hf)<span style="color:#e6db74">}</span><span style="color:#e6db74"> != </span><span style="color:#e6db74">{</span>len(sd_keys)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> sd_keys_hf:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> any(k<span style="color:#f92672">.</span>endswith(w) <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> transposed):
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># special treatment for the Conv1D weights we need to transpose</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">assert</span> sd_hf[k]<span style="color:#f92672">.</span>shape[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> sd[k]<span style="color:#f92672">.</span>shape, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Shape mismatch for </span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>sd_hf[k]<span style="color:#f92672">.</span>shape[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74"> vs </span><span style="color:#e6db74">{</span>sd[k]<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>                    sd[k]<span style="color:#f92672">.</span>copy_(sd_hf[k]<span style="color:#f92672">.</span>t())
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># vanilla copy over the other parameters</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">assert</span> sd_hf[k]<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> sd[k]<span style="color:#f92672">.</span>shape, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Shape mismatch for </span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>sd_hf[k]<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74"> vs </span><span style="color:#e6db74">{</span>sd[k]<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>                    sd[k]<span style="color:#f92672">.</span>copy_(sd_hf[k])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> model
</span></span></code></pre></div><ol start="3">
<li>Text generation for the main GPT class using the pretrained weights:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_return_sequences <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>max_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tiktoken
</span></span><span style="display:flex;"><span>enc <span style="color:#f92672">=</span> tiktoken<span style="color:#f92672">.</span>get_encoding(<span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;Hello, I&#39;m a language model,&#34;</span>)
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(tokens, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> tokens<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>repeat(num_return_sequences, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> tokens<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">while</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">&lt;</span> max_length:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> model(x)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> logits[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        topk_probs, topk_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(probs, <span style="color:#ae81ff">50</span>, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        ix <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(topk_probs, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        xcol <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>gather(topk_indices, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, ix)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((x, xcol), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_return_sequences):
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> x[i, :max_length]<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>    decoded <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>decode(tokens)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;&gt;&#34;</span>, decoded)
</span></span></code></pre></div><p><img src="/images/train-llm/train-llm-loading-weights-from-pretrained-gpt-2.png" alt="train-llm-loading-weights-from-pretrained-gpt-2"></p>
<hr>
<h2 id="wrapping-up-the-pretrained-model-setup">Wrapping Up the Pretrained Model Setup</h2>
<p>In this post, we&rsquo;ve explored the foundational setup for loading pretrained GPT-2 weights and performing basic text generation. From analyzing the pretrained model structure to initializing and modifying key parameters, this approach sets the stage for a deeper dive into the intricacies of GPT-2. We&rsquo;ve also covered the essentials for leveraging existing weights to understand and test the model&rsquo;s capabilities.</p>
<p>In the next part, we&rsquo;ll continue following Andrej Karpathy&rsquo;s video as he takes the GPT-2 model through hands-on training with actual datasets. We&rsquo;ll start with a small sample text to see the practical aspects of text generation training, followed by scaling to larger datasets.</p>
<hr>
<h2 id="optional---generate-model-from-scratch">Optional - Generate Model from Scratch</h2>
<p>To create a model entirely from scratch, we can leverage PyTorch’s default random initialization rather than loading GPT-2’s pretrained weights.</p>
<p>The only modification needed in the configuration is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># model = GPT.from_pretrained(&#39;gpt2&#39;)</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT(GPTConfig())
</span></span></code></pre></div><p>This setup will allow us to train a randomly initialized GPT model, shown below.</p>
<p><img src="/images/train-llm/train-llm-random-model.png" alt="train-llm-random-model"></p>

  
  <hr>
<div class="footer">
    
	    
            <a class="previous-post" href="https://seehiong.github.io/archives/2024/integrating-mlflow-and-kubeflow-on-talos/?ref=footer"><span style="font-weight:bold;">« Previous</span><br>Integrating MLflow and Kubeflow on Talos</a>
        
	    
            <div class="next-post">
                <a href="https://seehiong.github.io/archives/2024/gpt-2-training-guide/?ref=footer"><span style="font-weight:bold;">Next »</span><br>GPT-2 Training Guide</a>
            </div>
        
    
</div>

  
  <br>
<div class="comments">
    <script>
        const getStoredTheme = () => {
            const themeFromLS = localStorage.getItem("theme");
            const themeFromHugo = document.body.classList.contains("dark-theme") ? "dark" : "light";
            const currentTheme = themeFromLS ? themeFromLS : themeFromHugo;
            return currentTheme;
        };

        const setGiscusTheme = (theme) => {
            const sendMessage = (message) => {
                const iframe = document.querySelector('iframe.giscus-frame');
                if (iframe) {
                    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
                }
            };
            sendMessage({ setConfig: { theme } });
        };

        document.addEventListener("DOMContentLoaded", () => {
            setGiscusTheme(getStoredTheme());

            const giscusAttributes = {
                "src": "https://giscus.app/client.js",
                "data-repo": "seehiong/seehiong.github.io",
                "data-repo-id": "R_kgDOJg7Puw",
                "data-category": "General",
                "data-category-id": "DIC_kwDOJg7Pu84Cbyhc",
                "data-mapping": "url",
                "data-strict": "0",
                "data-reactions-enabled": "1",
                "data-emit-metadata": "0",
                "data-input-position": "top",
                "data-theme": getStoredTheme(),
                "data-lang": "en",
                "data-loading": "lazy",
                "crossorigin": "anonymous",
                "async": "",
            };

            
            const giscusScript = document.createElement("script");
            Object.entries(giscusAttributes).forEach(
                ([key, value]) => giscusScript.setAttribute(key, value));
            document.querySelector(".comments").appendChild(giscusScript);

            
            const themeSwitcher = document.querySelector(".btn-light-dark");
            if (themeSwitcher) {
                themeSwitcher.addEventListener("click", () => {
                    const currentTheme = getStoredTheme();
                    document.documentElement.classList.toggle("dark", currentTheme);
                    setGiscusTheme(currentTheme);
                });
            }
        });
    </script>
</div>
</div>
            </main>
            
  
    <div class="article-toc ">
    <div class="toc-wrapper">
      <h4 id="contents"></h4>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#the-basics">The Basics</a>
      <ul>
        <li><a href="#pytorch-basic-examples">PyTorch Basic Examples</a></li>
        <li><a href="#measuring-time-taken">Measuring Time Taken</a></li>
        <li><a href="#additional-pytorch-features">Additional PyTorch Features</a></li>
        <li><a href="#linear-transformations">Linear Transformations</a></li>
        <li><a href="#softmax">Softmax</a></li>
        <li><a href="#embedding">Embedding</a></li>
        <li><a href="#activation-functions-sigmoid-and-gelu">Activation Functions: Sigmoid and GELU</a></li>
        <li><a href="#broadcasting-semantics">Broadcasting semantics</a></li>
        <li><a href="#standard-deviation">Standard Deviation</a></li>
      </ul>
    </li>
    <li><a href="#reproducing-gpt-2-from-scratch">Reproducing GPT-2 from Scratch</a>
      <ul>
        <li><a href="#loading-pretrained-gpt-2-weights">Loading Pretrained GPT-2 Weights</a></li>
        <li><a href="#building-gpt-2-from-scratch">Building GPT-2 from Scratch</a></li>
      </ul>
    </li>
    <li><a href="#wrapping-up-the-pretrained-model-setup">Wrapping Up the Pretrained Model Setup</a></li>
    <li><a href="#optional---generate-model-from-scratch">Optional - Generate Model from Scratch</a></li>
  </ul>
</nav>

      
    </div>
</div>
  

        </div>
    </body>
</html>


<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>
