<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OKE on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/oke/</link>
    <description>Recent content in OKE on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 May 2025 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/oke/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploying KServe on OKE</title>
      <link>https://seehiong.github.io/2025/deploy-kserve-on-oke/</link>
      <pubDate>Mon, 12 May 2025 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2025/deploy-kserve-on-oke/</guid>
      <description>This post demonstrated how to deploy an XGBoost model using KServe on Oracle Kubernetes Engine (OKE). Starting from model upload to Object Storage, we served the model via KServe and exposed it through Istio Gateway. Instead of deploying the frontend in-cluster, we built a Streamlit app hosted on Streamlit Community Cloud, which sends requests to the public inference endpoint. This end-to-end setup showcases a scalable and cloud-native ML deployment pipeline on OCI, separating model serving and user interface layers for flexibility and ease of maintenance.</description>
    </item>
  </channel>
</rss>
