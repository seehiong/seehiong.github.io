<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/ai/</link>
    <description>Recent content in AI on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 13 Sep 2025 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Blender Meets MCP</title>
      <link>https://seehiong.github.io/posts/2025/09/blender-meets-mcp/</link>
      <pubDate>Sat, 13 Sep 2025 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2025/09/blender-meets-mcp/</guid>
      <description>A step-by-step guide to integrating the Model Context Protocol (MCP) with Blender. From setup and server connections to building a datacenter model, discover how AI-driven workflows enhance 3D design and creativity.</description>
    </item>
    <item>
      <title>Reverse-Engineering PDFs with AI Tools</title>
      <link>https://seehiong.github.io/posts/2025/08/reverse-engineering-pdfs-with-ai-tools/</link>
      <pubDate>Sun, 31 Aug 2025 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2025/08/reverse-engineering-pdfs-with-ai-tools/</guid>
      <description>What started as a corrupted PDF upload led to a deep exploration of PDF internals using tools like qpdf, PyMuPDF, and Grok Code Fast 1. This post walks through reconstructing broken files, extracting embedded images, and pulling structured data‚Äîall while testing the capabilities of AI-powered tools for reverse engineering. A practical guide for anyone curious about what‚Äôs really inside a PDF.</description>
    </item>
    <item>
      <title>Porting Llama3.java to Micronaut</title>
      <link>https://seehiong.github.io/posts/2024/12/porting-llama3.java-to-micronaut/</link>
      <pubDate>Sun, 15 Dec 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/12/porting-llama3.java-to-micronaut/</guid>
      <description>This project ports the original single-file llama3.java by Alfonso¬≤ Peterssen into a modular Micronaut application, transforming it from a console app to a stream-based, production-ready API. The internals and performance of the original GGUF-format LLM remain unchanged. Updates focus on restructuring the codebase into logical packages and adapting it to Micronaut‚Äôs ecosystem, enabling easier integration with Java microservices. This streamlined design retains the original‚Äôs simplicity while enhancing scalability and usability for modern AI applications.</description>
    </item>
    <item>
      <title>Porting Llama2.java to Micronaut</title>
      <link>https://seehiong.github.io/posts/2024/12/porting-llama2.java-to-micronaut/</link>
      <pubDate>Sat, 07 Dec 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/12/porting-llama2.java-to-micronaut/</guid>
      <description>This post explores porting the single-file llama2.java into a robust Micronaut application, demonstrating both JDK and GraalVM native mode performance. While GraalVM offers faster startup (56ms), its serving throughput is slower (210 tokens/sec). For high-performance inference, JDK mode is preferred, but GraalVM shines for lightweight, startup-critical scenarios. The journey highlights Micronaut&amp;rsquo;s ease of integration for AI applications.</description>
    </item>
    <item>
      <title>Building an AI Knowledge Assistant</title>
      <link>https://seehiong.github.io/posts/2024/11/building-an-ai-knowledge-assistant/</link>
      <pubDate>Sun, 24 Nov 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/11/building-an-ai-knowledge-assistant/</guid>
      <description>Learn how to build an AI-powered knowledge assistant using Python. This guide covers data ingestion from sources like PDFs, deploying a FastAPI-based API, and querying the assistant for detailed answers. Using advanced retrieval mechanisms, the assistant provides contextually relevant responses. You&amp;rsquo;ll also explore a working example with the CQRS design pattern. The complete project code is available on &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://github.com/seehiong/ai-knowledge-assistant&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;GitHub&lt;/a&gt;&#xD;&#xA;.</description>
    </item>
    <item>
      <title>Exploring AI with Raspberry Pi 5</title>
      <link>https://seehiong.github.io/posts/2024/11/exploring-ai-with-raspberry-pi-5/</link>
      <pubDate>Sat, 09 Nov 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/11/exploring-ai-with-raspberry-pi-5/</guid>
      <description>In this post, I dive into the powerful capabilities of the Raspberry Pi 5 paired with the Hailo-8L AI Kit, a neural network accelerator offering 13 TOPS. After setting up the system and camera, I explore object detection, pose estimation, and instance segmentation, showcasing the Pi‚Äôs impressive AI potential. Whether using pre-trained models or custom configurations, this compact setup proves to be a versatile tool for AI enthusiasts and developers alike. It&amp;rsquo;s a fun, hands-on introduction to the world of edge AI on a budget-friendly platform.</description>
    </item>
    <item>
      <title>GPT-2 Training Guide</title>
      <link>https://seehiong.github.io/posts/2024/10/gpt-2-training-guide/</link>
      <pubDate>Thu, 31 Oct 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/10/gpt-2-training-guide/</guid>
      <description>This post documents my journey training GPT-2 on the Tiny Shakespeare dataset, inspired by Andrej Karpathy&amp;rsquo;s instructional video and nanoGPT repository. Following each step, I detail the setup process, from encoding data and optimizing the model with AdamW, to improving training stability with mixed precision and flash attention. The post includes practical insights on using pretrained weights, weight sharing, and efficient data handling, concluding with sample outputs from training and sampling a ‚Äúbaby‚Äù GPT model.</description>
    </item>
    <item>
      <title>GPT-2 Setup and Pretraining Guide</title>
      <link>https://seehiong.github.io/posts/2024/10/gpt-2-setup-and-pretraining-guide/</link>
      <pubDate>Mon, 28 Oct 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/10/gpt-2-setup-and-pretraining-guide/</guid>
      <description>This guide explores reproducing GPT-2 (124M) using Andrej Karpathy‚Äôs video walkthrough. It begins with an overview of the GPT-2 architecture, a decoder-only transformer model inspired by &amp;ldquo;Attention Is All You Need.&amp;rdquo; Using pretrained GPT-2 weights, we analyze and initialize a custom GPT class, with detailed steps to handle token embeddings, causal attention, and layer normalization. The guide includes code for generating text from pretrained weights. In the next segment, we‚Äôll continue with a deeper dive into dataset preparation and training from scratch, moving from small samples to large-scale training.</description>
    </item>
    <item>
      <title>Building Advanced RAG Applications with MyScaleDB and LlamaIndex</title>
      <link>https://seehiong.github.io/posts/2024/06/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</link>
      <pubDate>Sat, 15 Jun 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/06/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</guid>
      <description>Explore how to build advanced Retrieval-Augmented Generation (RAG) applications using MyScaleDB and LlamaIndex. This guide covers the installation of necessary tools, setting up a virtual environment, and creating an index for document categorization. Learn how to execute simple and filtered queries, and troubleshoot common issues. Enhance your understanding of integrating high-performance SQL vector databases with cutting-edge data frameworks for efficient LLM applications.</description>
    </item>
    <item>
      <title>Coding with CrewAI: AI Orchestration Simplified</title>
      <link>https://seehiong.github.io/posts/2024/03/coding-with-crewai-ai-orchestration-simplified/</link>
      <pubDate>Fri, 29 Mar 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/03/coding-with-crewai-ai-orchestration-simplified/</guid>
      <description>Explore CrewAI, a pioneering framework streamlining AI agent orchestration. Discover practical applications, from Jan and LM Studio integration to Serper API utilization. Follow along as we delve into coding with CrewAI, showcasing its versatility in crafting resumes and more. Experience the seamless synergy of autonomous AI agents, revolutionizing workflows with efficiency and innovation. Unlock the power of CrewAI, propelling your projects to new heights in artificial intelligence.</description>
    </item>
    <item>
      <title>Text-to-Image with StableDiffusionPipeline</title>
      <link>https://seehiong.github.io/posts/2024/02/text-to-image-with-stablediffusionpipeline/</link>
      <pubDate>Sat, 10 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/02/text-to-image-with-stablediffusionpipeline/</guid>
      <description>In this post, we explore the capabilities of StableDiffusionPipeline for generating photorealistic images from textual inputs. We start with setting up the environment and installing necessary libraries. Then, we dive into Textual Inversion, demonstrating how the model learns new concepts from images. Image-to-Image transformations are also explored, showcasing the pipeline&amp;rsquo;s versatility. Additionally, we introduce Animagine XL 2.0, a model for high-resolution anime image creation, and provide sample code for its implementation. Lastly, we highlight Stable Diffusion XL, a powerful text-to-image model, and share a festive image generated using it.</description>
    </item>
    <item>
      <title>Stable Diffusion: Text-to-Image Modeling Journey</title>
      <link>https://seehiong.github.io/posts/2024/02/stable-diffusion-text-to-image-modeling-journey/</link>
      <pubDate>Sat, 03 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/02/stable-diffusion-text-to-image-modeling-journey/</guid>
      <description>This post explores Stable Diffusion, a latent text-to-image diffusion model in machine learning. Diffusion models, with forward, reverse, and sampling components, understand and generate patterns in datasets. Illustrating applications in image tasks, it introduces the process of installing and utilizing Stable Diffusion. The post details image generation and modification using prompts, with examples and troubleshooting. Notably, it shares an encounter with CUDA out-of-memory errors and the resolution through image resizing. Overall, it offers a comprehensive guide, combining theoretical insights with practical implementation steps in a professional manner.</description>
    </item>
    <item>
      <title>OpenVINO, Optimum-Intel, CPU: An Exploration in Model Optimization</title>
      <link>https://seehiong.github.io/posts/2024/01/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</link>
      <pubDate>Sat, 27 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</guid>
      <description>Explore the convergence of OpenVINO and Optimum-Intel in this post, where I detail the setup and execution of example code on my aging laptop. Focused on applying Quantization-aware Training and the Token Merging method to optimize the UNet model within the Stable Diffusion pipeline, this journey showcases the synergy of open-source tools for deep learning model deployment. Note that the provided code is tailored for CPU-based inference due to limitations in my aging GeForce graphics card, making it a valuable resource for users with similar hardware constraints. Dive into the world of optimized models and delightful Pokemon creation!</description>
    </item>
    <item>
      <title>Exploring Autogen Studio</title>
      <link>https://seehiong.github.io/posts/2024/01/exploring-autogen-studio/</link>
      <pubDate>Sun, 14 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/exploring-autogen-studio/</guid>
      <description>In this exploration of Autogen Studio, we navigated through the AI landscape, harnessing the LM Studio API to compare responses from diverse language models. Employing the Mistral Instruct 7B model, we scrutinized prompts like Stock Price and Paint, visualizing outcomes and delving into key configurations. The post also offered insights into the primary assistant, model configuration, and agent workflows, accompanied by a comparative analysis of Mistral model responses. This comprehensive journey demystifies the power of Autogen Studio and its seamless integration with LM Studio API, providing practical guidance for AI enthusiasts.</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/posts/2024/01/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>In this post, we explored deploying Lightweight Language Models (LLMs) using WasmEdge, a high-performance WebAssembly runtime, within a HomeLab environment. The process involved preparing an OpenAI-compatible API server, configuring the Wasi-NN plugin, and deploying the setup to HomeLab using Kubernetes (K3s). The post also detailed the steps for testing the API server and integrating it into a Java application. Overall, the guide provides a comprehensive walkthrough of hosting and utilizing LLMs with WasmEdge in a local environment.</description>
    </item>
    <item>
      <title>Integration of Kong into AI Workflow</title>
      <link>https://seehiong.github.io/posts/2024/01/integration-of-kong-into-ai-workflow/</link>
      <pubDate>Sat, 06 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/integration-of-kong-into-ai-workflow/</guid>
      <description>This comprehensive guide navigates through configuring Kong OSS and Kong Ingress Controller (KIC), seamlessly integrating Kong into an AI workflow. Starting with Kong OSS configuration, the tutorial covers updating environment variables and service ports. The Langchain4j application is then adapted to leverage Kong API, allowing for flexible path-based APIs. Additionally, potential timeout issues are addressed. The guide concludes with a demonstration of Kong Ingress Controller configuration, emphasizing optimal settings for specific use cases. Whether through Kong OSS or KIC, readers gain insights into enhancing API management and integration within their AI workflows.</description>
    </item>
    <item>
      <title>Unveiling Agent AutoBuild in Autogen</title>
      <link>https://seehiong.github.io/posts/2023/12/unveiling-agent-autobuild-in-autogen/</link>
      <pubDate>Sun, 17 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/unveiling-agent-autobuild-in-autogen/</guid>
      <description>In this blog, I explored Autogen&amp;rsquo;s Agent AutoBuild and experimented with the Mixtral 8x7B model. I configured Autogen, envisioning a software academy project for coding novices. Through code snippets, I showcased AutoBuild&amp;rsquo;s multi-agent system creation and tailored a task that wrote a General Paper article on art and courage. The Mixtral 8x7B model in LM Studio brought excitement but posed challenges with duplicate content. Check out the blog for a firsthand look at the dynamic interplay between Autogen and cutting-edge AI, complete with code snippets and images.</description>
    </item>
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>In this post, I explored enhancing Autogen&amp;rsquo;s capabilities by enabling seamless Java code execution. Drawing inspiration from 0xlws&amp;rsquo; fork supporting JavaScript, I embarked on modifying Autogen to robustly support Java. I detailed the setup process, including installing Java on Windows Subsystem for Linux (WSL) and modifying key files. The post includes code snippets showcasing the changes, recompilation steps, and instructions for generating Java code. I extended functionality to additional test cases, seamlessly switching between Java and Python code execution. Docker integration for Java code execution was also optimized, showcasing Autogen&amp;rsquo;s versatility and robust development experience.</description>
    </item>
    <item>
      <title>Multi-agent Conservation with Autogen</title>
      <link>https://seehiong.github.io/posts/2023/12/multi-agent-conservation-with-autogen/</link>
      <pubDate>Fri, 08 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/multi-agent-conservation-with-autogen/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://seehiong.github.io/posts/2023/12/exploring-autogen-with-lm-studio-and-local-llm/&#34; &gt;Exploration with Autogen&lt;/a&gt;&#xD;&#xA; and following the example of &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;Automated Multi Agent Chat&lt;/a&gt;&#xD;&#xA;, we&amp;rsquo;ll delve into the steps to create a dynamic debate environment.&lt;/p&gt;&#xA;&lt;h2 id=&#34;agent-setup&#34;&gt;Agent Setup&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll be setting up two agents: &lt;strong&gt;for_motion&lt;/strong&gt; and &lt;strong&gt;against_motion&lt;/strong&gt;. Each agent will engage in a debate on a given topic, providing examples and substantiating their points. A facilitator will oversee the debate rounds, ensuring that each response exceeds 300 words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring AutoGen with LM Studio and Local LLM</title>
      <link>https://seehiong.github.io/posts/2023/12/exploring-autogen-with-lm-studio-and-local-llm/</link>
      <pubDate>Sat, 02 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/exploring-autogen-with-lm-studio-and-local-llm/</guid>
      <description>I explored AutoGen, an innovative framework on GitHub, enabling the development of Large Language Model (LLM) applications. Collaborating with LM Studio, I set up a local LLM application, showcasing the step-by-step process. Installing LM Studio involved configuring context length, enabling GPU acceleration, and setting CPU threads. The integration process showcased a seamless environment for running local LLMs. Additionally, I explored the AutoGen setup, including installing Anaconda and creating a virtual environment. With the provided guidelines, I executed the app.py script, generating a stock price comparison chart through AutoGen&amp;rsquo;s dynamic conversation.</description>
    </item>
    <item>
      <title>Boosting Inference Speed: SSD and GPU Acceleration</title>
      <link>https://seehiong.github.io/posts/2023/11/boosting-inference-speed-ssd-and-gpu-acceleration/</link>
      <pubDate>Thu, 30 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/11/boosting-inference-speed-ssd-and-gpu-acceleration/</guid>
      <description>Embarking on an exhilarating upgrade journey, I chronicle the seamless migration to the powerful Lexar NM790 SSD and unveil the secrets behind turbocharging Langchain4j&amp;rsquo;s inferencing speed. With Clonezilla&amp;rsquo;s reliability, my Windows 11 transition to this SSD was flawless, offering a tangible boost. The GPU acceleration saga unfolded with CUDA installation and the NVIDIA Container Toolkit magic, resulting in a high-speed universe. Launching the LocalAI image in a GPU Docker container revealed the grand finale‚Äîa remarkable surge in Langchain4j&amp;rsquo;s inference speed. This transformation invites tech enthusiasts to explore elevated performance and redefine possibilities.</description>
    </item>
    <item>
      <title>RAG over Java code with Langchain4j</title>
      <link>https://seehiong.github.io/posts/2023/11/rag-over-java-code-with-langchain4j/</link>
      <pubDate>Sat, 11 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/11/rag-over-java-code-with-langchain4j/</guid>
      <description>In my latest post, I delve into seamlessly integrating Retrieval-Augmented Generation (RAG) with Java code using Langchain4j. Drawing inspiration from RAG over code, I explore Java Parser&amp;rsquo;s potential for robust codebase analysis. The pivotal JavaParsingService and EmbeddingStoreService orchestrate this integration, enabling users to effortlessly load Java projects and glean profound insights. The enhanced controller boasts user-friendly endpoints, fostering dynamic interactions. Witness Retrieval-Augmented Generation breathe life into Java code, from codebase ingestion to insightful querying with models like gpt4all-j, WizardLM, and OpenAI. This narrative unveils the nuanced capabilities of RAG in querying Java codebases.</description>
    </item>
    <item>
      <title>Building an AI Application with Langchain4j</title>
      <link>https://seehiong.github.io/posts/2023/11/building-an-ai-application-with-langchain4j/</link>
      <pubDate>Tue, 07 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/11/building-an-ai-application-with-langchain4j/</guid>
      <description>I embarked on a journey to harness the capabilities of Langchain4j, crafting a powerful AI application in Java using the local language model. Utilizing Spring Boot, Postman, and various Langchain4j components, I explored setting up, implementing a chat service, integrating custom tools, embedding functionality with Chroma, translation, persistence, retrieval, and streaming services. The blog post serves as a comprehensive guide for building personalized AI applications, showcasing the versatility and potential of Langchain4j in Java development.</description>
    </item>
    <item>
      <title>Unlocking the Power of Machine Learning with MLC LLM</title>
      <link>https://seehiong.github.io/posts/2023/09/unlocking-the-power-of-machine-learning-with-mlc-llm/</link>
      <pubDate>Sat, 02 Sep 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/09/unlocking-the-power-of-machine-learning-with-mlc-llm/</guid>
      <description>I delve into the transformative realm of MLC LLM, an advanced universal deployment solution for extensive language models. My post guides you personally through the setup, emphasizing critical components like TVM and Conda. I demonstrate the process, including TVM installation via pip, Conda setup on WSL, and Vulkan SDK installation for optimal performance. Navigating the MLC Chat exploration, I detail creating a Conda environment and running MLC LLM&amp;rsquo;s CLI version, offering a glimpse into its potential through a sample question. With MLC LLM and MLC Chat at your fingertips, the world of machine learning and language understanding unfolds boundless possibilities. üöÄüß†</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/posts/2023/08/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/08/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>Embarking on my journey with vLLM, I explore its potential for streamlined Large Language Model (LLM) inference and deployment. The blog details my personal experience setting up vLLM on a Windows Subsystem for Linux (WSL) instance running Ubuntu 22.04. I meticulously guide through installing WSL, NVIDIA GPU drivers, CUDA Toolkit, and Docker for efficient utilization. Delving into vLLM setup within the NVIDIA PyTorch Docker image, I navigate through the installation process and launch the API server. The blog provides insights into querying the model and creating a Docker image snapshot, offering a comprehensive guide to efficient language model serving.</description>
    </item>
    <item>
      <title>Unleashing the Power of LLaMA Server in Docker Container</title>
      <link>https://seehiong.github.io/posts/2023/07/unleashing-the-power-of-llama-server-in-docker-container/</link>
      <pubDate>Sat, 15 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/07/unleashing-the-power-of-llama-server-in-docker-container/</guid>
      <description>After completing the Generative AI with Large Language Models course, I&amp;rsquo;m thrilled to share my Dockerized experience running the LLaMA model. The guide covers setting up the project structure, creating a FastAPI application, and Dockerizing it. Additionally, I showcase building an AI chatbot, integrating it with FastAPI, HuggingFace embeddings, and LLaMA. The Docker environment loads the LLM and allows seamless interactions with PDFs. I conclude by enhancing performance with OpenBLAS, significantly reducing inferencing time. Explore the power of LLaMA Server in a Docker container for transformative AI experiences! üöÄ</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (II)</title>
      <link>https://seehiong.github.io/posts/2023/06/how-to-summarize-youtube-videos-in-minutes-ii/</link>
      <pubDate>Fri, 16 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/posts/2023/06/how-to-summarize-youtube-videos-in-minutes-ii/</guid>
      <description>In this comprehensive guide, I explore AI-powered techniques to extract and summarize YouTube videos using tools like Whisper.cpp, GPT4All, LLaMA.cpp, and OpenAI models. I detail the step-by-step process, from setting up the environment to transcribing audio and leveraging AI for summarization. Despite encountering issues with GPT4All&amp;rsquo;s accuracy, alternative approaches using LLaMA.cpp and OpenAI models provide versatile summarization options. The tutorial aims to empower researchers, content creators, and information enthusiasts to efficiently analyze and summarize YouTube content using cutting-edge AI technologies.</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (I)</title>
      <link>https://seehiong.github.io/posts/2023/06/how-to-summarize-youtube-videos-in-minutes-i/</link>
      <pubDate>Sat, 10 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/posts/2023/06/how-to-summarize-youtube-videos-in-minutes-i/</guid>
      <description>Hey folks! Today, I&amp;rsquo;m stoked to introduce you to the game-changer that is GPT4All for summarizing YouTube videos. Join me on this journey of transformation as we set up the magic using Python. We&amp;rsquo;ll load transcripts, chunk them for optimal processing, and then unleash the power of GPT4All for mind-blowing summarizations. Brace yourself for amazement as we witness the magic unfold! Additionally, we&amp;rsquo;ll explore an optional OpenAI approach for comparison. Stay tuned for more exciting updates in the next blog post on video content summarization without embedded transcripts! ‚ú®üöÄ</description>
    </item>
    <item>
      <title>Receipt OCR with LangChain, OpenAI and PyTesseract</title>
      <link>https://seehiong.github.io/posts/2023/06/receipt-ocr-with-langchain-openai-and-pytesseract/</link>
      <pubDate>Tue, 06 Jun 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/06/receipt-ocr-with-langchain-openai-and-pytesseract/</guid>
      <description>&lt;p&gt;Recently, I embarked on an exhilarating journey into the realm of receipt OCR using LangChain and OpenAI, inspired by the captivating course on &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://learn.deeplearning.ai/langchain/lesson/1/introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;LangChain for LLM Application Development&lt;/a&gt;&#xD;&#xA;. This exploration allowed me to unlock the full potential of PyTesseract, an extraordinary Python tool that serves as my guiding light for optical character recognition (OCR). By harnessing the power of OpenCV and seamlessly integrating OpenAI into the workflow, I aimed to compile the most optimal OCR results and validate them using LangChain&amp;rsquo;s impressive llm-math tool. Join me on this exciting adventure as we unravel the intricacies of receipt OCR and discover the true potential of LangChain, OpenAI, and PyTesseract.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Autofill PDF with LangChain and LangFlow</title>
      <link>https://seehiong.github.io/posts/2023/05/autofill-pdf-with-langchain-and-langflow/</link>
      <pubDate>Fri, 26 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/05/autofill-pdf-with-langchain-and-langflow/</guid>
      <description>In this journey, I explore automating PDF autofill using LangChain and LangFlow. Leveraging LangFlow and OpenAI, I streamline the employment form completion process, demonstrating steps to install LangFlow and set up a PostgreSQL table. Despite encountering challenges in prototyping with LangFlow, the exploration progresses to auto-fill PDFs. After extracting form fields and LLaMA model setup, I employ LangChain to fetch PostgreSQL data. Concluding with Python manipulation to interpolate and update the PDF, the process achieves seamless auto-fill. Dive into the details, overcome challenges, and witness the power of LangChain and LangFlow in revolutionizing PDF automation.</description>
    </item>
    <item>
      <title>Running GPT4All for your PostgreSQL with LangChain</title>
      <link>https://seehiong.github.io/posts/2023/05/running-gpt4all-for-your-postgresql-with-langchain/</link>
      <pubDate>Sun, 21 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/05/running-gpt4all-for-your-postgresql-with-langchain/</guid>
      <description>In this exploration, I guide you through setting up GPT4All on a Windows PC and demonstrate its synergy with SQL Chain for PostgreSQL queries using LangChain. Utilizing Jupyter Notebook and prerequisites like PostgreSQL and GPT4All-J v1.3-groovy, I install dependencies and showcase LangChain and GPT4All model setup. Navigating an Open Source Shakespeare database, I provide an ER diagram for clarity. Querying GPT4All through LangChain, we delve into PostgreSQL queries and also compare responses with OpenAI. The comprehensive walkthrough empowers you to seamlessly integrate GPT4All into your PostgreSQL workflows for efficient and dynamic interactions.</description>
    </item>
    <item>
      <title>Running LLaMA server in local machine</title>
      <link>https://seehiong.github.io/posts/2023/05/running-llama-server-in-local-machine/</link>
      <pubDate>Sat, 13 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/05/running-llama-server-in-local-machine/</guid>
      <description>In continuation from my previous post, I prepared the environment using Pipenv and installed the OpenAI-like web server with specific CMAKE arguments. Running the server with a provided model was straightforward. To create an SSH tunnel to the remote Ubuntu machine from my Windows PC, I used PuTTY, configuring it to forward port 8888. Connecting from BYO-GPT involved adjusting the server endpoint in the Dart file. This seamless integration allowed me to access the Open API for the LLAMA CPP server and successfully connect BYO-GPT to the specified server.</description>
    </item>
    <item>
      <title>Building ChatBot for your PDF files with LangChain</title>
      <link>https://seehiong.github.io/posts/2023/05/building-chatbot-for-your-pdf-files-with-langchain/</link>
      <pubDate>Sun, 07 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/05/building-chatbot-for-your-pdf-files-with-langchain/</guid>
      <description>In this post, I extend the use case from my previous post to demonstrate building a ChatBot for PDF files using LangChain. In the preparation phase, I install Chroma, an open-source embedding database, and ingest a PDF file using PyPDFLoader. I then split the document into chunks and use Chroma&amp;rsquo;s default embeddings. Due to a potential issue, I provide an alternative embedding approach. Next, I load a local LLaMA model, prepare for question-answering, and run queries using RetrievalQAWithSourcesChain. I also touch on running with OpenBLAS for optimization. The guide empowers users to explore personalized question-answering over their PDF documents.</description>
    </item>
    <item>
      <title>Building a Basic Chain with LangChain</title>
      <link>https://seehiong.github.io/posts/2023/05/building-a-basic-chain-with-langchain/</link>
      <pubDate>Mon, 01 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/05/building-a-basic-chain-with-langchain/</guid>
      <description>With the LangChain framework and a setup from a previous post, I delve into building a basic chain using Llama.cpp within LangChain. Following preparations, I install required packages and run interactive Python code to set up the LLM model. The process involves formatting a prompt template and creating a chain. I explore memory integration, adding a conversation buffer for context. The conversation with AI is initiated and continued through user inputs. Stay tuned for more explorations in upcoming posts!</description>
    </item>
    <item>
      <title>Running LLaMA model locally</title>
      <link>https://seehiong.github.io/posts/2023/04/running-llama-model-locally/</link>
      <pubDate>Sun, 30 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/04/running-llama-model-locally/</guid>
      <description>In this thorough guide, I prepared my Ubuntu machine (32GB) for the LLaMA (Language Model) build. Following Georgi Gergano&amp;rsquo;s llama.cpp, I executed CMake commands, ensuring the correct tag and building the model successfully. I downloaded Microsoft&amp;rsquo;s Phi2 model in GGUF format, enabling local execution without exposing prompts or data. Running the Phi2 model showcased its capabilities in a few-shot interaction, providing accurate responses. Additionally, I explored optional OpenBLAS integration for improved speed, offering insights into the installation and rebuild process.</description>
    </item>
  </channel>
</rss>
