<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/ai/</link>
    <description>Recent content in AI on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Nov 2024 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring AI with Raspberry Pi 5</title>
      <link>https://seehiong.github.io/2024/raspberry-pi/</link>
      <pubDate>Sat, 09 Nov 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/raspberry-pi/</guid>
      <description>&lt;p&gt;So, with the recent release of the &lt;a href=&#34;https://www.raspberrypi.com/products/ai-kit/&#34; target=&#34;_blank&#34;&gt;Raspberry Pi AI Kit&lt;/a&gt;, I just couldn’t resist jumping into the AI pool with my shiny new toy. The AI Kit packs a &lt;em&gt;whopping&lt;/em&gt; 13 tera-operations per second (TOPS) neural network inference accelerator powered by the mighty Hailo-8L chip.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/raspberry/raspberry-pi-ai-kit.jpeg&#34; alt=&#34;raspberry-pi-ai-kit&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;As always, I got straight to work, setting up my Pi by flashing the &lt;strong&gt;Raspberry Pi OS&lt;/strong&gt; onto a 64GB SD card using the reliable &lt;a href=&#34;https://www.raspberrypi.com/software/&#34; target=&#34;_blank&#34;&gt;Raspberry Pi Imager&lt;/a&gt;. I added the active cooler, connected the AI Kit, and hooked up the AI camera. And since I’m a big fan of building things (especially with Legos), I crafted a &lt;em&gt;highly professional&lt;/em&gt; Lego case to house it all.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPT-2 Training Guide</title>
      <link>https://seehiong.github.io/2024/gpt-2-training-guide/</link>
      <pubDate>Thu, 31 Oct 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/gpt-2-training-guide/</guid>
      <description>This post documents my journey training GPT-2 on the Tiny Shakespeare dataset, inspired by Andrej Karpathy&amp;rsquo;s instructional video and nanoGPT repository. Following each step, I detail the setup process, from encoding data and optimizing the model with AdamW, to improving training stability with mixed precision and flash attention. The post includes practical insights on using pretrained weights, weight sharing, and efficient data handling, concluding with sample outputs from training and sampling a “baby” GPT model.</description>
    </item>
    <item>
      <title>GPT-2 Setup and Pretraining Guide</title>
      <link>https://seehiong.github.io/2024/gpt-2-setup-and-pretraining-guide/</link>
      <pubDate>Mon, 28 Oct 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/gpt-2-setup-and-pretraining-guide/</guid>
      <description>This guide explores reproducing GPT-2 (124M) using Andrej Karpathy’s video walkthrough. It begins with an overview of the GPT-2 architecture, a decoder-only transformer model inspired by &amp;ldquo;Attention Is All You Need.&amp;rdquo; Using pretrained GPT-2 weights, we analyze and initialize a custom GPT class, with detailed steps to handle token embeddings, causal attention, and layer normalization. The guide includes code for generating text from pretrained weights. In the next segment, we’ll continue with a deeper dive into dataset preparation and training from scratch, moving from small samples to large-scale training.</description>
    </item>
    <item>
      <title>Building Advanced RAG Applications with MyScaleDB and LlamaIndex</title>
      <link>https://seehiong.github.io/2024/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</link>
      <pubDate>Sat, 15 Jun 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</guid>
      <description>Explore how to build advanced Retrieval-Augmented Generation (RAG) applications using MyScaleDB and LlamaIndex. This guide covers the installation of necessary tools, setting up a virtual environment, and creating an index for document categorization. Learn how to execute simple and filtered queries, and troubleshoot common issues. Enhance your understanding of integrating high-performance SQL vector databases with cutting-edge data frameworks for efficient LLM applications.</description>
    </item>
    <item>
      <title>Coding with CrewAI: AI Orchestration Simplified</title>
      <link>https://seehiong.github.io/2024/coding-with-crewai-ai-orchestration-simplified/</link>
      <pubDate>Fri, 29 Mar 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/coding-with-crewai-ai-orchestration-simplified/</guid>
      <description>Explore CrewAI, a pioneering framework streamlining AI agent orchestration. Discover practical applications, from Jan and LM Studio integration to Serper API utilization. Follow along as we delve into coding with CrewAI, showcasing its versatility in crafting resumes and more. Experience the seamless synergy of autonomous AI agents, revolutionizing workflows with efficiency and innovation. Unlock the power of CrewAI, propelling your projects to new heights in artificial intelligence.</description>
    </item>
    <item>
      <title>Text-to-Image with StableDiffusionPipeline</title>
      <link>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</link>
      <pubDate>Sat, 10 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</guid>
      <description>In this post, we explore the capabilities of StableDiffusionPipeline for generating photorealistic images from textual inputs. We start with setting up the environment and installing necessary libraries. Then, we dive into Textual Inversion, demonstrating how the model learns new concepts from images. Image-to-Image transformations are also explored, showcasing the pipeline&amp;rsquo;s versatility. Additionally, we introduce Animagine XL 2.0, a model for high-resolution anime image creation, and provide sample code for its implementation. Lastly, we highlight Stable Diffusion XL, a powerful text-to-image model, and share a festive image generated using it.</description>
    </item>
    <item>
      <title>Stable Diffusion: Text-to-Image Modeling Journey</title>
      <link>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</link>
      <pubDate>Sat, 03 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</guid>
      <description>This post explores Stable Diffusion, a latent text-to-image diffusion model in machine learning. Diffusion models, with forward, reverse, and sampling components, understand and generate patterns in datasets. Illustrating applications in image tasks, it introduces the process of installing and utilizing Stable Diffusion. The post details image generation and modification using prompts, with examples and troubleshooting. Notably, it shares an encounter with CUDA out-of-memory errors and the resolution through image resizing. Overall, it offers a comprehensive guide, combining theoretical insights with practical implementation steps in a professional manner.</description>
    </item>
    <item>
      <title>OpenVINO, Optimum-Intel, CPU: An Exploration in Model Optimization</title>
      <link>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</link>
      <pubDate>Sat, 27 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</guid>
      <description>Explore the convergence of OpenVINO and Optimum-Intel in this post, where I detail the setup and execution of example code on my aging laptop. Focused on applying Quantization-aware Training and the Token Merging method to optimize the UNet model within the Stable Diffusion pipeline, this journey showcases the synergy of open-source tools for deep learning model deployment. Note that the provided code is tailored for CPU-based inference due to limitations in my aging GeForce graphics card, making it a valuable resource for users with similar hardware constraints. Dive into the world of optimized models and delightful Pokemon creation!</description>
    </item>
    <item>
      <title>Exploring Autogen Studio</title>
      <link>https://seehiong.github.io/2024/exploring-autogen-studio/</link>
      <pubDate>Sun, 14 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/exploring-autogen-studio/</guid>
      <description>In this exploration of Autogen Studio, we navigated through the AI landscape, harnessing the LM Studio API to compare responses from diverse language models. Employing the Mistral Instruct 7B model, we scrutinized prompts like Stock Price and Paint, visualizing outcomes and delving into key configurations. The post also offered insights into the primary assistant, model configuration, and agent workflows, accompanied by a comparative analysis of Mistral model responses. This comprehensive journey demystifies the power of Autogen Studio and its seamless integration with LM Studio API, providing practical guidance for AI enthusiasts.</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>In this post, we explored deploying Lightweight Language Models (LLMs) using WasmEdge, a high-performance WebAssembly runtime, within a HomeLab environment. The process involved preparing an OpenAI-compatible API server, configuring the Wasi-NN plugin, and deploying the setup to HomeLab using Kubernetes (K3s). The post also detailed the steps for testing the API server and integrating it into a Java application. Overall, the guide provides a comprehensive walkthrough of hosting and utilizing LLMs with WasmEdge in a local environment.</description>
    </item>
    <item>
      <title>Integration of Kong into AI Workflow</title>
      <link>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</link>
      <pubDate>Sat, 06 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</guid>
      <description>This comprehensive guide navigates through configuring Kong OSS and Kong Ingress Controller (KIC), seamlessly integrating Kong into an AI workflow. Starting with Kong OSS configuration, the tutorial covers updating environment variables and service ports. The Langchain4j application is then adapted to leverage Kong API, allowing for flexible path-based APIs. Additionally, potential timeout issues are addressed. The guide concludes with a demonstration of Kong Ingress Controller configuration, emphasizing optimal settings for specific use cases. Whether through Kong OSS or KIC, readers gain insights into enhancing API management and integration within their AI workflows.</description>
    </item>
    <item>
      <title>Unveiling Agent AutoBuild in Autogen</title>
      <link>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</link>
      <pubDate>Sun, 17 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</guid>
      <description>&lt;p&gt;Discover the capabilities of &lt;a href=&#34;https://microsoft.github.io/autogen/blog/2023/11/26/Agent-AutoBuild/&#34; target=&#34;_blank&#34;&gt;Agent AutoBuild&lt;/a&gt; in my recent exploration with Autogen using &lt;em&gt;app.py&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setup-configuration&#34;&gt;Setup Configuration&lt;/h2&gt;&#xA;&lt;p&gt;In my model setup configuration, defined in &lt;em&gt;OAI_CONFIG_LIST&lt;/em&gt;, I&amp;rsquo;m leveraging the latest version of Autogen (&lt;em&gt;pyautogen==0.2.2&lt;/em&gt;) with the following specifications:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gpt-4&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;api_key&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NULL&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://192.168.68.114:1234/v1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>&lt;p&gt;In the pursuit of enhancing Autogen&amp;rsquo;s capabilities, I drew inspiration from &lt;a href=&#34;https://github.com/0xlws/autogen&#34; target=&#34;_blank&#34;&gt;0xlws&amp;rsquo; fork&lt;/a&gt; supporting JavaScript. This led me to embark on a journey to modify Autogen, enabling robust support for Java code execution.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up&#34;&gt;Setting up&lt;/h2&gt;&#xA;&lt;p&gt;Begin by ensuring that Java is installed on your Windows Subsystem for Linux (WSL) using the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install openjdk-17-jdk-headless&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Multi-agent Conservation with Autogen</title>
      <link>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</link>
      <pubDate>Fri, 08 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post &lt;a href=&#34;https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/&#34; target=&#34;_blank&#34;&gt;Exploration with Autogen&lt;/a&gt; and following the example of &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat&#34; target=&#34;_blank&#34;&gt;Automated Multi Agent Chat&lt;/a&gt;, we&amp;rsquo;ll delve into the steps to create a dynamic debate environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;agent-setup&#34;&gt;Agent Setup&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll be setting up two agents: &lt;strong&gt;for_motion&lt;/strong&gt; and &lt;strong&gt;against_motion&lt;/strong&gt;. Each agent will engage in a debate on a given topic, providing examples and substantiating their points. A facilitator will oversee the debate rounds, ensuring that each response exceeds 300 words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring AutoGen with LM Studio and Local LLM</title>
      <link>https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/</link>
      <pubDate>Sat, 02 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/</guid>
      <description>&lt;p&gt;AutoGen, an innovative framework available on &lt;a href=&#34;https://github.com/microsoft/autogen&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;, empowers the development of LLM (Large Language Model) applications. These applications utilize multiple agents that engage in conversation to collaboratively solve tasks.&lt;/p&gt;&#xA;&lt;p&gt;In conjunction with AutoGen, &lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34;&gt;LM Studio&lt;/a&gt; provides a platform to discover, download, and run local LLMs. In this blog post, we&amp;rsquo;ll delve into the integration of AutoGen with LM Studio, showcasing a step-by-step guide on setting up a local LLM application served through LM Studio.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Boosting Inference Speed: SSD and GPU Acceleration</title>
      <link>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</link>
      <pubDate>Thu, 30 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</guid>
      <description>&lt;p&gt;In the relentless pursuit of optimal disk space and lightning-fast inference speeds, I embarked on an exciting upgrade journey by integrating the formidable &lt;a href=&#34;https://www.lexar.com/product/lexar-nm790-m-2-2280-pcie-gen-4x4-nvme-ssd/&#34; target=&#34;_blank&#34;&gt;Lexar NM790 M.2 2280 PCIe SSD&lt;/a&gt;. This blog post unfolds in two parts: the first chronicles the meticulous migration of my Windows 11 to this powerhouse SSD, while the second unveils the secrets behind the enhanced inferencing speed for the &lt;a href=&#34;https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;Langchain4j application&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;part-1-seamless-os-migration-with-clonezilla&#34;&gt;Part 1: Seamless OS Migration with Clonezilla&lt;/h2&gt;&#xA;&lt;p&gt;Amidst a sea of software promising seamless disk cloning, I found solace in the reliability of &lt;a href=&#34;https://clonezilla.org/&#34; target=&#34;_blank&#34;&gt;Clonezilla&lt;/a&gt;, a robust open-source tool for disk imaging and cloning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG over Java code with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</link>
      <pubDate>Sat, 11 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</guid>
      <description>&lt;p&gt;Expanding upon the concepts introduced in the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; and drawing inspiration from &lt;a href=&#34;https://python.langchain.com/docs/use_cases/question_answering/code_understanding&#34; target=&#34;_blank&#34;&gt;RAG over code&lt;/a&gt;, this article dives into the integration of a Retrieval-Augmented Generation (RAG) service. The goal is to empower users to query their Java codebase effectively.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To embark on this journey, I&amp;rsquo;ve opted for &lt;a href=&#34;https://javaparser.org/&#34; target=&#34;_blank&#34;&gt;Java Parser&lt;/a&gt; , a powerful tool for traversing Java source code. Let&amp;rsquo;s begin by incorporating the latest version of Java Parser into our build.gradle file:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building an AI Application with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</link>
      <pubDate>Tue, 07 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</guid>
      <description>&lt;p&gt;In this blog post, I&amp;rsquo;ll walk you through my journey of harnessing the capabilities of &lt;a href=&#34;https://github.com/langchain4j/langchain4j/&#34; target=&#34;_blank&#34;&gt;langchain4j&lt;/a&gt; to craft a powerful AI application using Java, specifically with a local language model. Unlike my previous exploration with Python, this post focuses on the Java implementation with Langchain4j.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To kick things off, I&amp;rsquo;ve chosen &lt;a href=&#34;https://spring.io/tools&#34; target=&#34;_blank&#34;&gt;STS4&lt;/a&gt; as my Integrated Development Environment (IDE) and opted for &lt;a href=&#34;https://adoptium.net/temurin/archive/?version=17&#34; target=&#34;_blank&#34;&gt;Java 17&lt;/a&gt; as my programming language. Leveraging &lt;a href=&#34;https://www.postman.com/&#34; target=&#34;_blank&#34;&gt;Postman&lt;/a&gt; as my API platform and &lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34;&gt;Spring Boot&lt;/a&gt; as the framework of choice, let&amp;rsquo;s delve into the process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unlocking the Power of Machine Learning with MLC LLM</title>
      <link>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</link>
      <pubDate>Sat, 02 Sep 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</guid>
      <description>&lt;p&gt;Machine Learning Compilation for LLM, or &lt;a href=&#34;https://mlc.ai/mlc-llm/docs/index.html&#34; target=&#34;_blank&#34;&gt;MLC LLM&lt;/a&gt;, is a cutting-edge universal deployment solution for large language models. In this blog post, we&amp;rsquo;ll guide you through the setup process and show you how to harness the immense potential of MLC LLM.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-your-environment&#34;&gt;Setting Up Your Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started with MLC LLM, you need to set up your environment properly. Follow these steps:&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-install-tvm&#34;&gt;1. Install TVM&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://tvm.apache.org/docs/install/index.html&#34; target=&#34;_blank&#34;&gt;TVM&lt;/a&gt;  is a critical component for MLC LLM. You can install it locally using pip:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://vllm.ai/&#34; target=&#34;_blank&#34;&gt;vLLM&lt;/a&gt; is an open-source library designed for rapid LLM (Large Language Model) inference and deployment. It leverages their novel algorithm called &lt;strong&gt;PagedAttention&lt;/strong&gt;, which optimizes the management of attention keys and values.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;In this blog post, I will share my experience of utilizing vLLM on a WSL (Windows Subsystem for Linux) instance running Ubuntu 22.04. Let&amp;rsquo;s start by setting up the environment:&lt;/p&gt;&#xA;&lt;h3 id=&#34;installing-wsl-and-configuring-ubuntu&#34;&gt;Installing WSL and Configuring Ubuntu&lt;/h3&gt;&#xA;&lt;p&gt;Begin by installing WSL and configuring it to use Ubuntu as the default distribution:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unleashing the Power of LLaMA Server in Docker Container</title>
      <link>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</link>
      <pubDate>Sat, 15 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</guid>
      <description>&lt;p&gt;Having recently completed the enlightening &lt;a href=&#34;https://www.coursera.org/learn/generative-ai-with-llms&#34; target=&#34;_blank&#34;&gt;Generative AI with Large Language Models&lt;/a&gt; course, where we gained invaluable knowledge and hands-on skills, we are now excited to share an exhilarating experience of running the LLaMA model in a Dockerized container.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, we&amp;rsquo;ll walk you through the setup and demonstrate how to unleash the full potential of running LLaMA Server within a Docker container.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;the-setup&#34;&gt;The Setup&lt;/h2&gt;&#xA;&lt;p&gt;Before we delve into the magic of LLaMA, let&amp;rsquo;s set up our application structure. To ensure smooth execution, we&amp;rsquo;ve structured our project as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (II)</title>
      <link>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</link>
      <pubDate>Fri, 16 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</guid>
      <description>&lt;p&gt;In continuation with the &lt;a href=&#34;https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/&#34; target=&#34;_blank&#34;&gt;previous&lt;/a&gt; post, we will explore the power of AI by leveraging the &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34; target=&#34;_blank&#34;&gt;whisper.cpp&lt;/a&gt; library to convert audio to text, extracting audio from YouTube videos using &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34; target=&#34;_blank&#34;&gt;yt-dlp&lt;/a&gt;, and demonstrating how to utilize AI models like GPT4All and OpenAI for summarization.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-the-environment&#34;&gt;Setting Up the Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started, we need to set up the necessary tools and libraries. Follow the steps below:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (I)</title>
      <link>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/</link>
      <pubDate>Sat, 10 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/</guid>
      <description>&lt;p&gt;Hey there, readers! Today, I&amp;rsquo;m thrilled to introduce you to an incredible tool that will completely transform the way you summarize YouTube videos. Get ready to dive into the captivating world of video content summarization using the powerful GPT4All. Trust me, this is an opportunity you don&amp;rsquo;t want to miss!&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-the-magic&#34;&gt;Setting up the Magic&lt;/h2&gt;&#xA;&lt;p&gt;Before we embark on this exciting journey, let&amp;rsquo;s ensure we have everything we need to get started. Install the necessary dependencies by running the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Receipt OCR with LangChain, OpenAI and PyTesseract</title>
      <link>https://seehiong.github.io/archives/2023/receipt-ocr-with-langchain-and-openai/</link>
      <pubDate>Tue, 06 Jun 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/receipt-ocr-with-langchain-and-openai/</guid>
      <description>&lt;p&gt;Recently, I embarked on an exhilarating journey into the realm of receipt OCR using LangChain and OpenAI, inspired by the captivating course on &lt;a href=&#34;https://learn.deeplearning.ai/langchain/lesson/1/introduction&#34; target=&#34;_blank&#34;&gt;LangChain for LLM Application Development&lt;/a&gt;. This exploration allowed me to unlock the full potential of PyTesseract, an extraordinary Python tool that serves as my guiding light for optical character recognition (OCR). By harnessing the power of OpenCV and seamlessly integrating OpenAI into the workflow, I aimed to compile the most optimal OCR results and validate them using LangChain&amp;rsquo;s impressive llm-math tool. Join me on this exciting adventure as we unravel the intricacies of receipt OCR and discover the true potential of LangChain, OpenAI, and PyTesseract.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Autofill PDF with LangChain and LangFlow</title>
      <link>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</link>
      <pubDate>Fri, 26 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</guid>
      <description>&lt;p&gt;In this blog post, we will explore the usage of &lt;a href=&#34;https://pypi.org/project/langflow/&#34; target=&#34;_blank&#34;&gt;LangFlow&lt;/a&gt;, a Python library available on PyPI, to streamline the process of capturing ideas and conducting proof-of-concepts for our intended use case. Considering the current &amp;ldquo;trend&amp;rdquo; of tech layoffs, there might be a time (touch wood) where there is a need to go for interviews and fill-up various interview forms that require filling out personal information. Building upon the previous blog post on running GPT4All for PostgreSQL with LangChain (referenced &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), we will now leverage LangFlow and OpenAI to automate the population of a sample employment form with our personal data stored in PostgreSQL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running GPT4All for your PostgreSQL with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/</link>
      <pubDate>Sun, 21 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/</guid>
      <description>&lt;p&gt;In this post, I will walk you through the process of setting up &lt;a href=&#34;https://github.com/nomic-ai/gpt4all/blob/main/gpt4all-bindings/python/README.md&#34; target=&#34;_blank&#34;&gt;Python GPT4All&lt;/a&gt; on my Windows PC. Additionally, I will demonstrate how to utilize the power of GPT4All along with &lt;a href=&#34;https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html&#34; target=&#34;_blank&#34;&gt;SQL Chain&lt;/a&gt; for querying a postgreSQL database.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;&#xA;&lt;p&gt;Before we proceed with the installation process, it is important to have the necessary prerequisites in place.&lt;/p&gt;&#xA;&lt;p&gt;To follow along with this guide, make sure you have the following:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA server in local machine</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</link>
      <pubDate>Sat, 13 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</guid>
      <description>&lt;p&gt;Referencing the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, we will run a web server which aims to act as a drop-in replacement for the OpenAI API, which can in turn be used by &lt;a href=&#34;https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/&#34; target=&#34;_blank&#34;&gt;byogpt&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(3 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://packaging.python.org/en/latest/key_projects/#pipenv&#34; target=&#34;_blank&#34;&gt;Pipenv&lt;/a&gt; aims to help users manage environments, dependencies and imported packages and I will be using it in this guide.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install pipenv uvicorn fastapi sse_starlette&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipenv shell&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Building ChatBot for your PDF files with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</link>
      <pubDate>Sun, 07 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</guid>
      <description>&lt;p&gt;Extending the use case on the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I will demostrate how you could ingest your own PDF file to your own &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;LLaMa model in local machine&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start off by installing &lt;a href=&#34;https://github.com/chroma-core/chroma&#34; target=&#34;_blank&#34;&gt;Chroma&lt;/a&gt;, the open-source embedding database:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install chromadb pypdf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pdf/langchain-chromadb-install.png&#34; alt=&#34;langchain-chromadb-install&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;ingesting-your-pdf&#34;&gt;Ingesting your PDF&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 mins)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a basic Chain with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</link>
      <pubDate>Mon, 01 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://js.langchain.com/docs/&#34; target=&#34;_blank&#34;&gt;LangChain&lt;/a&gt; is a framework for developing applications powered by language models. With the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; setup, I will follow closely to using &lt;a href=&#34;https://python.langchain.com/en/latest/ecosystem/llamacpp.html&#34; target=&#34;_blank&#34;&gt;Llama.cpp within LangChain&lt;/a&gt; for building the simplest form of chain with LangChain.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, installs the required python packages:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo pip install llama-cpp-python langchain &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/langchain/langchain-llama-dependencies.png&#34; alt=&#34;langchain-llama-dependencies&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA model locally</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-model-locally/</link>
      <pubDate>Sun, 30 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-model-locally/</guid>
      <description>&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(30 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34; target=&#34;_blank&#34;&gt;LLaMA&lt;/a&gt; is a collection of foundation language models ranging from 7B to 65B parameters.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will be using and following &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34; target=&#34;_blank&#34;&gt;Georgi Gergano&amp;rsquo;s llama.cpp&lt;/a&gt;, a inference of LLaMA model in pure C/C++.&lt;/p&gt;&#xA;&lt;p&gt;I will be setting this up in a Ubuntu machine with 32Gb.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/llama/hp-system-info.png&#34; alt=&#34;hp-system-info&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;To prepare for the build system, I installed these:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
