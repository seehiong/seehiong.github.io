<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/ai/</link>
    <description>Recent content in AI on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Jun 2024 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building Advanced RAG Applications with MyScaleDB and LlamaIndex</title>
      <link>https://seehiong.github.io/2024/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</link>
      <pubDate>Sat, 15 Jun 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/building-advanced-rag-applications-with-myscaledb-and-llamaindex/</guid>
      <description>&lt;p&gt;In this post, I will explore &lt;a href=&#34;https://github.com/myscale/myscaledb&#34; target=&#34;_blank&#34;&gt;MyScaleDB&lt;/a&gt;, an open-source, high-performance SQL vector database built on ClickHouse, and &lt;a href=&#34;https://www.llamaindex.ai/&#34; target=&#34;_blank&#34;&gt;LlamaIndex&lt;/a&gt;, the leading data framework for building LLM applications.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;After installing &lt;a href=&#34;https://vscodium.com/#install&#34; target=&#34;_blank&#34;&gt;VSCodium&lt;/a&gt; as my primary IDE, I proceeded with installing the Python extension via &lt;a href=&#34;https://open-vsx.org/vscode/item?itemName=ms-python.python&#34; target=&#34;_blank&#34;&gt;Marketplace Link&lt;/a&gt;.&#xA;Next, I created the &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34; target=&#34;_blank&#34;&gt;virtual environment&lt;/a&gt; using venv:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create the envrionment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python -m venv myscaledb&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Activate the environment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;myscaledb&lt;span style=&#34;color:#ae81ff&#34;&gt;\S&lt;/span&gt;cripts&lt;span style=&#34;color:#ae81ff&#34;&gt;\a&lt;/span&gt;ctivate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Coding with CrewAI: AI Orchestration Simplified</title>
      <link>https://seehiong.github.io/2024/coding-with-crewai-ai-orchestration-simplified/</link>
      <pubDate>Fri, 29 Mar 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/coding-with-crewai-ai-orchestration-simplified/</guid>
      <description>&lt;p&gt;Welcome to an exploration of &lt;a href=&#34;https://www.crewai.io/&#34; target=&#34;_blank&#34;&gt;CrewAI&lt;/a&gt;, a state-of-the-art framework designed to orchestrate autonomous AI agents. In this post, we&amp;rsquo;ll dive into the practical aspects of CrewAI, discovering its functionalities and potential applications.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To dive into the world of AI-driven creativity, let&amp;rsquo;s start by setting up our environment. We&amp;rsquo;ll create a dedicated Conda environment to ensure seamless integration with CrewAI:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a new Conda environment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n crewai python&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3.10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Active the environment&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate crewai&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Text-to-Image with StableDiffusionPipeline</title>
      <link>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</link>
      <pubDate>Sat, 10 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll delve into the capabilities of the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline&#34; target=&#34;_blank&#34;&gt;StableDiffusionPipeline&lt;/a&gt; for generating photorealistic images based on textual inputs.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;text-to-image&#34;&gt;Text-to-Image&lt;/h2&gt;&#xA;&lt;p&gt;Continuing from the &lt;a href=&#34;https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I initiated the environment setup:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd stable-diffusion&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate ldm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Subsequently, I installed the necessary libraries, &lt;a href=&#34;https://pypi.org/project/diffusers/&#34; target=&#34;_blank&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://pypi.org/project/transformers/&#34; target=&#34;_blank&#34;&gt;transformers&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install --upgrade diffusers&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; transformers&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Stable Diffusion: Text-to-Image Modeling Journey</title>
      <link>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</link>
      <pubDate>Sat, 03 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</guid>
      <description>&lt;p&gt;In this article, we will delve into &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34; target=&#34;_blank&#34;&gt;Stable Diffusion&lt;/a&gt;, a latent text-to-image diffusion model. In simple terms, diffusion models in machine learning represent a type of sophisticated computer program designed to learn how patterns evolve over time. Comprising three essential components – a forward process, a reverse process, and a sampling procedure – these models aim to comprehend and generate intricate patterns within a given dataset.&lt;/p&gt;&#xA;&lt;p&gt;Consider having a blurry image that needs enhancement. Diffusion models act as intelligent tools that learn to eliminate blurriness by grasping how images blur and then effectively reversing that process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenVINO, Optimum-Intel, CPU: An Exploration in Model Optimization</title>
      <link>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</link>
      <pubDate>Sat, 27 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.openvino.ai/2023.3/home.html&#34; target=&#34;_blank&#34;&gt;OpenVINO&lt;/a&gt; represents an open-source toolkit designed for the optimization and deployment of deep learning models. Acting as the interface between the Transformers and Diffusers libraries, &lt;a href=&#34;https://huggingface.co/docs/optimum/intel/inference&#34; target=&#34;_blank&#34;&gt;Optimum-Intel&lt;/a&gt; seamlessly integrates with various Intel tools and libraries, facilitating the acceleration of end-to-end pipelines on Intel architectures. This post documents my journey as I set up and execute example code on my aging laptop, exploring the application of Quantization-aware Training (QAT) and the Token Merging method to optimize the UNet model within the Stable Diffusion pipeline.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Autogen Studio</title>
      <link>https://seehiong.github.io/2024/exploring-autogen-studio/</link>
      <pubDate>Sun, 14 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/exploring-autogen-studio/</guid>
      <description>&lt;p&gt;In this comprehensive exploration, we delve into the realm of &lt;a href=&#34;https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/&#34; target=&#34;_blank&#34;&gt;Autogen Studio&lt;/a&gt;, a powerful tool designed to streamline the rapid prototyping of multi-agent solutions for various tasks.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;The journey begins with the initial setup. A new Python virtual environment is created using Conda, followed by the installation of Autogen Studio and the essential configuration of API keys.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n autogenstudio python&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3.10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate autogenstudio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install autogenstudio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export OPENAI_API_KEY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sk-xxxx&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;autogenstudio ui --port &lt;span style=&#34;color:#ae81ff&#34;&gt;8081&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>&lt;p&gt;In this post, we delve into the deployment of Lightweight Language Models (LLMs) using &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge&#34; target=&#34;_blank&#34;&gt;WasmEdge&lt;/a&gt;, a lightweight, high-performance, and extensible WebAssembly runtime. This setup is tailored to run LLMs in our previously configured HomeLab environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;To establish an OpenAI-compatible &lt;a href=&#34;https://github.com/LlamaEdge/LlamaEdge/tree/main/llama-api-server&#34; target=&#34;_blank&#34;&gt;API server&lt;/a&gt;, begin by downloading the API server application:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For Rust-based &lt;a href=&#34;https://wasmedge.org/docs/develop/rust/wasinn/llm_inference/&#34; target=&#34;_blank&#34;&gt;Llama 2 inference&lt;/a&gt;, we require the &lt;a href=&#34;https://wasmedge.org/docs/contribute/source/plugin/wasi_nn/&#34; target=&#34;_blank&#34;&gt;Wasi-NN&lt;/a&gt; plugin. The &lt;em&gt;Dockerfile&lt;/em&gt; below reflects this configuration:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integration of Kong into AI Workflow</title>
      <link>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</link>
      <pubDate>Sat, 06 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</guid>
      <description>&lt;p&gt;This post will guide you through the process of configuring Kong Gateway OSS and Kong Ingress Controller (KIC) separately and integrating Kong into our AI workflow.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;integrate-via-kong-gateway-oss-configuration&#34;&gt;Integrate via Kong Gateway OSS Configuration&lt;/h2&gt;&#xA;&lt;p&gt;If you followed my earlier guide on &lt;a href=&#34;https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/&#34; target=&#34;_blank&#34;&gt;setting up Kong Gateway&lt;/a&gt; setup, you likely use api.local:8000 to access the API.&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s revisit and update &lt;em&gt;KONG_ADMIN_GUI_URL&lt;/em&gt; environment  variable in the &lt;em&gt;kong-deploy-svc.yaml&lt;/em&gt; file:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unveiling Agent AutoBuild in Autogen</title>
      <link>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</link>
      <pubDate>Sun, 17 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</guid>
      <description>&lt;p&gt;Discover the capabilities of &lt;a href=&#34;https://microsoft.github.io/autogen/blog/2023/11/26/Agent-AutoBuild/&#34; target=&#34;_blank&#34;&gt;Agent AutoBuild&lt;/a&gt; in my recent exploration with Autogen using &lt;em&gt;app.py&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setup-configuration&#34;&gt;Setup Configuration&lt;/h2&gt;&#xA;&lt;p&gt;In my model setup configuration, defined in &lt;em&gt;OAI_CONFIG_LIST&lt;/em&gt;, I&amp;rsquo;m leveraging the latest version of Autogen (&lt;em&gt;pyautogen==0.2.2&lt;/em&gt;) with the following specifications:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gpt-4&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;api_key&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NULL&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://192.168.68.114:1234/v1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>&lt;p&gt;In the pursuit of enhancing Autogen&amp;rsquo;s capabilities, I drew inspiration from &lt;a href=&#34;https://github.com/0xlws/autogen&#34; target=&#34;_blank&#34;&gt;0xlws&amp;rsquo; fork&lt;/a&gt; supporting JavaScript. This led me to embark on a journey to modify Autogen, enabling robust support for Java code execution.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up&#34;&gt;Setting up&lt;/h2&gt;&#xA;&lt;p&gt;Begin by ensuring that Java is installed on your Windows Subsystem for Linux (WSL) using the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install openjdk-17-jdk-headless&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Multi-agent Conservation with Autogen</title>
      <link>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</link>
      <pubDate>Fri, 08 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post &lt;a href=&#34;https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/&#34; target=&#34;_blank&#34;&gt;Exploration with Autogen&lt;/a&gt; and following the example of &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat&#34; target=&#34;_blank&#34;&gt;Automated Multi Agent Chat&lt;/a&gt;, we&amp;rsquo;ll delve into the steps to create a dynamic debate environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;agent-setup&#34;&gt;Agent Setup&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll be setting up two agents: &lt;strong&gt;for_motion&lt;/strong&gt; and &lt;strong&gt;against_motion&lt;/strong&gt;. Each agent will engage in a debate on a given topic, providing examples and substantiating their points. A facilitator will oversee the debate rounds, ensuring that each response exceeds 300 words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring AutoGen with LM Studio and Local LLM</title>
      <link>https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/</link>
      <pubDate>Sat, 02 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/</guid>
      <description>&lt;p&gt;AutoGen, an innovative framework available on &lt;a href=&#34;https://github.com/microsoft/autogen&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;, empowers the development of LLM (Large Language Model) applications. These applications utilize multiple agents that engage in conversation to collaboratively solve tasks.&lt;/p&gt;&#xA;&lt;p&gt;In conjunction with AutoGen, &lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34;&gt;LM Studio&lt;/a&gt; provides a platform to discover, download, and run local LLMs. In this blog post, we&amp;rsquo;ll delve into the integration of AutoGen with LM Studio, showcasing a step-by-step guide on setting up a local LLM application served through LM Studio.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Boosting Inference Speed: SSD and GPU Acceleration</title>
      <link>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</link>
      <pubDate>Thu, 30 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</guid>
      <description>&lt;p&gt;In the relentless pursuit of optimal disk space and lightning-fast inference speeds, I embarked on an exciting upgrade journey by integrating the formidable &lt;a href=&#34;https://www.lexar.com/product/lexar-nm790-m-2-2280-pcie-gen-4x4-nvme-ssd/&#34; target=&#34;_blank&#34;&gt;Lexar NM790 M.2 2280 PCIe SSD&lt;/a&gt;. This blog post unfolds in two parts: the first chronicles the meticulous migration of my Windows 11 to this powerhouse SSD, while the second unveils the secrets behind the enhanced inferencing speed for the &lt;a href=&#34;https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;Langchain4j application&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;part-1-seamless-os-migration-with-clonezilla&#34;&gt;Part 1: Seamless OS Migration with Clonezilla&lt;/h2&gt;&#xA;&lt;p&gt;Amidst a sea of software promising seamless disk cloning, I found solace in the reliability of &lt;a href=&#34;https://clonezilla.org/&#34; target=&#34;_blank&#34;&gt;Clonezilla&lt;/a&gt;, a robust open-source tool for disk imaging and cloning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG over Java code with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</link>
      <pubDate>Sat, 11 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</guid>
      <description>&lt;p&gt;Expanding upon the concepts introduced in the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; and drawing inspiration from &lt;a href=&#34;https://python.langchain.com/docs/use_cases/question_answering/code_understanding&#34; target=&#34;_blank&#34;&gt;RAG over code&lt;/a&gt;, this article dives into the integration of a Retrieval-Augmented Generation (RAG) service. The goal is to empower users to query their Java codebase effectively.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To embark on this journey, I&amp;rsquo;ve opted for &lt;a href=&#34;https://javaparser.org/&#34; target=&#34;_blank&#34;&gt;Java Parser&lt;/a&gt; , a powerful tool for traversing Java source code. Let&amp;rsquo;s begin by incorporating the latest version of Java Parser into our build.gradle file:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building an AI Application with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</link>
      <pubDate>Tue, 07 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</guid>
      <description>&lt;p&gt;In this blog post, I&amp;rsquo;ll walk you through my journey of harnessing the capabilities of &lt;a href=&#34;https://github.com/langchain4j/langchain4j/&#34; target=&#34;_blank&#34;&gt;langchain4j&lt;/a&gt; to craft a powerful AI application using Java, specifically with a local language model. Unlike my previous exploration with Python, this post focuses on the Java implementation with Langchain4j.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To kick things off, I&amp;rsquo;ve chosen &lt;a href=&#34;https://spring.io/tools&#34; target=&#34;_blank&#34;&gt;STS4&lt;/a&gt; as my Integrated Development Environment (IDE) and opted for &lt;a href=&#34;https://adoptium.net/temurin/archive/?version=17&#34; target=&#34;_blank&#34;&gt;Java 17&lt;/a&gt; as my programming language. Leveraging &lt;a href=&#34;https://www.postman.com/&#34; target=&#34;_blank&#34;&gt;Postman&lt;/a&gt; as my API platform and &lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34;&gt;Spring Boot&lt;/a&gt; as the framework of choice, let&amp;rsquo;s delve into the process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unlocking the Power of Machine Learning with MLC LLM</title>
      <link>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</link>
      <pubDate>Sat, 02 Sep 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</guid>
      <description>&lt;p&gt;Machine Learning Compilation for LLM, or &lt;a href=&#34;https://mlc.ai/mlc-llm/docs/index.html&#34; target=&#34;_blank&#34;&gt;MLC LLM&lt;/a&gt;, is a cutting-edge universal deployment solution for large language models. In this blog post, we&amp;rsquo;ll guide you through the setup process and show you how to harness the immense potential of MLC LLM.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-your-environment&#34;&gt;Setting Up Your Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started with MLC LLM, you need to set up your environment properly. Follow these steps:&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-install-tvm&#34;&gt;1. Install TVM&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://tvm.apache.org/docs/install/index.html&#34; target=&#34;_blank&#34;&gt;TVM&lt;/a&gt;  is a critical component for MLC LLM. You can install it locally using pip:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://vllm.ai/&#34; target=&#34;_blank&#34;&gt;vLLM&lt;/a&gt; is an open-source library designed for rapid LLM (Large Language Model) inference and deployment. It leverages their novel algorithm called &lt;strong&gt;PagedAttention&lt;/strong&gt;, which optimizes the management of attention keys and values.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;In this blog post, I will share my experience of utilizing vLLM on a WSL (Windows Subsystem for Linux) instance running Ubuntu 22.04. Let&amp;rsquo;s start by setting up the environment:&lt;/p&gt;&#xA;&lt;h3 id=&#34;installing-wsl-and-configuring-ubuntu&#34;&gt;Installing WSL and Configuring Ubuntu&lt;/h3&gt;&#xA;&lt;p&gt;Begin by installing WSL and configuring it to use Ubuntu as the default distribution:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unleashing the Power of LLaMA Server in Docker Container</title>
      <link>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</link>
      <pubDate>Sat, 15 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</guid>
      <description>&lt;p&gt;Having recently completed the enlightening &lt;a href=&#34;https://www.coursera.org/learn/generative-ai-with-llms&#34; target=&#34;_blank&#34;&gt;Generative AI with Large Language Models&lt;/a&gt; course, where we gained invaluable knowledge and hands-on skills, we are now excited to share an exhilarating experience of running the LLaMA model in a Dockerized container.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, we&amp;rsquo;ll walk you through the setup and demonstrate how to unleash the full potential of running LLaMA Server within a Docker container.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;the-setup&#34;&gt;The Setup&lt;/h2&gt;&#xA;&lt;p&gt;Before we delve into the magic of LLaMA, let&amp;rsquo;s set up our application structure. To ensure smooth execution, we&amp;rsquo;ve structured our project as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (II)</title>
      <link>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</link>
      <pubDate>Fri, 16 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</guid>
      <description>&lt;p&gt;In continuation with the &lt;a href=&#34;https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/&#34; target=&#34;_blank&#34;&gt;previous&lt;/a&gt; post, we will explore the power of AI by leveraging the &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34; target=&#34;_blank&#34;&gt;whisper.cpp&lt;/a&gt; library to convert audio to text, extracting audio from YouTube videos using &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34; target=&#34;_blank&#34;&gt;yt-dlp&lt;/a&gt;, and demonstrating how to utilize AI models like GPT4All and OpenAI for summarization.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-the-environment&#34;&gt;Setting Up the Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started, we need to set up the necessary tools and libraries. Follow the steps below:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (I)</title>
      <link>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/</link>
      <pubDate>Sat, 10 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/</guid>
      <description>&lt;p&gt;Hey there, readers! Today, I&amp;rsquo;m thrilled to introduce you to an incredible tool that will completely transform the way you summarize YouTube videos. Get ready to dive into the captivating world of video content summarization using the powerful GPT4All. Trust me, this is an opportunity you don&amp;rsquo;t want to miss!&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-the-magic&#34;&gt;Setting up the Magic&lt;/h2&gt;&#xA;&lt;p&gt;Before we embark on this exciting journey, let&amp;rsquo;s ensure we have everything we need to get started. Install the necessary dependencies by running the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Receipt OCR with LangChain, OpenAI and PyTesseract</title>
      <link>https://seehiong.github.io/archives/2023/receipt-ocr-with-langchain-and-openai/</link>
      <pubDate>Tue, 06 Jun 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/receipt-ocr-with-langchain-and-openai/</guid>
      <description>&lt;p&gt;Recently, I embarked on an exhilarating journey into the realm of receipt OCR using LangChain and OpenAI, inspired by the captivating course on &lt;a href=&#34;https://learn.deeplearning.ai/langchain/lesson/1/introduction&#34; target=&#34;_blank&#34;&gt;LangChain for LLM Application Development&lt;/a&gt;. This exploration allowed me to unlock the full potential of PyTesseract, an extraordinary Python tool that serves as my guiding light for optical character recognition (OCR). By harnessing the power of OpenCV and seamlessly integrating OpenAI into the workflow, I aimed to compile the most optimal OCR results and validate them using LangChain&amp;rsquo;s impressive llm-math tool. Join me on this exciting adventure as we unravel the intricacies of receipt OCR and discover the true potential of LangChain, OpenAI, and PyTesseract.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Autofill PDF with LangChain and LangFlow</title>
      <link>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</link>
      <pubDate>Fri, 26 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</guid>
      <description>&lt;p&gt;In this blog post, we will explore the usage of &lt;a href=&#34;https://pypi.org/project/langflow/&#34; target=&#34;_blank&#34;&gt;LangFlow&lt;/a&gt;, a Python library available on PyPI, to streamline the process of capturing ideas and conducting proof-of-concepts for our intended use case. Considering the current &amp;ldquo;trend&amp;rdquo; of tech layoffs, there might be a time (touch wood) where there is a need to go for interviews and fill-up various interview forms that require filling out personal information. Building upon the previous blog post on running GPT4All for PostgreSQL with LangChain (referenced &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), we will now leverage LangFlow and OpenAI to automate the population of a sample employment form with our personal data stored in PostgreSQL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running GPT4All for your PostgreSQL with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/</link>
      <pubDate>Sun, 21 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/</guid>
      <description>&lt;p&gt;In this post, I will walk you through the process of setting up &lt;a href=&#34;https://github.com/nomic-ai/gpt4all/blob/main/gpt4all-bindings/python/README.md&#34; target=&#34;_blank&#34;&gt;Python GPT4All&lt;/a&gt; on my Windows PC. Additionally, I will demonstrate how to utilize the power of GPT4All along with &lt;a href=&#34;https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html&#34; target=&#34;_blank&#34;&gt;SQL Chain&lt;/a&gt; for querying a postgreSQL database.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;&#xA;&lt;p&gt;Before we proceed with the installation process, it is important to have the necessary prerequisites in place.&lt;/p&gt;&#xA;&lt;p&gt;To follow along with this guide, make sure you have the following:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA server in local machine</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</link>
      <pubDate>Sat, 13 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</guid>
      <description>&lt;p&gt;Referencing the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, we will run a web server which aims to act as a drop-in replacement for the OpenAI API, which can in turn be used by &lt;a href=&#34;https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/&#34; target=&#34;_blank&#34;&gt;byogpt&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(3 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://packaging.python.org/en/latest/key_projects/#pipenv&#34; target=&#34;_blank&#34;&gt;Pipenv&lt;/a&gt; aims to help users manage environments, dependencies and imported packages and I will be using it in this guide.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install pipenv uvicorn fastapi sse_starlette&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipenv shell&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Building ChatBot for your PDF files with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</link>
      <pubDate>Sun, 07 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</guid>
      <description>&lt;p&gt;Extending the use case on the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I will demostrate how you could ingest your own PDF file to your own &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;LLaMa model in local machine&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start off by installing &lt;a href=&#34;https://github.com/chroma-core/chroma&#34; target=&#34;_blank&#34;&gt;Chroma&lt;/a&gt;, the open-source embedding database:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install chromadb pypdf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pdf/langchain-chromadb-install.png&#34; alt=&#34;langchain-chromadb-install&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;ingesting-your-pdf&#34;&gt;Ingesting your PDF&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 mins)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a basic Chain with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</link>
      <pubDate>Mon, 01 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://js.langchain.com/docs/&#34; target=&#34;_blank&#34;&gt;LangChain&lt;/a&gt; is a framework for developing applications powered by language models. With the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; setup, I will follow closely to using &lt;a href=&#34;https://python.langchain.com/en/latest/ecosystem/llamacpp.html&#34; target=&#34;_blank&#34;&gt;Llama.cpp within LangChain&lt;/a&gt; for building the simplest form of chain with LangChain.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, installs the required python packages:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo pip install llama-cpp-python langchain &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/langchain/langchain-llama-dependencies.png&#34; alt=&#34;langchain-llama-dependencies&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA model locally</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-model-locally/</link>
      <pubDate>Sun, 30 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-model-locally/</guid>
      <description>&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(30 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34; target=&#34;_blank&#34;&gt;LLaMA&lt;/a&gt; is a collection of foundation language models ranging from 7B to 65B parameters.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will be using and following &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34; target=&#34;_blank&#34;&gt;Georgi Gergano&amp;rsquo;s llama.cpp&lt;/a&gt;, a inference of LLaMA model in pure C/C++.&lt;/p&gt;&#xA;&lt;p&gt;I will be setting this up in a Ubuntu machine with 32Gb.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/llama/hp-system-info.png&#34; alt=&#34;hp-system-info&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;To prepare for the build system, I installed these:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
