<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mistral on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/mistral/</link>
    <description>Recent content in Mistral on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 10 Dec 2023 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/mistral/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>In this post, I explored enhancing Autogen&amp;rsquo;s capabilities by enabling seamless Java code execution. Drawing inspiration from 0xlws&amp;rsquo; fork supporting JavaScript, I embarked on modifying Autogen to robustly support Java. I detailed the setup process, including installing Java on Windows Subsystem for Linux (WSL) and modifying key files. The post includes code snippets showcasing the changes, recompilation steps, and instructions for generating Java code. I extended functionality to additional test cases, seamlessly switching between Java and Python code execution. Docker integration for Java code execution was also optimized, showcasing Autogen&amp;rsquo;s versatility and robust development experience.</description>
    </item>
    <item>
      <title>Exploring AutoGen with LM Studio and Local LLM</title>
      <link>https://seehiong.github.io/posts/2023/12/exploring-autogen-with-lm-studio-and-local-llm/</link>
      <pubDate>Sat, 02 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/exploring-autogen-with-lm-studio-and-local-llm/</guid>
      <description>I explored AutoGen, an innovative framework on GitHub, enabling the development of Large Language Model (LLM) applications. Collaborating with LM Studio, I set up a local LLM application, showcasing the step-by-step process. Installing LM Studio involved configuring context length, enabling GPU acceleration, and setting CPU threads. The integration process showcased a seamless environment for running local LLMs. Additionally, I explored the AutoGen setup, including installing Anaconda and creating a virtual environment. With the provided guidelines, I executed the app.py script, generating a stock price comparison chart through AutoGen&amp;rsquo;s dynamic conversation.</description>
    </item>
  </channel>
</rss>
