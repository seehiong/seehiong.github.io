<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/docker/</link>
    <description>Recent content in Docker on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 23 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Log Management with Graylog</title>
      <link>https://seehiong.github.io/posts/2024/04/log-management-with-graylog/</link>
      <pubDate>Fri, 19 Apr 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/04/log-management-with-graylog/</guid>
      <description>Explore setting up Graylog in your HomeLab for comprehensive log management. Configure MongoDB and OpenSearch, deploy Fluent Bit for log forwarding, and implement advanced features like Grok patterns and pipelines. Troubleshoot efficiently with tools like netshoot and tcpdump. Enhance your HomeLab environment with a scalable and efficient log management solution.</description>
    </item>
    <item>
      <title>AI Integration: LocalAI, Chroma, and Langchain4j</title>
      <link>https://seehiong.github.io/posts/2023/12/ai-integration-localai-chroma-and-langchain4j/</link>
      <pubDate>Fri, 29 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/ai-integration-localai-chroma-and-langchain4j/</guid>
      <description>Explore AI integration in a home lab with LocalAI, Chroma, and Langchain4j. Begin by creating a custom LocalAI image, deploying it alongside Chroma, and configuring the Kubernetes environment. The post details deploying and exposing services, ensuring seamless communication between applications. Learn to modify endpoints in the Langchain4j application for smooth integration with the Home Lab setup. With a focus on simplicity, this guide empowers users to harness the capabilities of these AI tools within a controlled home environment, fostering experimentation and development.</description>
    </item>
    <item>
      <title>GitLab Setup: Installation, Migration, and CI/CD Simplified</title>
      <link>https://seehiong.github.io/posts/2023/12/gitlab-setup-installation-migration-and-ci/cd-simplified/</link>
      <pubDate>Sun, 24 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/gitlab-setup-installation-migration-and-ci/cd-simplified/</guid>
      <description>&lt;p&gt;In this guide, I&amp;rsquo;ll walk you through the process of installing &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://docs.gitlab.com/omnibus/installation/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;GitLab&lt;/a&gt;&#xD;&#xA;, a comprehensive suite of tools for version control, continuous integration, continuous delivery, and more, in my Home Lab collection.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After obtaining the latest &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;Ubuntu Server&lt;/a&gt;&#xD;&#xA;, I utilized &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://rufus.ie/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;Rufus&lt;/a&gt;&#xD;&#xA;, a utility for formatting and creating bootable USB flash drives.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Following the &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://packages.gitlab.com/gitlab/gitlab-ce/install&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;installation instructions&lt;/a&gt;&#xD;&#xA;, initiate a quick installation using the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying OpenAI-Compatible LLAMA CPP Server with K3S</title>
      <link>https://seehiong.github.io/posts/2023/12/deploying-openai-compatible-llama-cpp-server-with-k3s/</link>
      <pubDate>Fri, 22 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/deploying-openai-compatible-llama-cpp-server-with-k3s/</guid>
      <description>In this post, I expand my Home Lab by adding a perpetual LLAMA model for on-demand inferencing. The steps involve crafting a Dockerfile, packaging Microsoft&amp;rsquo;s Phi2 model, and deploying it with K3S. This ensures a continuously accessible LLAMA server for seamless integration into various inferencing projects.</description>
    </item>
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>In this post, I explored enhancing Autogen&amp;rsquo;s capabilities by enabling seamless Java code execution. Drawing inspiration from 0xlws&amp;rsquo; fork supporting JavaScript, I embarked on modifying Autogen to robustly support Java. I detailed the setup process, including installing Java on Windows Subsystem for Linux (WSL) and modifying key files. The post includes code snippets showcasing the changes, recompilation steps, and instructions for generating Java code. I extended functionality to additional test cases, seamlessly switching between Java and Python code execution. Docker integration for Java code execution was also optimized, showcasing Autogen&amp;rsquo;s versatility and robust development experience.</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/posts/2023/08/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/08/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>Embarking on my journey with vLLM, I explore its potential for streamlined Large Language Model (LLM) inference and deployment. The blog details my personal experience setting up vLLM on a Windows Subsystem for Linux (WSL) instance running Ubuntu 22.04. I meticulously guide through installing WSL, NVIDIA GPU drivers, CUDA Toolkit, and Docker for efficient utilization. Delving into vLLM setup within the NVIDIA PyTorch Docker image, I navigate through the installation process and launch the API server. The blog provides insights into querying the model and creating a Docker image snapshot, offering a comprehensive guide to efficient language model serving.</description>
    </item>
    <item>
      <title>Setting up K3s</title>
      <link>https://seehiong.github.io/posts/2023/07/setting-up-k3s/</link>
      <pubDate>Sun, 30 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/07/setting-up-k3s/</guid>
      <description>In my latest blog post, I share my journey setting up K3S, a lightweight Kubernetes distribution, in my home lab. With a step-by-step guide, I install K3S on an Ubuntu Server 22.04.2 LTS, offering a seamless experience. The post covers creating useful aliases for simplifying interactions with K3S and verifying the installation. Additionally, I introduce Portainer to manage Docker and Kubernetes in my home lab. I walk through setting up Portainer, adding a Kubernetes environment, and connecting it to the K3S cluster. Furthermore, I establish a local Docker registry and demonstrate optional steps for pushing and deploying Docker images within the K3S cluster.</description>
    </item>
    <item>
      <title>Unleashing the Power of LLaMA Server in Docker Container</title>
      <link>https://seehiong.github.io/posts/2023/07/unleashing-the-power-of-llama-server-in-docker-container/</link>
      <pubDate>Sat, 15 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/07/unleashing-the-power-of-llama-server-in-docker-container/</guid>
      <description>After completing the Generative AI with Large Language Models course, I&amp;rsquo;m thrilled to share my Dockerized experience running the LLaMA model. The guide covers setting up the project structure, creating a FastAPI application, and Dockerizing it. Additionally, I showcase building an AI chatbot, integrating it with FastAPI, HuggingFace embeddings, and LLaMA. The Docker environment loads the LLM and allows seamless interactions with PDFs. I conclude by enhancing performance with OpenBLAS, significantly reducing inferencing time. Explore the power of LLaMA Server in a Docker container for transformative AI experiences! ðŸš€</description>
    </item>
    <item>
      <title>HA K8s Pi Cluster (II)</title>
      <link>https://seehiong.github.io/posts/2020/08/ha-k8s-pi-cluster-ii/</link>
      <pubDate>Mon, 17 Aug 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2020/08/ha-k8s-pi-cluster-ii/</guid>
      <description>In my journey to establish a Highly Available Kubernetes Pi Cluster, I&amp;rsquo;ve successfully configured the cluster following an external etcd setup. The process involves installing Docker, setting up the Docker daemon, and installing kubeadm. Initializing Kubernetes Master Nodes, preparing certificates, and configuring Calico for networking are key steps. Troubleshooting tips include addressing refused connections and certificate expiration. To rejoin a faulty node, cordoning, draining, and generating new keys are essential. Now, I proudly own a fully operational Highly Available Kubernetes Pi Cluster.</description>
    </item>
    <item>
      <title>Gitea for K8s Cluster</title>
      <link>https://seehiong.github.io/posts/2020/07/gitea-for-k8s-cluster/</link>
      <pubDate>Fri, 10 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2020/07/gitea-for-k8s-cluster/</guid>
      <description>In my recent endeavor, I spent 40 minutes setting up Gitea on my Kubernetes Pi cluster, granting me absolute control over personal Git repositories. I seamlessly integrated MySQL, using Docker images and Kubernetes configurations. The meticulous setup involved creating necessary paths on an external HDD, configuring persistent volumes, and ensuring a smooth deployment. I prepared MySQL for Gitea, creating a user, database, and granting privileges. Gitea installation via Docker and subsequent exposure to external access using NodePort were executed flawlessly. A troubleshooting tip addressed a MySQL access issue. Now, my Gitea on Kubernetes Pi Cluster is fully operational for efficient repository management.</description>
    </item>
    <item>
      <title>Kubernetes Cluster on Pi</title>
      <link>https://seehiong.github.io/posts/2020/07/kubernetes-cluster-on-pi/</link>
      <pubDate>Sat, 04 Jul 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2020/07/kubernetes-cluster-on-pi/</guid>
      <description>I recently spent 70 minutes setting up a Kubernetes Cluster on Raspberry Pi using Ubuntu Server 20.04 LTS. After burning the OS image and configuring a headless setup, I updated the OS, changed the hostname, and enabled memory cgroup. The Docker installation involved setting up external storage and configuring Docker daemon. Installing kubeadm and creating the Kubernetes cluster took an additional 45 minutes. I verified the cluster status, installed networking addons (Calico), and added leaf nodes. Troubleshooting included resolving conntrack and socat issues. Overall, the Raspberry Pi Kubernetes Cluster provides full control over Docker container orchestration.</description>
    </item>
    <item>
      <title>Gitea for MicroK8s Cluster</title>
      <link>https://seehiong.github.io/posts/2020/06/gitea-for-microk8s-cluster/</link>
      <pubDate>Mon, 29 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2020/06/gitea-for-microk8s-cluster/</guid>
      <description>I dedicated 45 minutes to set up Gitea on my Raspberry Pi cluster using MicroK8s. Gitea, a self-hosted Git service, grants my team cost savings and enhanced server control. I ensured a smooth installation by preparing MySQL, creating required paths on an external HDD, and configuring persistent volumes. Following MySQL setup, I seamlessly prepared the database and moved on to setting up Gitea with Docker, utilizing a docker-compose.yml file. After injecting images into MicroK8s cache, I exposed Gitea externally using NodePort. Troubleshooting tips were handy, addressing MySQL connection issues. My self-hosted Git service is now ready for efficient collaboration.</description>
    </item>
    <item>
      <title>Docker for MicroK8s Cluster</title>
      <link>https://seehiong.github.io/posts/2020/06/docker-for-microk8s-cluster/</link>
      <pubDate>Sun, 21 Jun 2020 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2020/06/docker-for-microk8s-cluster/</guid>
      <description>Setting up Docker on my Raspberry Pi Cluster took just 15 minutes. After installing Docker, I added the ubuntu user to the Docker group. Configuring Docker included adjusting the daemon settings for external storage. Testing Docker with a hello-world container went smoothly. To use local images for MicroK8s, I exported and injected the image successfully. Troubleshooting involved resolving daemon start errors, addressing connection issues, and handling permission errors. Formatting the existing NTFS HDD to ext4 and adjusting boot-up settings resolved challenges, making Docker work seamlessly on my Raspberry Pi Cluster.</description>
    </item>
  </channel>
</rss>
