<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CUDA on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/cuda/</link>
    <description>Recent content in CUDA on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 03 Feb 2024 10:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/cuda/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Stable Diffusion: Text-to-Image Modeling Journey</title>
      <link>https://seehiong.github.io/archives/2024/stable-diffusion-text-to-image-modeling-journey/</link>
      <pubDate>Sat, 03 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/stable-diffusion-text-to-image-modeling-journey/</guid>
      <description>This post explores Stable Diffusion, a latent text-to-image diffusion model in machine learning. Diffusion models, with forward, reverse, and sampling components, understand and generate patterns in datasets. Illustrating applications in image tasks, it introduces the process of installing and utilizing Stable Diffusion. The post details image generation and modification using prompts, with examples and troubleshooting. Notably, it shares an encounter with CUDA out-of-memory errors and the resolution through image resizing. Overall, it offers a comprehensive guide, combining theoretical insights with practical implementation steps in a professional manner.</description>
    </item>
    <item>
      <title>Boosting Inference Speed: SSD and GPU Acceleration</title>
      <link>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</link>
      <pubDate>Thu, 30 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/boosting-inference-speed-ssd-and-gpu-acceleration/</guid>
      <description>&lt;p&gt;In the relentless pursuit of optimal disk space and lightning-fast inference speeds, I embarked on an exciting upgrade journey by integrating the formidable &lt;a href=&#34;https://www.lexar.com/product/lexar-nm790-m-2-2280-pcie-gen-4x4-nvme-ssd/&#34; target=&#34;_blank&#34;&gt;Lexar NM790 M.2 2280 PCIe SSD&lt;/a&gt;. This blog post unfolds in two parts: the first chronicles the meticulous migration of my Windows 11 to this powerhouse SSD, while the second unveils the secrets behind the enhanced inferencing speed for the &lt;a href=&#34;https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;Langchain4j application&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;part-1-seamless-os-migration-with-clonezilla&#34;&gt;Part 1: Seamless OS Migration with Clonezilla&lt;/h2&gt;&#xA;&lt;p&gt;Amidst a sea of software promising seamless disk cloning, I found solace in the reliability of &lt;a href=&#34;https://clonezilla.org/&#34; target=&#34;_blank&#34;&gt;Clonezilla&lt;/a&gt;, a robust open-source tool for disk imaging and cloning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://vllm.ai/&#34; target=&#34;_blank&#34;&gt;vLLM&lt;/a&gt; is an open-source library designed for rapid LLM (Large Language Model) inference and deployment. It leverages their novel algorithm called &lt;strong&gt;PagedAttention&lt;/strong&gt;, which optimizes the management of attention keys and values.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;In this blog post, I will share my experience of utilizing vLLM on a WSL (Windows Subsystem for Linux) instance running Ubuntu 22.04. Let&amp;rsquo;s start by setting up the environment:&lt;/p&gt;&#xA;&lt;h3 id=&#34;installing-wsl-and-configuring-ubuntu&#34;&gt;Installing WSL and Configuring Ubuntu&lt;/h3&gt;&#xA;&lt;p&gt;Begin by installing WSL and configuring it to use Ubuntu as the default distribution:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
