<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GPT on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/gpt/</link>
    <description>Recent content in GPT on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 31 Oct 2024 10:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPT-2 Training Guide</title>
      <link>https://seehiong.github.io/posts/2024/10/gpt-2-training-guide/</link>
      <pubDate>Thu, 31 Oct 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/10/gpt-2-training-guide/</guid>
      <description>This post documents my journey training GPT-2 on the Tiny Shakespeare dataset, inspired by Andrej Karpathy&amp;rsquo;s instructional video and nanoGPT repository. Following each step, I detail the setup process, from encoding data and optimizing the model with AdamW, to improving training stability with mixed precision and flash attention. The post includes practical insights on using pretrained weights, weight sharing, and efficient data handling, concluding with sample outputs from training and sampling a “baby” GPT model.</description>
    </item>
    <item>
      <title>GPT-2 Setup and Pretraining Guide</title>
      <link>https://seehiong.github.io/posts/2024/10/gpt-2-setup-and-pretraining-guide/</link>
      <pubDate>Mon, 28 Oct 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/10/gpt-2-setup-and-pretraining-guide/</guid>
      <description>This guide explores reproducing GPT-2 (124M) using Andrej Karpathy’s video walkthrough. It begins with an overview of the GPT-2 architecture, a decoder-only transformer model inspired by &amp;ldquo;Attention Is All You Need.&amp;rdquo; Using pretrained GPT-2 weights, we analyze and initialize a custom GPT class, with detailed steps to handle token embeddings, causal attention, and layer normalization. The guide includes code for generating text from pretrained weights. In the next segment, we’ll continue with a deeper dive into dataset preparation and training from scratch, moving from small samples to large-scale training.</description>
    </item>
    <item>
      <title>Developing BYO-GPT with Flutter</title>
      <link>https://seehiong.github.io/posts/2023/04/developing-byo-gpt-with-flutter/</link>
      <pubDate>Sat, 29 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/04/developing-byo-gpt-with-flutter/</guid>
      <description>I dedicate around 10 minutes to create BYO-GPT, a Flutter app that allows easy interaction with ChatGPT through OpenAI&amp;rsquo;s API. After installing Flutter, setting up the project, and creating necessary widgets and models, I utilize the OpenAI API for chat completion. The app includes user and GPT message bubbles, as well as a user input section with a GPT icon. By employing the Provider package, the app efficiently manages state changes. Additionally, I provide the option to switch models for experimentation. Overall, BYO-GPT provides a user-friendly interface for seamless communication with ChatGPT.</description>
    </item>
  </channel>
</rss>
