<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GPT on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/gpt/</link>
    <description>Recent content in GPT on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Oct 2024 10:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPT-2 Training Guide</title>
      <link>https://seehiong.github.io/2024/gpt-2-training-guide/</link>
      <pubDate>Thu, 31 Oct 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/gpt-2-training-guide/</guid>
      <description>This post documents my journey training GPT-2 on the Tiny Shakespeare dataset, inspired by Andrej Karpathy&amp;rsquo;s instructional video and nanoGPT repository. Following each step, I detail the setup process, from encoding data and optimizing the model with AdamW, to improving training stability with mixed precision and flash attention. The post includes practical insights on using pretrained weights, weight sharing, and efficient data handling, concluding with sample outputs from training and sampling a “baby” GPT model.</description>
    </item>
    <item>
      <title>GPT-2 Setup and Pretraining Guide</title>
      <link>https://seehiong.github.io/2024/gpt-2-setup-and-pretraining-guide/</link>
      <pubDate>Mon, 28 Oct 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/gpt-2-setup-and-pretraining-guide/</guid>
      <description>This guide explores reproducing GPT-2 (124M) using Andrej Karpathy’s video walkthrough. It begins with an overview of the GPT-2 architecture, a decoder-only transformer model inspired by &amp;ldquo;Attention Is All You Need.&amp;rdquo; Using pretrained GPT-2 weights, we analyze and initialize a custom GPT class, with detailed steps to handle token embeddings, causal attention, and layer normalization. The guide includes code for generating text from pretrained weights. In the next segment, we’ll continue with a deeper dive into dataset preparation and training from scratch, moving from small samples to large-scale training.</description>
    </item>
    <item>
      <title>Developing BYO-GPT with Flutter</title>
      <link>https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/</link>
      <pubDate>Sat, 29 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/byogpt/byo-gpt-feature.png&#34; alt=&#34;byo-gpt-feature&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Developing a user-friendly interface to converse with ChatGPT via OpenAI&amp;rsquo;s API with your own openAI API token.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;developing-byo-gpt-with-flutter&#34;&gt;Developing BYO-GPT with Flutter&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(Total Setup Time: 10 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this post, I will develop a &amp;ldquo;Bring Your Own - Generative Pre-Trained Transformer&amp;rdquo;, a user-friendly interface to converse with ChatGPT via &lt;a href=&#34;https://platform.openai.com/&#34; target=&#34;_blank&#34;&gt;OpenAI&amp;rsquo;s API&lt;/a&gt; with &lt;a href=&#34;https://flutter.dev/&#34; target=&#34;_blank&#34;&gt;Flutter&lt;/a&gt;. You may download &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.seehiong.byogpt&#34; target=&#34;_blank&#34;&gt;BYO-GPT&lt;/a&gt; and check it out!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
