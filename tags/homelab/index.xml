<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HomeLab on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/homelab/</link>
    <description>Recent content in HomeLab on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 20 Oct 2024 08:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/homelab/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Integrating MLflow and Kubeflow on Talos</title>
      <link>https://seehiong.github.io/posts/2024/10/integrating-mlflow-and-kubeflow-on-talos/</link>
      <pubDate>Sun, 20 Oct 2024 08:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/10/integrating-mlflow-and-kubeflow-on-talos/</guid>
      <description>This post details the installation of MLflow and Kubeflow on a Talos HomeLab cluster. It covers the setup process, including Talos configuration, local-path and NFS provisioning, and Metallb installation. Step-by-step instructions are provided for deploying Kubeflow, followed by the installation of MLflow for managing the machine learning lifecycle. Finally, the post illustrates how to log experiments and models in MLflow and perform inference, demonstrating a seamless integration of these tools for enhanced machine learning operations.</description>
    </item>
    <item>
      <title>Talos Linux: Setting Up a Secure, Immutable Kubernetes Cluster</title>
      <link>https://seehiong.github.io/posts/2024/09/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</link>
      <pubDate>Sun, 01 Sep 2024 08:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/09/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</guid>
      <description>This post guides you through setting up a secure, immutable Kubernetes cluster using Talos Linux. It covers installing Talos on control and worker nodes, configuring local storage with hostPath and Local Path Provisioner, and setting up the Kubernetes Dashboard with an admin user for cluster management. With Talos Linux, you achieve a minimal, API-managed Kubernetes environment without SSH or systemd, making it ideal for a secure and reliable homelab or production setup.</description>
    </item>
    <item>
      <title>Setting Up Kafka with MicroK8s and Multipass</title>
      <link>https://seehiong.github.io/posts/2024/08/setting-up-kafka-with-microk8s-and-multipass/</link>
      <pubDate>Sat, 03 Aug 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/08/setting-up-kafka-with-microk8s-and-multipass/</guid>
      <description>My homelab is a playground for experimenting with various tools and setups. However, for Proof of Concept (POC) environments, a lightweight and portable setup is often more suitable. In this post, I will guide you through setting up a MicroK8s environment in a virtual machine using Multipass. This POC demonstrates how Kafka can be set up in this environment.</description>
    </item>
    <item>
      <title>Building Your First Kubeflow Pipeline: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/posts/2024/07/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</link>
      <pubDate>Sat, 20 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/07/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</guid>
      <description>In this blog post, I guide you through creating and running your first Kubeflow pipeline. We&amp;rsquo;ll start with the &amp;ldquo;Hello World&amp;rdquo; example, demonstrate how to manage sequential and shared pipelines, and explore artifact storage with MinIO. Additionally, I&amp;rsquo;ll introduce K9s, a powerful terminal-based UI for managing your Kubernetes clusters efficiently. By the end, you&amp;rsquo;ll have a solid understanding of setting up and managing Kubeflow pipelines in your machine learning workflows.</description>
    </item>
    <item>
      <title>Integrating Draw.io and PlantUML with GitLab</title>
      <link>https://seehiong.github.io/posts/2024/07/integrating-draw.io-and-plantuml-with-gitlab/</link>
      <pubDate>Sat, 06 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/07/integrating-draw.io-and-plantuml-with-gitlab/</guid>
      <description>As we transition from Lucidchart to draw.io for team diagramming, this guide outlines the steps to integrate draw.io and PlantUML with GitLab. I&amp;rsquo;ll configure the Diagrams.net server, enable integration, and demonstrate creating and editing diagrams within GitLab. Additionally, I&amp;rsquo;ll cover the setup and integration of PlantUML for creating detailed design diagrams. Follow along to seamlessly incorporate these powerful diagramming tools into your GitLab workflow.</description>
    </item>
    <item>
      <title>Setting Up and Using KServe with Kubeflow</title>
      <link>https://seehiong.github.io/posts/2024/06/setting-up-and-using-kserve-with-kubeflow/</link>
      <pubDate>Sun, 30 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/06/setting-up-and-using-kserve-with-kubeflow/</guid>
      <description>In this post, we explore KServe, a model inference platform on Kubernetes designed for scalability. Building on our previous Kubeflow guide, we detail how to set up your first KServe endpoint, make predictions, and troubleshoot common issues. Follow our step-by-step instructions to seamlessly integrate KServe with your Kubeflow environment and enhance your machine learning deployment process.</description>
    </item>
    <item>
      <title>Setting Up Kubeflow on Kubernetes: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/posts/2024/06/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</link>
      <pubDate>Mon, 24 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/06/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</guid>
      <description>In this post, I provide a comprehensive guide to setting up Kubeflow, a machine learning toolkit for Kubernetes. From initial preparation and downloading necessary binaries to installing all Kubeflow components and troubleshooting common issues, this step-by-step tutorial ensures a smooth installation process. You&amp;rsquo;ll also learn how to create your first notebook and resolve potential errors, making it easier to leverage Kubeflow&amp;rsquo;s powerful features for your machine learning projects.</description>
    </item>
    <item>
      <title>Planning Gift Deliveries With QGIS</title>
      <link>https://seehiong.github.io/posts/2024/06/planning-gift-deliveries-with-qgis/</link>
      <pubDate>Sat, 08 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/06/planning-gift-deliveries-with-qgis/</guid>
      <description>In this post, I document my journey of using QGIS, a free and open-source geographic information system, to plan gift deliveries. I outline the steps to install QGIS, add essential plugins, create a shapefile layer for mapping locations, and use ORS Tools for route planning. After configuring the map, I determined that my delivery route would take 3 hours. This process, while tailored to my use case, is versatile and applicable to various fields such as logistics, urban planning, and environmental management. QGIS provides robust tools for efficient spatial analysis and mapping tasks.</description>
    </item>
    <item>
      <title>Log Management with Graylog</title>
      <link>https://seehiong.github.io/posts/2024/04/log-management-with-graylog/</link>
      <pubDate>Fri, 19 Apr 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/04/log-management-with-graylog/</guid>
      <description>Explore setting up Graylog in your HomeLab for comprehensive log management. Configure MongoDB and OpenSearch, deploy Fluent Bit for log forwarding, and implement advanced features like Grok patterns and pipelines. Troubleshoot efficiently with tools like netshoot and tcpdump. Enhance your HomeLab environment with a scalable and efficient log management solution.</description>
    </item>
    <item>
      <title>Configuring Appwrite Functions with K3s</title>
      <link>https://seehiong.github.io/posts/2024/03/configuring-appwrite-functions-with-k3s/</link>
      <pubDate>Sun, 10 Mar 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/03/configuring-appwrite-functions-with-k3s/</guid>
      <description>This guide outlines configuring Appwrite Functions within a K3s environment. It covers essential steps, including installing ngrok for external network access, registering a GitHub App, and setting up Appwrite with required configurations. The process involves updating YAML files for deployment, ensuring proper routing with Traefik, and creating functions through the Appwrite interface. Once set up, the functions are deployed successfully, enabling seamless integration and execution within the K3s infrastructure.</description>
    </item>
    <item>
      <title>Deploying Budibase in HomeLab</title>
      <link>https://seehiong.github.io/posts/2024/02/deploying-budibase-in-homelab/</link>
      <pubDate>Sun, 25 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/02/deploying-budibase-in-homelab/</guid>
      <description>This post outlines the process of installing Budibase in a HomeLab environment, starting with testing it on Docker Desktop and then deploying it using Helm in Kubernetes. It guides through setting up an admin user, building the first app by creating a database, designing an application form, and configuring submission actions. The summary encapsulates the steps involved in testing, deploying, and building an application with Budibase, highlighting key actions such as Docker Compose setup, Helm installation, app creation, and deployment in a concise manner.</description>
    </item>
    <item>
      <title>Deploying Appwrite in HomeLab with K3s</title>
      <link>https://seehiong.github.io/posts/2024/02/deploying-appwrite-in-homelab-with-k3s/</link>
      <pubDate>Fri, 16 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/02/deploying-appwrite-in-homelab-with-k3s/</guid>
      <description>Learn how to seamlessly integrate Appwrite, an open-source platform, into your HomeLab setup using K3s. Follow step-by-step instructions to deploy K3s with Portainer, prepare Appwrite volumes, and configure miscellaneous services like MariaDB and InfluxDB. Utilize Kompose to convert Docker Compose files to Kubernetes for efficient deployment. Ensure smooth installation by mapping necessary environment variables and applying all required deployments and services. Finally, witness the successful deployment of Appwrite services and access the login page to start building scalable applications. Master the art of HomeLab application deployment with Appwrite and K3s.</description>
    </item>
    <item>
      <title>Java Integration with Jupyter Notebooks</title>
      <link>https://seehiong.github.io/posts/2024/01/java-integration-with-jupyter-notebooks/</link>
      <pubDate>Sun, 21 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/java-integration-with-jupyter-notebooks/</guid>
      <description>Embark on a seamless integration of Java into Jupyter notebooks with this comprehensive guide. Beginning with the selection of a relevant Jupyter Docker Stack, the post details setup steps and deployment in HomeLab, showcasing application results for verification. The integration of Java Kernel through JBang and testing with &amp;ldquo;Hello World&amp;rdquo; and Apache Commons library exemplifies the versatility. Further exploration involves experimenting with Java in a Python kernel using JBang. Concluding with a call to joyful coding, this journey promises a harmonious blend of Java&amp;rsquo;s robustness and Jupyter&amp;rsquo;s interactive nature. Discover the joy of coding in this enriched Java-in-Jupyter experience. Happy coding!</description>
    </item>
    <item>
      <title>Exploring Autogen Studio</title>
      <link>https://seehiong.github.io/posts/2024/01/exploring-autogen-studio/</link>
      <pubDate>Sun, 14 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/exploring-autogen-studio/</guid>
      <description>In this exploration of Autogen Studio, we navigated through the AI landscape, harnessing the LM Studio API to compare responses from diverse language models. Employing the Mistral Instruct 7B model, we scrutinized prompts like Stock Price and Paint, visualizing outcomes and delving into key configurations. The post also offered insights into the primary assistant, model configuration, and agent workflows, accompanied by a comparative analysis of Mistral model responses. This comprehensive journey demystifies the power of Autogen Studio and its seamless integration with LM Studio API, providing practical guidance for AI enthusiasts.</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/posts/2024/01/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>In this post, we explored deploying Lightweight Language Models (LLMs) using WasmEdge, a high-performance WebAssembly runtime, within a HomeLab environment. The process involved preparing an OpenAI-compatible API server, configuring the Wasi-NN plugin, and deploying the setup to HomeLab using Kubernetes (K3s). The post also detailed the steps for testing the API server and integrating it into a Java application. Overall, the guide provides a comprehensive walkthrough of hosting and utilizing LLMs with WasmEdge in a local environment.</description>
    </item>
    <item>
      <title>Integrating NFS for Improved Scalability</title>
      <link>https://seehiong.github.io/posts/2024/01/integrating-nfs-for-improved-scalability/</link>
      <pubDate>Sun, 07 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/integrating-nfs-for-improved-scalability/</guid>
      <description>In this post, we explored the integration of NFS to enhance scalability in deploying LLM models within a home lab. Setting up NFS involved connecting to a TerraMaster NAS, and the K3s cluster was configured to dynamically provision storage. With NFS in place, the deployment process became more efficient, eliminating the need to rebuild images for each new model introduction. The post detailed the setup steps, from configuring NFS and K3s to deploying LLM models dynamically. This approach simplifies the scaling of Language Models, providing a centralized and scalable storage solution through NFS in a Kubernetes environment.</description>
    </item>
    <item>
      <title>Integration of Kong into AI Workflow</title>
      <link>https://seehiong.github.io/posts/2024/01/integration-of-kong-into-ai-workflow/</link>
      <pubDate>Sat, 06 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/integration-of-kong-into-ai-workflow/</guid>
      <description>This comprehensive guide navigates through configuring Kong OSS and Kong Ingress Controller (KIC), seamlessly integrating Kong into an AI workflow. Starting with Kong OSS configuration, the tutorial covers updating environment variables and service ports. The Langchain4j application is then adapted to leverage Kong API, allowing for flexible path-based APIs. Additionally, potential timeout issues are addressed. The guide concludes with a demonstration of Kong Ingress Controller configuration, emphasizing optimal settings for specific use cases. Whether through Kong OSS or KIC, readers gain insights into enhancing API management and integration within their AI workflows.</description>
    </item>
    <item>
      <title>Exploring Kong Ingress Controller (KIC)</title>
      <link>https://seehiong.github.io/posts/2024/01/exploring-kong-ingress-controller-kic/</link>
      <pubDate>Mon, 01 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/exploring-kong-ingress-controller-kic/</guid>
      <description>Embark on a journey into the new year by exploring Kong Ingress Controller (KIC) in your home lab. This guide, transitioning from a previous discussion on Kong Gateway, details the seamless setup of KIC using Helm and K3s. From initial preparations to installing Kong Ingress Controller and Gateway, witness the efficient management of APIs in your home lab environment. Learn to add Kong Ingresses, ensuring optimal routing for various paths. Through concise steps and illustrative visuals, this post simplifies the process, allowing you to experience KIC&amp;rsquo;s capabilities firsthand. Dive into the year with a hands-on exploration of API management with Kong in your home lab. Happy New Year!</description>
    </item>
    <item>
      <title>Streamlining API Management with Kong</title>
      <link>https://seehiong.github.io/posts/2023/12/streamlining-api-management-with-kong/</link>
      <pubDate>Sun, 31 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/streamlining-api-management-with-kong/</guid>
      <description>This comprehensive guide walks you through integrating Kong, a powerful unified API platform, into your home lab environment. Starting with Docker installation and a custom Kong image, it covers PostgreSQL deployment, MetalLB setup for load balancing, and Kong service and route configuration. The post concludes with troubleshooting tips and instructions for deploying your customized Kong image in a K3s cluster. This step-by-step tutorial empowers you to efficiently manage APIs in your home lab using Kong.</description>
    </item>
    <item>
      <title>AI Integration: LocalAI, Chroma, and Langchain4j</title>
      <link>https://seehiong.github.io/posts/2023/12/ai-integration-localai-chroma-and-langchain4j/</link>
      <pubDate>Fri, 29 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/ai-integration-localai-chroma-and-langchain4j/</guid>
      <description>Explore AI integration in a home lab with LocalAI, Chroma, and Langchain4j. Begin by creating a custom LocalAI image, deploying it alongside Chroma, and configuring the Kubernetes environment. The post details deploying and exposing services, ensuring seamless communication between applications. Learn to modify endpoints in the Langchain4j application for smooth integration with the Home Lab setup. With a focus on simplicity, this guide empowers users to harness the capabilities of these AI tools within a controlled home environment, fostering experimentation and development.</description>
    </item>
    <item>
      <title>GitLab Setup: Installation, Migration, and CI/CD Simplified</title>
      <link>https://seehiong.github.io/posts/2023/12/gitlab-setup-installation-migration-and-ci/cd-simplified/</link>
      <pubDate>Sun, 24 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/gitlab-setup-installation-migration-and-ci/cd-simplified/</guid>
      <description>&lt;p&gt;In this guide, I&amp;rsquo;ll walk you through the process of installing &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://docs.gitlab.com/omnibus/installation/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;GitLab&lt;/a&gt;&#xD;&#xA;, a comprehensive suite of tools for version control, continuous integration, continuous delivery, and more, in my Home Lab collection.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After obtaining the latest &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;Ubuntu Server&lt;/a&gt;&#xD;&#xA;, I utilized &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://rufus.ie/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;Rufus&lt;/a&gt;&#xD;&#xA;, a utility for formatting and creating bootable USB flash drives.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Following the &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://packages.gitlab.com/gitlab/gitlab-ce/install&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;installation instructions&lt;/a&gt;&#xD;&#xA;, initiate a quick installation using the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying OpenAI-Compatible LLAMA CPP Server with K3S</title>
      <link>https://seehiong.github.io/posts/2023/12/deploying-openai-compatible-llama-cpp-server-with-k3s/</link>
      <pubDate>Fri, 22 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/deploying-openai-compatible-llama-cpp-server-with-k3s/</guid>
      <description>In this post, I expand my Home Lab by adding a perpetual LLAMA model for on-demand inferencing. The steps involve crafting a Dockerfile, packaging Microsoft&amp;rsquo;s Phi2 model, and deploying it with K3S. This ensures a continuously accessible LLAMA server for seamless integration into various inferencing projects.</description>
    </item>
    <item>
      <title>Setting up K3s</title>
      <link>https://seehiong.github.io/posts/2023/07/setting-up-k3s/</link>
      <pubDate>Sun, 30 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/07/setting-up-k3s/</guid>
      <description>In my latest blog post, I share my journey setting up K3S, a lightweight Kubernetes distribution, in my home lab. With a step-by-step guide, I install K3S on an Ubuntu Server 22.04.2 LTS, offering a seamless experience. The post covers creating useful aliases for simplifying interactions with K3S and verifying the installation. Additionally, I introduce Portainer to manage Docker and Kubernetes in my home lab. I walk through setting up Portainer, adding a Kubernetes environment, and connecting it to the K3S cluster. Furthermore, I establish a local Docker registry and demonstrate optional steps for pushing and deploying Docker images within the K3S cluster.</description>
    </item>
  </channel>
</rss>
