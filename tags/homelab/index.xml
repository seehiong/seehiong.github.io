<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HomeLab on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/homelab/</link>
    <description>Recent content in HomeLab on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Sep 2024 08:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/homelab/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Talos Linux: Setting Up a Secure, Immutable Kubernetes Cluster</title>
      <link>https://seehiong.github.io/2024/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</link>
      <pubDate>Sun, 01 Sep 2024 08:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</guid>
      <description>&lt;p&gt;In this post, I will walk you through the process of setting up a Kubernetes cluster using &lt;a href=&#34;https://www.talos.dev/&#34; target=&#34;_blank&#34;&gt;Talos Linux&lt;/a&gt;, an operating system specifically designed for Kubernetes that is secure, immutable, and minimal by design. Talos Linux is distinguished by its unique architecture: it is hardened by default, has no shell (bash), no SSH access, and no systemd. Instead, all management is conducted through an API.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After downloading the &lt;a href=&#34;https://www.talos.dev/v1.7/talos-guides/install/bare-metal-platforms/iso/&#34; target=&#34;_blank&#34;&gt;ISO&lt;/a&gt; image, I used &lt;a href=&#34;https://etcher.balena.io/&#34; target=&#34;_blank&#34;&gt;balenaEtcher&lt;/a&gt; to create a bootable USB installation media. My setup consists of one control plane node and two worker nodes. The following IP addresses were assigned:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setting Up Kafka with MicroK8s and Multipass</title>
      <link>https://seehiong.github.io/2024/setting-up-kafka-with-microk8s-and-multipass/</link>
      <pubDate>Sat, 03 Aug 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/setting-up-kafka-with-microk8s-and-multipass/</guid>
      <description>&lt;p&gt;My homelab is a playground for experimenting with various tools and setups. However, for Proof of Concept (POC) environments, a lightweight and portable setup is often more suitable. In this post, I will guide you through setting up a MicroK8s environment in a virtual machine using Multipass. This POC demonstrates how Kafka can be set up in this environment. &lt;a href=&#34;https://multipass.run/&#34; target=&#34;_blank&#34;&gt;Multipass&lt;/a&gt; is a CLI tool for launching and managing VMs on Windows, Mac, and Linux, simulating a cloud environment with support for cloud-init.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Your First Kubeflow Pipeline: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/2024/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</link>
      <pubDate>Sat, 20 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.kubeflow.org/docs/components/pipelines/overview/&#34; target=&#34;_blank&#34;&gt;Kubeflow Pipelines (KFP)&lt;/a&gt; is a powerful platform for creating and deploying scalable machine learning (ML) workflows using Docker containers. It enables data scientists and ML engineers to author workflows in Python, manage and visualize pipeline runs, and efficiently utilize compute resources. KFP supports custom ML components, leverages existing ones, and ensures cross-platform portability with a platform-neutral &lt;a href=&#34;https://www.kubeflow.org/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/#ir-yaml&#34; target=&#34;_blank&#34;&gt;IR YAML definition&lt;/a&gt;. In this post, Iâ€™ll share my learnings about KFP v2.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integrating Draw.io and PlantUML with GitLab</title>
      <link>https://seehiong.github.io/2024/integrating-draw-io-and-plantuml-with-gitlab/</link>
      <pubDate>Sat, 06 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integrating-draw-io-and-plantuml-with-gitlab/</guid>
      <description>&lt;p&gt;As we are migrating away from &lt;a href=&#34;https://www.lucidchart.com/&#34; target=&#34;_blank&#34;&gt;Lucidchart&lt;/a&gt; to &lt;a href=&#34;https://www.drawio.com/&#34; target=&#34;_blank&#34;&gt;draw.io&lt;/a&gt;, a security-first diagramming for teams, I will be documenting the steps to integrate draw.io with GitLab.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;configure-diagramsnet-server&#34;&gt;Configure Diagrams.net Server&lt;/h2&gt;&#xA;&lt;p&gt;Referencing the official &lt;a href=&#34;https://docs.gitlab.com/ee/administration/integration/diagrams_net.html&#34; target=&#34;_blank&#34;&gt;Diagrams.net&lt;/a&gt; documentation, I run the diagrams.net container in Docker, using the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --rm --name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;draw&amp;#34;&lt;/span&gt; -p 8888:8080 -p 8443:8443 jgraph/drawio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Setting Up and Using KServe with Kubeflow</title>
      <link>https://seehiong.github.io/2024/setting-up-and-using-kserve-with-kubeflow/</link>
      <pubDate>Sun, 30 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/setting-up-and-using-kserve-with-kubeflow/</guid>
      <description>&lt;p&gt;Expanding on my previous post on &lt;a href=&#34;https://seehiong.github.io/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/&#34; target=&#34;_blank&#34;&gt;Kubeflow&lt;/a&gt;, I will explore &lt;a href=&#34;https://kserve.github.io/website/latest/&#34; target=&#34;_blank&#34;&gt;KServe&lt;/a&gt;, a standard Model Inference Platform on Kubernetes built for highly scalable use cases.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;first-kserve-endpoint&#34;&gt;First KServe Endpoint&lt;/h2&gt;&#xA;&lt;p&gt;Referencing &lt;a href=&#34;https://github.com/kserve/kserve/blob/master/docs/samples/istio-dex/README.md&#34; target=&#34;_blank&#34;&gt;KServe on Kubeflow with Istio-Dex&lt;/a&gt;, below is the &lt;em&gt;sklearn.yaml&lt;/em&gt; configuration. Note the sidecar annotation, which instructs not to inject the istio sidecar. Without this annotation, you may encounter error (refer to the troubleshooting section):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setting Up Kubeflow on Kubernetes: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</link>
      <pubDate>Mon, 24 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</guid>
      <description>&lt;p&gt;The car inspection went well, and I will spend the rest of my half-day leave documenting the steps for setting up &lt;a href=&#34;https://www.kubeflow.org/&#34; target=&#34;_blank&#34;&gt;Kubeflow&lt;/a&gt;, the machine learning toolkit for kubernetes.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt; introduces a template-free way to customize application configuration, simplifying the use of off-the-shelf application. The simplest way to get started is to download the precompiled binaries:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -s &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh&amp;#34;&lt;/span&gt;  | bash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Moves kustomize to a system-wide location&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo mv kustomize /usr/local/bin/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Planning Gift Deliveries With QGIS</title>
      <link>https://seehiong.github.io/2024/planning-gift-deliveries-with-qgis/</link>
      <pubDate>Sat, 08 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/planning-gift-deliveries-with-qgis/</guid>
      <description>&lt;p&gt;In this post, I will document my journey into learning &lt;a href=&#34;https://qgis.org/en/site/&#34; target=&#34;_blank&#34;&gt;QGIS&lt;/a&gt;, a free and open-source geographic information system, with the goal of visualizing and calculating the total time required for me to deliver gifts to friends and relatives during special occasions.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After installing QGIS 3.36 desktop version on my Windows PC, I proceeded to the &lt;a href=&#34;https://docs.qgis.org/3.34/en/docs/training_manual/index.html&#34; target=&#34;_blank&#34;&gt;QGIS Training Manual&lt;/a&gt;, to familiarize myself with the software.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Log Management with Graylog</title>
      <link>https://seehiong.github.io/2024/log-management-with-graylog/</link>
      <pubDate>Fri, 19 Apr 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/log-management-with-graylog/</guid>
      <description>&lt;p&gt;In this blog post, I&amp;rsquo;ll guide you through the setup of &lt;a href=&#34;https://graylog.org/&#34; target=&#34;_blank&#34;&gt;Graylog&lt;/a&gt;, an open-source log management platform, within a HomeLab environment, providing a comprehensive solution for log analysis and monitoring.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-graylog-with-docker&#34;&gt;Setting up Graylog with Docker&lt;/h2&gt;&#xA;&lt;p&gt;To initiate our exploration of Graylog, we&amp;rsquo;ll opt for a &lt;a href=&#34;https://go2docs.graylog.org/5-2/downloading_and_installing_graylog/docker_installation.htm&#34; target=&#34;_blank&#34;&gt;Docker Installation&lt;/a&gt;, which ensures simplicity and ease of deployment. Follow the steps outlined in the official documentation to set up Graylog via Docker. Upon successful installation, access the Graylog interface by navigating to &lt;em&gt;http://localhost:9000/&lt;/em&gt;, and use the default credentials: &lt;strong&gt;admin/admin&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configuring Appwrite Functions with K3s</title>
      <link>https://seehiong.github.io/2024/configuring-appwrite-functions-with-k3s/</link>
      <pubDate>Sun, 10 Mar 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/configuring-appwrite-functions-with-k3s/</guid>
      <description>&lt;p&gt;Following up on my previous post about &lt;a href=&#34;https://seehiong.github.io/2024/deploying-appwrite-in-homelab-with-k3s/&#34; target=&#34;_blank&#34;&gt;deploying Appwrite with K3s&lt;/a&gt;, I will now guide you through configuring K3s to support Appwrite Functions.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prepartion&#34;&gt;Prepartion&lt;/h2&gt;&#xA;&lt;h3 id=&#34;install-ngrok&#34;&gt;Install Ngrok&lt;/h3&gt;&#xA;&lt;p&gt;Since I am running Appwrite in my HomeLab, I need to utilize &lt;a href=&#34;https://ngrok.com/&#34; target=&#34;_blank&#34;&gt;ngrok&lt;/a&gt; to enable external network access (such as GitHub) to our internal network. After signing up, install ngrok via Chocolatey:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;choco install ngrok&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ngrok config add-authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ngrok http http://appwrite.local/                      &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Deploying Budibase in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-budibase-in-homelab/</link>
      <pubDate>Sun, 25 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-budibase-in-homelab/</guid>
      <description>&lt;p&gt;In this guide, we&amp;rsquo;ll delve into the process of installing &lt;a href=&#34;https://budibase.com/&#34; target=&#34;_blank&#34;&gt;Budibase&lt;/a&gt; within our HomeLab environment. Budibase offers the capability to craft robust applications and workflows from various data sources, enabling the secure deployment of professional-grade solutions across our teams.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;testing-budibase-with-docker-desktop&#34;&gt;Testing Budibase with Docker Desktop&lt;/h2&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start our exploration by testing Budibase using &lt;a href=&#34;https://docs.budibase.com/docs/docker-compose&#34; target=&#34;_blank&#34;&gt;Docker compose&lt;/a&gt;. To begin, download both the &lt;em&gt;docker-compose.yaml&lt;/em&gt; and &lt;em&gt;.env&lt;/em&gt; files, then launch the platform with the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying Appwrite in HomeLab with K3s</title>
      <link>https://seehiong.github.io/2024/deploying-appwrite-in-homelab-with-k3s/</link>
      <pubDate>Fri, 16 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-appwrite-in-homelab-with-k3s/</guid>
      <description>&lt;p&gt;In this post, we&amp;rsquo;ll embark on installing &lt;a href=&#34;https://appwrite.io/&#34; target=&#34;_blank&#34;&gt;Appwrite&lt;/a&gt;, an open-source platform designed to facilitate the integration of authentication, databases, functions, and storage, enabling the development of scalable applications within our HomeLab setup.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prepartion&#34;&gt;Prepartion&lt;/h2&gt;&#xA;&lt;p&gt;Referencing my &lt;a href=&#34;https://seehiong.github.io/archives/2023/setting-up-k3s/&#34; target=&#34;_blank&#34;&gt;previous K3s setup post&lt;/a&gt;, let&amp;rsquo;s initiate the installation process by deploying K3s server, this time with Traefik disabled:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--disable traefik&amp;#34;&lt;/span&gt; K3S_KUBECONFIG_MODE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;644&amp;#34;&lt;/span&gt; sh -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Java Integration with Jupyter Notebooks</title>
      <link>https://seehiong.github.io/2024/java-integration-with-jupyter-notebooks/</link>
      <pubDate>Sun, 21 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/java-integration-with-jupyter-notebooks/</guid>
      <description>&lt;p&gt;In this post, I am delighted to share my journey of seamlessly integrating Java programming within Jupyter notebooks.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;&#xA;&lt;p&gt;Commencing with the selection of a pertinent Jupyter Docker Stack image, as detailed in the &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html&#34; target=&#34;_blank&#34;&gt;Jupyter Docker Stacks documentation&lt;/a&gt;, the following Docker command initializes the setup:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker pull quay.io/jupyter/minimal-notebook:notebook-7.0.6&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Subsequently, the Docker image is run on a Windows WSL environment, with the host IP set to &lt;em&gt;192.168.68.114&lt;/em&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Autogen Studio</title>
      <link>https://seehiong.github.io/2024/exploring-autogen-studio/</link>
      <pubDate>Sun, 14 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/exploring-autogen-studio/</guid>
      <description>&lt;p&gt;In this comprehensive exploration, we delve into the realm of &lt;a href=&#34;https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/&#34; target=&#34;_blank&#34;&gt;Autogen Studio&lt;/a&gt;, a powerful tool designed to streamline the rapid prototyping of multi-agent solutions for various tasks.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;The journey begins with the initial setup. A new Python virtual environment is created using Conda, followed by the installation of Autogen Studio and the essential configuration of API keys.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n autogenstudio python&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3.10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate autogenstudio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install autogenstudio&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export OPENAI_API_KEY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sk-xxxx&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;autogenstudio ui --port &lt;span style=&#34;color:#ae81ff&#34;&gt;8081&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>&lt;p&gt;In this post, we delve into the deployment of Lightweight Language Models (LLMs) using &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge&#34; target=&#34;_blank&#34;&gt;WasmEdge&lt;/a&gt;, a lightweight, high-performance, and extensible WebAssembly runtime. This setup is tailored to run LLMs in our previously configured HomeLab environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;To establish an OpenAI-compatible &lt;a href=&#34;https://github.com/LlamaEdge/LlamaEdge/tree/main/llama-api-server&#34; target=&#34;_blank&#34;&gt;API server&lt;/a&gt;, begin by downloading the API server application:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For Rust-based &lt;a href=&#34;https://wasmedge.org/docs/develop/rust/wasinn/llm_inference/&#34; target=&#34;_blank&#34;&gt;Llama 2 inference&lt;/a&gt;, we require the &lt;a href=&#34;https://wasmedge.org/docs/contribute/source/plugin/wasi_nn/&#34; target=&#34;_blank&#34;&gt;Wasi-NN&lt;/a&gt; plugin. The &lt;em&gt;Dockerfile&lt;/em&gt; below reflects this configuration:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integrating NFS for Improved Scalability</title>
      <link>https://seehiong.github.io/2024/integrating-nfs-for-improved-scalability/</link>
      <pubDate>Sun, 07 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integrating-nfs-for-improved-scalability/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;ve been following the &lt;a href=&#34;https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, you might have observed that deploying LLM may not be as scalable. In this post, we delve into the integration of NFS (Network File System) to externalize model environment variables. This approach eliminates the need to rebuild a new image each time a new LLM (Language Model) is introduced into your workflow.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-nfs&#34;&gt;Setting up NFS&lt;/h2&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start by setting up NFS to connect to my recently acquired TerraMaster NAS.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integration of Kong into AI Workflow</title>
      <link>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</link>
      <pubDate>Sat, 06 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integration-of-kong-into-ai-workflow/</guid>
      <description>&lt;p&gt;This post will guide you through the process of configuring Kong Gateway OSS and Kong Ingress Controller (KIC) separately and integrating Kong into our AI workflow.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;integrate-via-kong-gateway-oss-configuration&#34;&gt;Integrate via Kong Gateway OSS Configuration&lt;/h2&gt;&#xA;&lt;p&gt;If you followed my earlier guide on &lt;a href=&#34;https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/&#34; target=&#34;_blank&#34;&gt;setting up Kong Gateway&lt;/a&gt; setup, you likely use api.local:8000 to access the API.&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s revisit and update &lt;em&gt;KONG_ADMIN_GUI_URL&lt;/em&gt; environment  variable in the &lt;em&gt;kong-deploy-svc.yaml&lt;/em&gt; file:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Kong Ingress Controller (KIC)</title>
      <link>https://seehiong.github.io/2024/exploring-kong-ingress-controller-kic/</link>
      <pubDate>Mon, 01 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/exploring-kong-ingress-controller-kic/</guid>
      <description>&lt;p&gt;Wishing everyone a Happy New Year 2024! In this post, I shift focus from my previous discussion on &lt;a href=&#34;https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/&#34; target=&#34;_blank&#34;&gt;Kong Gateway&lt;/a&gt; to delve into the setup of the Kong Ingress Controller (KIC). Keeping it concise and celebratory for the New Year!&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt; serves as a Kubernetes package manager. To install it, execute the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo snap install helm --classic&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Streamlining API Management with Kong</title>
      <link>https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/</link>
      <pubDate>Sun, 31 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/</guid>
      <description>&lt;p&gt;In this comprehensive guide, we will walk through the process of integrating &lt;a href=&#34;https://konghq.com/&#34; target=&#34;_blank&#34;&gt;Kong&lt;/a&gt;, a robust unified API platform, into our home lab environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prerequistes&#34;&gt;Prerequistes&lt;/h2&gt;&#xA;&lt;p&gt;To begin, I will start with a fresh Ubuntu server instance. We&amp;rsquo;ll start by installing Docker and configuring it for non-root usage:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run Docker without sudo by logging back in or executing this&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;su - pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>AI Integration: LocalAI, Chroma, and Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/ai-integration-localai-chroma-langchain4j/</link>
      <pubDate>Fri, 29 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/ai-integration-localai-chroma-langchain4j/</guid>
      <description>&lt;p&gt;Referring to the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;Building an AI application with Langchaing4j&lt;/a&gt; guide, the deployment of necessary Docker images, LocalAI, and Chroma to our Home Lab is outlined.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;creating-custom-localai-image&#34;&gt;Creating custom LocalAI image&lt;/h2&gt;&#xA;&lt;p&gt;Begin with pulling the latest image using the &lt;a href=&#34;https://localai.io/howtos/easy-setup-docker/&#34; target=&#34;_blank&#34;&gt;easy docker setup&lt;/a&gt; guide:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker pull quay.io/go-skynet/local-ai:v2.2.0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, run LocalAI from the &lt;em&gt;~/localai&lt;/em&gt; folder and download a model:&lt;/p&gt;</description>
    </item>
    <item>
      <title>GitLab Setup: Installation, Migration, and CI/CD Simplified</title>
      <link>https://seehiong.github.io/archives/2023/gitlab-setup-installation-migration-and-ci-cd-simplified/</link>
      <pubDate>Sun, 24 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/gitlab-setup-installation-migration-and-ci-cd-simplified/</guid>
      <description>&lt;p&gt;In this guide, I&amp;rsquo;ll walk you through the process of installing &lt;a href=&#34;https://docs.gitlab.com/omnibus/installation/&#34; target=&#34;_blank&#34;&gt;GitLab&lt;/a&gt;, a comprehensive suite of tools for version control, continuous integration, continuous delivery, and more, in my Home Lab collection.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After obtaining the latest &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34;&gt;Ubuntu Server&lt;/a&gt;, I utilized &lt;a href=&#34;https://rufus.ie/en/&#34; target=&#34;_blank&#34;&gt;Rufus&lt;/a&gt;, a utility for formatting and creating bootable USB flash drives.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Following the &lt;a href=&#34;https://packages.gitlab.com/gitlab/gitlab-ce/install&#34; target=&#34;_blank&#34;&gt;installation instructions&lt;/a&gt;, initiate a quick installation using the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying OpenAI-Compatible LLAMA CPP Server with K3S</title>
      <link>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</link>
      <pubDate>Fri, 22 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</guid>
      <description>&lt;p&gt;Commencing my week-long Christmas break, I extend the concepts from my &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; to establish an OpenAI-compatible server in my &lt;a href=&#34;https://seehiong.github.io/archives/2023/setting-up-k3s/&#34; target=&#34;_blank&#34;&gt;Home Lab&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;technical-setup&#34;&gt;Technical Setup&lt;/h2&gt;&#xA;&lt;p&gt;After fine-tuning a sample &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python/blob/main/docker/openblas_simple/Dockerfile&#34; target=&#34;_blank&#34;&gt;Dockerfile&lt;/a&gt;, I reinstalled my Ubuntu server, incorporating necessary adjustments. The subsequent setup commands, reflecting my Home Lab&amp;rsquo;s new IP address (192.168.68.115), include:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt update &amp;amp; sudo apt upgrade -y&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install docker&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install Anaconda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chmod +x Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;./Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Init conda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source /home/pi/anaconda3/bin/activate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda init&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n docker-llama python&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate docker-llama &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Setting up K3s</title>
      <link>https://seehiong.github.io/archives/2023/setting-up-k3s/</link>
      <pubDate>Sun, 30 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/setting-up-k3s/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.k3s.io/&#34; target=&#34;_blank&#34;&gt;K3S&lt;/a&gt; is a lightweight and easy-to-install Kubernetes distribution, making it an ideal choice for running a Kubernetes cluster in your home lab. In this blog post, we will walk you through the step-by-step process of setting up K3s on an Ubuntu Server 22.04.2 LTS.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-setting-up-k3s&#34;&gt;1 Setting up K3S&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-installing-ubuntu-server-22042-lts&#34;&gt;1.1 Installing Ubuntu Server 22.04.2 LTS&lt;/h3&gt;&#xA;&lt;p&gt;To start, we&amp;rsquo;ll install &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34;&gt;Ubuntu server 22.04.2 LTS&lt;/a&gt; on our laptop. You can verify the Linux distribution using the following command:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
