<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HomeLab on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/homelab/</link>
    <description>Recent content in HomeLab on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 20 Oct 2024 08:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/homelab/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Integrating MLflow and Kubeflow on Talos</title>
      <link>https://seehiong.github.io/archives/2024/integrating-mlflow-and-kubeflow-on-talos/</link>
      <pubDate>Sun, 20 Oct 2024 08:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/integrating-mlflow-and-kubeflow-on-talos/</guid>
      <description>This post details the installation of MLflow and Kubeflow on a Talos HomeLab cluster. It covers the setup process, including Talos configuration, local-path and NFS provisioning, and Metallb installation. Step-by-step instructions are provided for deploying Kubeflow, followed by the installation of MLflow for managing the machine learning lifecycle. Finally, the post illustrates how to log experiments and models in MLflow and perform inference, demonstrating a seamless integration of these tools for enhanced machine learning operations.</description>
    </item>
    <item>
      <title>Talos Linux: Setting Up a Secure, Immutable Kubernetes Cluster</title>
      <link>https://seehiong.github.io/archives/2024/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</link>
      <pubDate>Sun, 01 Sep 2024 08:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/talos-linux-setting-up-a-secure-immutable-kubernetes-cluster/</guid>
      <description>This post guides you through setting up a secure, immutable Kubernetes cluster using Talos Linux. It covers installing Talos on control and worker nodes, configuring local storage with hostPath and Local Path Provisioner, and setting up the Kubernetes Dashboard with an admin user for cluster management. With Talos Linux, you achieve a minimal, API-managed Kubernetes environment without SSH or systemd, making it ideal for a secure and reliable homelab or production setup.</description>
    </item>
    <item>
      <title>Setting Up Kafka with MicroK8s and Multipass</title>
      <link>https://seehiong.github.io/archives/2024/setting-up-kafka-with-microk8s-and-multipass/</link>
      <pubDate>Sat, 03 Aug 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/setting-up-kafka-with-microk8s-and-multipass/</guid>
      <description>My homelab is a playground for experimenting with various tools and setups. However, for Proof of Concept (POC) environments, a lightweight and portable setup is often more suitable. In this post, I will guide you through setting up a MicroK8s environment in a virtual machine using Multipass. This POC demonstrates how Kafka can be set up in this environment.</description>
    </item>
    <item>
      <title>Building Your First Kubeflow Pipeline: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/archives/2024/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</link>
      <pubDate>Sat, 20 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/building-your-first-kubeflow-pipeline-a-step-by-step-guide/</guid>
      <description>In this blog post, I guide you through creating and running your first Kubeflow pipeline. We&amp;rsquo;ll start with the &amp;ldquo;Hello World&amp;rdquo; example, demonstrate how to manage sequential and shared pipelines, and explore artifact storage with MinIO. Additionally, I&amp;rsquo;ll introduce K9s, a powerful terminal-based UI for managing your Kubernetes clusters efficiently. By the end, you&amp;rsquo;ll have a solid understanding of setting up and managing Kubeflow pipelines in your machine learning workflows.</description>
    </item>
    <item>
      <title>Integrating Draw.io and PlantUML with GitLab</title>
      <link>https://seehiong.github.io/archives/2024/integrating-draw-io-and-plantuml-with-gitlab/</link>
      <pubDate>Sat, 06 Jul 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/integrating-draw-io-and-plantuml-with-gitlab/</guid>
      <description>As we transition from Lucidchart to draw.io for team diagramming, this guide outlines the steps to integrate draw.io and PlantUML with GitLab. I&amp;rsquo;ll configure the Diagrams.net server, enable integration, and demonstrate creating and editing diagrams within GitLab. Additionally, I&amp;rsquo;ll cover the setup and integration of PlantUML for creating detailed design diagrams. Follow along to seamlessly incorporate these powerful diagramming tools into your GitLab workflow.</description>
    </item>
    <item>
      <title>Setting Up and Using KServe with Kubeflow</title>
      <link>https://seehiong.github.io/archives/2024/setting-up-and-using-kserve-with-kubeflow/</link>
      <pubDate>Sun, 30 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/setting-up-and-using-kserve-with-kubeflow/</guid>
      <description>In this post, we explore KServe, a model inference platform on Kubernetes designed for scalability. Building on our previous Kubeflow guide, we detail how to set up your first KServe endpoint, make predictions, and troubleshoot common issues. Follow our step-by-step instructions to seamlessly integrate KServe with your Kubeflow environment and enhance your machine learning deployment process.</description>
    </item>
    <item>
      <title>Setting Up Kubeflow on Kubernetes: A Step-by-Step Guide</title>
      <link>https://seehiong.github.io/archives/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</link>
      <pubDate>Mon, 24 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/setting-up-kubeflow-on-kubernetes-a-step-by-step-guide/</guid>
      <description>In this post, I provide a comprehensive guide to setting up Kubeflow, a machine learning toolkit for Kubernetes. From initial preparation and downloading necessary binaries to installing all Kubeflow components and troubleshooting common issues, this step-by-step tutorial ensures a smooth installation process. You&amp;rsquo;ll also learn how to create your first notebook and resolve potential errors, making it easier to leverage Kubeflow&amp;rsquo;s powerful features for your machine learning projects.</description>
    </item>
    <item>
      <title>Planning Gift Deliveries With QGIS</title>
      <link>https://seehiong.github.io/archives/2024/planning-gift-deliveries-with-qgis/</link>
      <pubDate>Sat, 08 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/planning-gift-deliveries-with-qgis/</guid>
      <description>In this post, I document my journey of using QGIS, a free and open-source geographic information system, to plan gift deliveries. I outline the steps to install QGIS, add essential plugins, create a shapefile layer for mapping locations, and use ORS Tools for route planning. After configuring the map, I determined that my delivery route would take 3 hours. This process, while tailored to my use case, is versatile and applicable to various fields such as logistics, urban planning, and environmental management. QGIS provides robust tools for efficient spatial analysis and mapping tasks.</description>
    </item>
    <item>
      <title>Log Management with Graylog</title>
      <link>https://seehiong.github.io/archives/2024/log-management-with-graylog/</link>
      <pubDate>Fri, 19 Apr 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/log-management-with-graylog/</guid>
      <description>Explore setting up Graylog in your HomeLab for comprehensive log management. Configure MongoDB and OpenSearch, deploy Fluent Bit for log forwarding, and implement advanced features like Grok patterns and pipelines. Troubleshoot efficiently with tools like netshoot and tcpdump. Enhance your HomeLab environment with a scalable and efficient log management solution.</description>
    </item>
    <item>
      <title>Configuring Appwrite Functions with K3s</title>
      <link>https://seehiong.github.io/archives/2024/configuring-appwrite-functions-with-k3s/</link>
      <pubDate>Sun, 10 Mar 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/configuring-appwrite-functions-with-k3s/</guid>
      <description>This guide outlines configuring Appwrite Functions within a K3s environment. It covers essential steps, including installing ngrok for external network access, registering a GitHub App, and setting up Appwrite with required configurations. The process involves updating YAML files for deployment, ensuring proper routing with Traefik, and creating functions through the Appwrite interface. Once set up, the functions are deployed successfully, enabling seamless integration and execution within the K3s infrastructure.</description>
    </item>
    <item>
      <title>Deploying Budibase in HomeLab</title>
      <link>https://seehiong.github.io/archives/2024/deploying-budibase-in-homelab/</link>
      <pubDate>Sun, 25 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/deploying-budibase-in-homelab/</guid>
      <description>This post outlines the process of installing Budibase in a HomeLab environment, starting with testing it on Docker Desktop and then deploying it using Helm in Kubernetes. It guides through setting up an admin user, building the first app by creating a database, designing an application form, and configuring submission actions. The summary encapsulates the steps involved in testing, deploying, and building an application with Budibase, highlighting key actions such as Docker Compose setup, Helm installation, app creation, and deployment in a concise manner.</description>
    </item>
    <item>
      <title>Deploying Appwrite in HomeLab with K3s</title>
      <link>https://seehiong.github.io/archives/2024/deploying-appwrite-in-homelab-with-k3s/</link>
      <pubDate>Fri, 16 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/deploying-appwrite-in-homelab-with-k3s/</guid>
      <description>Learn how to seamlessly integrate Appwrite, an open-source platform, into your HomeLab setup using K3s. Follow step-by-step instructions to deploy K3s with Portainer, prepare Appwrite volumes, and configure miscellaneous services like MariaDB and InfluxDB. Utilize Kompose to convert Docker Compose files to Kubernetes for efficient deployment. Ensure smooth installation by mapping necessary environment variables and applying all required deployments and services. Finally, witness the successful deployment of Appwrite services and access the login page to start building scalable applications. Master the art of HomeLab application deployment with Appwrite and K3s.</description>
    </item>
    <item>
      <title>Java Integration with Jupyter Notebooks</title>
      <link>https://seehiong.github.io/archives/2024/java-integration-with-jupyter-notebooks/</link>
      <pubDate>Sun, 21 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/java-integration-with-jupyter-notebooks/</guid>
      <description>Embark on a seamless integration of Java into Jupyter notebooks with this comprehensive guide. Beginning with the selection of a relevant Jupyter Docker Stack, the post details setup steps and deployment in HomeLab, showcasing application results for verification. The integration of Java Kernel through JBang and testing with &amp;ldquo;Hello World&amp;rdquo; and Apache Commons library exemplifies the versatility. Further exploration involves experimenting with Java in a Python kernel using JBang. Concluding with a call to joyful coding, this journey promises a harmonious blend of Java&amp;rsquo;s robustness and Jupyter&amp;rsquo;s interactive nature. Discover the joy of coding in this enriched Java-in-Jupyter experience. Happy coding!</description>
    </item>
    <item>
      <title>Exploring Autogen Studio</title>
      <link>https://seehiong.github.io/archives/2024/exploring-autogen-studio/</link>
      <pubDate>Sun, 14 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/exploring-autogen-studio/</guid>
      <description>In this exploration of Autogen Studio, we navigated through the AI landscape, harnessing the LM Studio API to compare responses from diverse language models. Employing the Mistral Instruct 7B model, we scrutinized prompts like Stock Price and Paint, visualizing outcomes and delving into key configurations. The post also offered insights into the primary assistant, model configuration, and agent workflows, accompanied by a comparative analysis of Mistral model responses. This comprehensive journey demystifies the power of Autogen Studio and its seamless integration with LM Studio API, providing practical guidance for AI enthusiasts.</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/archives/2024/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>In this post, we explored deploying Lightweight Language Models (LLMs) using WasmEdge, a high-performance WebAssembly runtime, within a HomeLab environment. The process involved preparing an OpenAI-compatible API server, configuring the Wasi-NN plugin, and deploying the setup to HomeLab using Kubernetes (K3s). The post also detailed the steps for testing the API server and integrating it into a Java application. Overall, the guide provides a comprehensive walkthrough of hosting and utilizing LLMs with WasmEdge in a local environment.</description>
    </item>
    <item>
      <title>Integrating NFS for Improved Scalability</title>
      <link>https://seehiong.github.io/archives/2024/integrating-nfs-for-improved-scalability/</link>
      <pubDate>Sun, 07 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/integrating-nfs-for-improved-scalability/</guid>
      <description>In this post, we explored the integration of NFS to enhance scalability in deploying LLM models within a home lab. Setting up NFS involved connecting to a TerraMaster NAS, and the K3s cluster was configured to dynamically provision storage. With NFS in place, the deployment process became more efficient, eliminating the need to rebuild images for each new model introduction. The post detailed the setup steps, from configuring NFS and K3s to deploying LLM models dynamically. This approach simplifies the scaling of Language Models, providing a centralized and scalable storage solution through NFS in a Kubernetes environment.</description>
    </item>
    <item>
      <title>Integration of Kong into AI Workflow</title>
      <link>https://seehiong.github.io/archives/2024/integration-of-kong-into-ai-workflow/</link>
      <pubDate>Sat, 06 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/integration-of-kong-into-ai-workflow/</guid>
      <description>This comprehensive guide navigates through configuring Kong OSS and Kong Ingress Controller (KIC), seamlessly integrating Kong into an AI workflow. Starting with Kong OSS configuration, the tutorial covers updating environment variables and service ports. The Langchain4j application is then adapted to leverage Kong API, allowing for flexible path-based APIs. Additionally, potential timeout issues are addressed. The guide concludes with a demonstration of Kong Ingress Controller configuration, emphasizing optimal settings for specific use cases. Whether through Kong OSS or KIC, readers gain insights into enhancing API management and integration within their AI workflows.</description>
    </item>
    <item>
      <title>Exploring Kong Ingress Controller (KIC)</title>
      <link>https://seehiong.github.io/archives/2024/exploring-kong-ingress-controller-kic/</link>
      <pubDate>Mon, 01 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/exploring-kong-ingress-controller-kic/</guid>
      <description>Embark on a journey into the new year by exploring Kong Ingress Controller (KIC) in your home lab. This guide, transitioning from a previous discussion on Kong Gateway, details the seamless setup of KIC using Helm and K3s. From initial preparations to installing Kong Ingress Controller and Gateway, witness the efficient management of APIs in your home lab environment. Learn to add Kong Ingresses, ensuring optimal routing for various paths. Through concise steps and illustrative visuals, this post simplifies the process, allowing you to experience KIC&amp;rsquo;s capabilities firsthand. Dive into the year with a hands-on exploration of API management with Kong in your home lab. Happy New Year!</description>
    </item>
    <item>
      <title>Streamlining API Management with Kong</title>
      <link>https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/</link>
      <pubDate>Sun, 31 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/streamlining-api-management-with-kong/</guid>
      <description>&lt;p&gt;In this comprehensive guide, we will walk through the process of integrating &lt;a href=&#34;https://konghq.com/&#34; target=&#34;_blank&#34;&gt;Kong&lt;/a&gt;, a robust unified API platform, into our home lab environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;prerequistes&#34;&gt;Prerequistes&lt;/h2&gt;&#xA;&lt;p&gt;To begin, I will start with a fresh Ubuntu server instance. We&amp;rsquo;ll start by installing Docker and configuring it for non-root usage:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run Docker without sudo by logging back in or executing this&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;su - pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>AI Integration: LocalAI, Chroma, and Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/ai-integration-localai-chroma-langchain4j/</link>
      <pubDate>Fri, 29 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/ai-integration-localai-chroma-langchain4j/</guid>
      <description>&lt;p&gt;Referring to the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;Building an AI application with Langchaing4j&lt;/a&gt; guide, the deployment of necessary Docker images, LocalAI, and Chroma to our Home Lab is outlined.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;creating-custom-localai-image&#34;&gt;Creating custom LocalAI image&lt;/h2&gt;&#xA;&lt;p&gt;Begin with pulling the latest image using the &lt;a href=&#34;https://localai.io/howtos/easy-setup-docker/&#34; target=&#34;_blank&#34;&gt;easy docker setup&lt;/a&gt; guide:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker pull quay.io/go-skynet/local-ai:v2.2.0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, run LocalAI from the &lt;em&gt;~/localai&lt;/em&gt; folder and download a model:&lt;/p&gt;</description>
    </item>
    <item>
      <title>GitLab Setup: Installation, Migration, and CI/CD Simplified</title>
      <link>https://seehiong.github.io/archives/2023/gitlab-setup-installation-migration-and-ci-cd-simplified/</link>
      <pubDate>Sun, 24 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/gitlab-setup-installation-migration-and-ci-cd-simplified/</guid>
      <description>&lt;p&gt;In this guide, I&amp;rsquo;ll walk you through the process of installing &lt;a href=&#34;https://docs.gitlab.com/omnibus/installation/&#34; target=&#34;_blank&#34;&gt;GitLab&lt;/a&gt;, a comprehensive suite of tools for version control, continuous integration, continuous delivery, and more, in my Home Lab collection.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;After obtaining the latest &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34;&gt;Ubuntu Server&lt;/a&gt;, I utilized &lt;a href=&#34;https://rufus.ie/en/&#34; target=&#34;_blank&#34;&gt;Rufus&lt;/a&gt;, a utility for formatting and creating bootable USB flash drives.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Following the &lt;a href=&#34;https://packages.gitlab.com/gitlab/gitlab-ce/install&#34; target=&#34;_blank&#34;&gt;installation instructions&lt;/a&gt;, initiate a quick installation using the following command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying OpenAI-Compatible LLAMA CPP Server with K3S</title>
      <link>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</link>
      <pubDate>Fri, 22 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</guid>
      <description>&lt;p&gt;Commencing my week-long Christmas break, I extend the concepts from my &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; to establish an OpenAI-compatible server in my &lt;a href=&#34;https://seehiong.github.io/archives/2023/setting-up-k3s/&#34; target=&#34;_blank&#34;&gt;Home Lab&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;technical-setup&#34;&gt;Technical Setup&lt;/h2&gt;&#xA;&lt;p&gt;After fine-tuning a sample &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python/blob/main/docker/openblas_simple/Dockerfile&#34; target=&#34;_blank&#34;&gt;Dockerfile&lt;/a&gt;, I reinstalled my Ubuntu server, incorporating necessary adjustments. The subsequent setup commands, reflecting my Home Lab&amp;rsquo;s new IP address (192.168.68.115), include:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt update &amp;amp; sudo apt upgrade -y&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install docker&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install Anaconda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chmod +x Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;./Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Init conda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source /home/pi/anaconda3/bin/activate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda init&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n docker-llama python&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate docker-llama &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Setting up K3s</title>
      <link>https://seehiong.github.io/archives/2023/setting-up-k3s/</link>
      <pubDate>Sun, 30 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/setting-up-k3s/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.k3s.io/&#34; target=&#34;_blank&#34;&gt;K3S&lt;/a&gt; is a lightweight and easy-to-install Kubernetes distribution, making it an ideal choice for running a Kubernetes cluster in your home lab. In this blog post, we will walk you through the step-by-step process of setting up K3s on an Ubuntu Server 22.04.2 LTS.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-setting-up-k3s&#34;&gt;1 Setting up K3S&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-installing-ubuntu-server-22042-lts&#34;&gt;1.1 Installing Ubuntu Server 22.04.2 LTS&lt;/h3&gt;&#xA;&lt;p&gt;To start, we&amp;rsquo;ll install &lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34;&gt;Ubuntu server 22.04.2 LTS&lt;/a&gt; on our laptop. You can verify the Linux distribution using the following command:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
