<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Development on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/ai-development/</link>
    <description>Recent content in AI Development on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 19 Aug 2025 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/ai-development/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Run gpt-oss Locally on Ryzen AI</title>
      <link>https://seehiong.github.io/posts/2025/08/run-gpt-oss-locally-on-ryzen-ai/</link>
      <pubDate>Tue, 19 Aug 2025 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2025/08/run-gpt-oss-locally-on-ryzen-ai/</guid>
      <description>Learn how to run OpenAI’s new gpt-oss model locally on a Ryzen AI Max+ 395 mini PC. This guide covers setting up Ollama, Cursor with Kilo Code, and OpenRouter to build a multi-model chat app that queries GPT-3.5, Claude, Mistral, LLaMA, Gemini, and more in parallel. Responses are consolidated into one clean interface—no more juggling multiple tabs. While local gpt-oss runs slower than cloud-hosted models, this experiment shows how anyone can explore AI development locally with modern tools.</description>
    </item>
  </channel>
</rss>
