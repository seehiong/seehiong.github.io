<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stable Diffusion on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/stable-diffusion/</link>
    <description>Recent content in Stable Diffusion on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Feb 2024 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/stable-diffusion/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Text-to-Image with StableDiffusionPipeline</title>
      <link>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</link>
      <pubDate>Sat, 10 Feb 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/text-to-image-with-stablediffusionpipeline/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll delve into the capabilities of the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline&#34; target=&#34;_blank&#34;&gt;StableDiffusionPipeline&lt;/a&gt; for generating photorealistic images based on textual inputs.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;text-to-image&#34;&gt;Text-to-Image&lt;/h2&gt;&#xA;&lt;p&gt;Continuing from the &lt;a href=&#34;https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I initiated the environment setup:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd stable-diffusion&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate ldm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Subsequently, I installed the necessary libraries, &lt;a href=&#34;https://pypi.org/project/diffusers/&#34; target=&#34;_blank&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://pypi.org/project/transformers/&#34; target=&#34;_blank&#34;&gt;transformers&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install --upgrade diffusers&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; transformers&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Stable Diffusion: Text-to-Image Modeling Journey</title>
      <link>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</link>
      <pubDate>Sat, 03 Feb 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/stable-diffusion-text-to-image-modeling-journey/</guid>
      <description>&lt;p&gt;In this article, we will delve into &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34; target=&#34;_blank&#34;&gt;Stable Diffusion&lt;/a&gt;, a latent text-to-image diffusion model. In simple terms, diffusion models in machine learning represent a type of sophisticated computer program designed to learn how patterns evolve over time. Comprising three essential components – a forward process, a reverse process, and a sampling procedure – these models aim to comprehend and generate intricate patterns within a given dataset.&lt;/p&gt;&#xA;&lt;p&gt;Consider having a blurry image that needs enhancement. Diffusion models act as intelligent tools that learn to eliminate blurriness by grasping how images blur and then effectively reversing that process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenVINO, Optimum-Intel, CPU: An Exploration in Model Optimization</title>
      <link>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</link>
      <pubDate>Sat, 27 Jan 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/openvino-optimum-intel-cpu-an-exploration-in-model-optimization/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.openvino.ai/2023.3/home.html&#34; target=&#34;_blank&#34;&gt;OpenVINO&lt;/a&gt; represents an open-source toolkit designed for the optimization and deployment of deep learning models. Acting as the interface between the Transformers and Diffusers libraries, &lt;a href=&#34;https://huggingface.co/docs/optimum/intel/inference&#34; target=&#34;_blank&#34;&gt;Optimum-Intel&lt;/a&gt; seamlessly integrates with various Intel tools and libraries, facilitating the acceleration of end-to-end pipelines on Intel architectures. This post documents my journey as I set up and execute example code on my aging laptop, exploring the application of Quantization-aware Training (QAT) and the Token Merging method to optimize the UNet model within the Stable Diffusion pipeline.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
