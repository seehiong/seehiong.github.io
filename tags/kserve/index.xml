<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>KServe on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/kserve/</link>
    <description>Recent content in KServe on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 May 2025 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/kserve/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploying KServe on OKE</title>
      <link>https://seehiong.github.io/2025/deploy-kserve-on-oke/</link>
      <pubDate>Mon, 12 May 2025 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2025/deploy-kserve-on-oke/</guid>
      <description>This post demonstrated how to deploy an XGBoost model using KServe on Oracle Kubernetes Engine (OKE). Starting from model upload to Object Storage, we served the model via KServe and exposed it through Istio Gateway. Instead of deploying the frontend in-cluster, we built a Streamlit app hosted on Streamlit Community Cloud, which sends requests to the public inference endpoint. This end-to-end setup showcases a scalable and cloud-native ML deployment pipeline on OCI, separating model serving and user interface layers for flexibility and ease of maintenance.</description>
    </item>
    <item>
      <title>From Model to HDB App</title>
      <link>https://seehiong.github.io/2025/from-model-to-hdb-app/</link>
      <pubDate>Mon, 21 Apr 2025 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2025/from-model-to-hdb-app/</guid>
      <description>In this post, I demonstrated how to deploy a Jupyter notebook using the jupyter-tensorflow-full image in Kubeflow, develop an HDB resale price predictor app with Streamlit, and access it via port forwarding. The setup runs locally and showcases how to bring together machine learning, visualization, and interactivity in a seamless development workflow within Kubeflow.</description>
    </item>
    <item>
      <title>Setting Up and Using KServe with Kubeflow</title>
      <link>https://seehiong.github.io/archives/2024/setting-up-and-using-kserve-with-kubeflow/</link>
      <pubDate>Sun, 30 Jun 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2024/setting-up-and-using-kserve-with-kubeflow/</guid>
      <description>In this post, we explore KServe, a model inference platform on Kubernetes designed for scalability. Building on our previous Kubeflow guide, we detail how to set up your first KServe endpoint, make predictions, and troubleshoot common issues. Follow our step-by-step instructions to seamlessly integrate KServe with your Kubeflow environment and enhance your machine learning deployment process.</description>
    </item>
  </channel>
</rss>
