<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/llm/</link>
    <description>Recent content in LLM on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 09 Feb 2025 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chat-Driven Insights with Chart.js</title>
      <link>https://seehiong.github.io/posts/2025/02/chat-driven-insights-with-chart.js/</link>
      <pubDate>Sun, 09 Feb 2025 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2025/02/chat-driven-insights-with-chart.js/</guid>
      <description>Enhance your Vue.js application by integrating chat capabilities with Chart.js and LLMs like OpenAI and Deepseek-R1. This post walks through adding a chat node to the Micronaut-Optimizer workflow, enabling dynamic interactions with optimization results. Learn how to configure environment variables, connect workflow nodes, and send Chart.js data to LLMs. See it in action with sample inputs and responses, and explore running Deepseek-R1 locally with Ollama.</description>
    </item>
    <item>
      <title>Porting Llama3.java to Micronaut</title>
      <link>https://seehiong.github.io/posts/2024/12/porting-llama3.java-to-micronaut/</link>
      <pubDate>Sun, 15 Dec 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/12/porting-llama3.java-to-micronaut/</guid>
      <description>This project ports the original single-file llama3.java by Alfonso¬≤ Peterssen into a modular Micronaut application, transforming it from a console app to a stream-based, production-ready API. The internals and performance of the original GGUF-format LLM remain unchanged. Updates focus on restructuring the codebase into logical packages and adapting it to Micronaut‚Äôs ecosystem, enabling easier integration with Java microservices. This streamlined design retains the original‚Äôs simplicity while enhancing scalability and usability for modern AI applications.</description>
    </item>
    <item>
      <title>GPT-2 Training Guide</title>
      <link>https://seehiong.github.io/posts/2024/10/gpt-2-training-guide/</link>
      <pubDate>Thu, 31 Oct 2024 10:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/10/gpt-2-training-guide/</guid>
      <description>This post documents my journey training GPT-2 on the Tiny Shakespeare dataset, inspired by Andrej Karpathy&amp;rsquo;s instructional video and nanoGPT repository. Following each step, I detail the setup process, from encoding data and optimizing the model with AdamW, to improving training stability with mixed precision and flash attention. The post includes practical insights on using pretrained weights, weight sharing, and efficient data handling, concluding with sample outputs from training and sampling a ‚Äúbaby‚Äù GPT model.</description>
    </item>
    <item>
      <title>GPT-2 Setup and Pretraining Guide</title>
      <link>https://seehiong.github.io/posts/2024/10/gpt-2-setup-and-pretraining-guide/</link>
      <pubDate>Mon, 28 Oct 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/10/gpt-2-setup-and-pretraining-guide/</guid>
      <description>This guide explores reproducing GPT-2 (124M) using Andrej Karpathy‚Äôs video walkthrough. It begins with an overview of the GPT-2 architecture, a decoder-only transformer model inspired by &amp;ldquo;Attention Is All You Need.&amp;rdquo; Using pretrained GPT-2 weights, we analyze and initialize a custom GPT class, with detailed steps to handle token embeddings, causal attention, and layer normalization. The guide includes code for generating text from pretrained weights. In the next segment, we‚Äôll continue with a deeper dive into dataset preparation and training from scratch, moving from small samples to large-scale training.</description>
    </item>
    <item>
      <title>Audio Generation with NVIDIA Jetson Orin NX</title>
      <link>https://seehiong.github.io/posts/2024/08/audio-generation-with-nvidia-jetson-orin-nx/</link>
      <pubDate>Sun, 25 Aug 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/08/audio-generation-with-nvidia-jetson-orin-nx/</guid>
      <description>This post explores the audio generation capabilities of the NVIDIA Jetson Orin NX. It covers transcribing audio using Whisper, setting up text-to-speech (TTS) and automatic speech recognition (ASR) with Llamaspeak, and preparing the RIVA server for advanced speech AI applications. Detailed instructions and command examples are provided, making it easy for developers to experiment with these tools on the Jetson platform.</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/posts/2024/01/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>In this post, we explored deploying Lightweight Language Models (LLMs) using WasmEdge, a high-performance WebAssembly runtime, within a HomeLab environment. The process involved preparing an OpenAI-compatible API server, configuring the Wasi-NN plugin, and deploying the setup to HomeLab using Kubernetes (K3s). The post also detailed the steps for testing the API server and integrating it into a Java application. Overall, the guide provides a comprehensive walkthrough of hosting and utilizing LLMs with WasmEdge in a local environment.</description>
    </item>
    <item>
      <title>Integrating NFS for Improved Scalability</title>
      <link>https://seehiong.github.io/posts/2024/01/integrating-nfs-for-improved-scalability/</link>
      <pubDate>Sun, 07 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2024/01/integrating-nfs-for-improved-scalability/</guid>
      <description>In this post, we explored the integration of NFS to enhance scalability in deploying LLM models within a home lab. Setting up NFS involved connecting to a TerraMaster NAS, and the K3s cluster was configured to dynamically provision storage. With NFS in place, the deployment process became more efficient, eliminating the need to rebuild images for each new model introduction. The post detailed the setup steps, from configuring NFS and K3s to deploying LLM models dynamically. This approach simplifies the scaling of Language Models, providing a centralized and scalable storage solution through NFS in a Kubernetes environment.</description>
    </item>
    <item>
      <title>Unveiling Agent AutoBuild in Autogen</title>
      <link>https://seehiong.github.io/posts/2023/12/unveiling-agent-autobuild-in-autogen/</link>
      <pubDate>Sun, 17 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/unveiling-agent-autobuild-in-autogen/</guid>
      <description>In this blog, I explored Autogen&amp;rsquo;s Agent AutoBuild and experimented with the Mixtral 8x7B model. I configured Autogen, envisioning a software academy project for coding novices. Through code snippets, I showcased AutoBuild&amp;rsquo;s multi-agent system creation and tailored a task that wrote a General Paper article on art and courage. The Mixtral 8x7B model in LM Studio brought excitement but posed challenges with duplicate content. Check out the blog for a firsthand look at the dynamic interplay between Autogen and cutting-edge AI, complete with code snippets and images.</description>
    </item>
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>In this post, I explored enhancing Autogen&amp;rsquo;s capabilities by enabling seamless Java code execution. Drawing inspiration from 0xlws&amp;rsquo; fork supporting JavaScript, I embarked on modifying Autogen to robustly support Java. I detailed the setup process, including installing Java on Windows Subsystem for Linux (WSL) and modifying key files. The post includes code snippets showcasing the changes, recompilation steps, and instructions for generating Java code. I extended functionality to additional test cases, seamlessly switching between Java and Python code execution. Docker integration for Java code execution was also optimized, showcasing Autogen&amp;rsquo;s versatility and robust development experience.</description>
    </item>
    <item>
      <title>Multi-agent Conservation with Autogen</title>
      <link>https://seehiong.github.io/posts/2023/12/multi-agent-conservation-with-autogen/</link>
      <pubDate>Fri, 08 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/12/multi-agent-conservation-with-autogen/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://seehiong.github.io/posts/2023/12/exploring-autogen-with-lm-studio-and-local-llm/&#34; &gt;Exploration with Autogen&lt;/a&gt;&#xD;&#xA; and following the example of &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; &gt;Automated Multi Agent Chat&lt;/a&gt;&#xD;&#xA;, we&amp;rsquo;ll delve into the steps to create a dynamic debate environment.&lt;/p&gt;&#xA;&lt;h2 id=&#34;agent-setup&#34;&gt;Agent Setup&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll be setting up two agents: &lt;strong&gt;for_motion&lt;/strong&gt; and &lt;strong&gt;against_motion&lt;/strong&gt;. Each agent will engage in a debate on a given topic, providing examples and substantiating their points. A facilitator will oversee the debate rounds, ensuring that each response exceeds 300 words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG over Java code with Langchain4j</title>
      <link>https://seehiong.github.io/posts/2023/11/rag-over-java-code-with-langchain4j/</link>
      <pubDate>Sat, 11 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/11/rag-over-java-code-with-langchain4j/</guid>
      <description>In my latest post, I delve into seamlessly integrating Retrieval-Augmented Generation (RAG) with Java code using Langchain4j. Drawing inspiration from RAG over code, I explore Java Parser&amp;rsquo;s potential for robust codebase analysis. The pivotal JavaParsingService and EmbeddingStoreService orchestrate this integration, enabling users to effortlessly load Java projects and glean profound insights. The enhanced controller boasts user-friendly endpoints, fostering dynamic interactions. Witness Retrieval-Augmented Generation breathe life into Java code, from codebase ingestion to insightful querying with models like gpt4all-j, WizardLM, and OpenAI. This narrative unveils the nuanced capabilities of RAG in querying Java codebases.</description>
    </item>
    <item>
      <title>Building an AI Application with Langchain4j</title>
      <link>https://seehiong.github.io/posts/2023/11/building-an-ai-application-with-langchain4j/</link>
      <pubDate>Tue, 07 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/11/building-an-ai-application-with-langchain4j/</guid>
      <description>I embarked on a journey to harness the capabilities of Langchain4j, crafting a powerful AI application in Java using the local language model. Utilizing Spring Boot, Postman, and various Langchain4j components, I explored setting up, implementing a chat service, integrating custom tools, embedding functionality with Chroma, translation, persistence, retrieval, and streaming services. The blog post serves as a comprehensive guide for building personalized AI applications, showcasing the versatility and potential of Langchain4j in Java development.</description>
    </item>
    <item>
      <title>Unlocking the Power of Machine Learning with MLC LLM</title>
      <link>https://seehiong.github.io/posts/2023/09/unlocking-the-power-of-machine-learning-with-mlc-llm/</link>
      <pubDate>Sat, 02 Sep 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/09/unlocking-the-power-of-machine-learning-with-mlc-llm/</guid>
      <description>I delve into the transformative realm of MLC LLM, an advanced universal deployment solution for extensive language models. My post guides you personally through the setup, emphasizing critical components like TVM and Conda. I demonstrate the process, including TVM installation via pip, Conda setup on WSL, and Vulkan SDK installation for optimal performance. Navigating the MLC Chat exploration, I detail creating a Conda environment and running MLC LLM&amp;rsquo;s CLI version, offering a glimpse into its potential through a sample question. With MLC LLM and MLC Chat at your fingertips, the world of machine learning and language understanding unfolds boundless possibilities. üöÄüß†</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/posts/2023/08/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/posts/2023/08/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>Embarking on my journey with vLLM, I explore its potential for streamlined Large Language Model (LLM) inference and deployment. The blog details my personal experience setting up vLLM on a Windows Subsystem for Linux (WSL) instance running Ubuntu 22.04. I meticulously guide through installing WSL, NVIDIA GPU drivers, CUDA Toolkit, and Docker for efficient utilization. Delving into vLLM setup within the NVIDIA PyTorch Docker image, I navigate through the installation process and launch the API server. The blog provides insights into querying the model and creating a Docker image snapshot, offering a comprehensive guide to efficient language model serving.</description>
    </item>
  </channel>
</rss>
