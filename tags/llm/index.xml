<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/llm/</link>
    <description>Recent content in LLM on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 25 Aug 2024 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Audio Generation with NVIDIA Jetson Orin NX</title>
      <link>https://seehiong.github.io/2024/audio-generation-with-nvidia-jetson-orin-nx/</link>
      <pubDate>Sun, 25 Aug 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/audio-generation-with-nvidia-jetson-orin-nx/</guid>
      <description>This post explores the audio generation capabilities of the NVIDIA Jetson Orin NX. It covers transcribing audio using Whisper, setting up text-to-speech (TTS) and automatic speech recognition (ASR) with Llamaspeak, and preparing the RIVA server for advanced speech AI applications. Detailed instructions and command examples are provided, making it easy for developers to experiment with these tools on the Jetson platform.</description>
    </item>
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>In this post, we explored deploying Lightweight Language Models (LLMs) using WasmEdge, a high-performance WebAssembly runtime, within a HomeLab environment. The process involved preparing an OpenAI-compatible API server, configuring the Wasi-NN plugin, and deploying the setup to HomeLab using Kubernetes (K3s). The post also detailed the steps for testing the API server and integrating it into a Java application. Overall, the guide provides a comprehensive walkthrough of hosting and utilizing LLMs with WasmEdge in a local environment.</description>
    </item>
    <item>
      <title>Integrating NFS for Improved Scalability</title>
      <link>https://seehiong.github.io/2024/integrating-nfs-for-improved-scalability/</link>
      <pubDate>Sun, 07 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/integrating-nfs-for-improved-scalability/</guid>
      <description>In this post, we explored the integration of NFS to enhance scalability in deploying LLM models within a home lab. Setting up NFS involved connecting to a TerraMaster NAS, and the K3s cluster was configured to dynamically provision storage. With NFS in place, the deployment process became more efficient, eliminating the need to rebuild images for each new model introduction. The post detailed the setup steps, from configuring NFS and K3s to deploying LLM models dynamically. This approach simplifies the scaling of Language Models, providing a centralized and scalable storage solution through NFS in a Kubernetes environment.</description>
    </item>
    <item>
      <title>Unveiling Agent AutoBuild in Autogen</title>
      <link>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</link>
      <pubDate>Sun, 17 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unveiling-agent-autobuild-in-autogen/</guid>
      <description>&lt;p&gt;Discover the capabilities of &lt;a href=&#34;https://microsoft.github.io/autogen/blog/2023/11/26/Agent-AutoBuild/&#34; target=&#34;_blank&#34;&gt;Agent AutoBuild&lt;/a&gt; in my recent exploration with Autogen using &lt;em&gt;app.py&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setup-configuration&#34;&gt;Setup Configuration&lt;/h2&gt;&#xA;&lt;p&gt;In my model setup configuration, defined in &lt;em&gt;OAI_CONFIG_LIST&lt;/em&gt;, I&amp;rsquo;m leveraging the latest version of Autogen (&lt;em&gt;pyautogen==0.2.2&lt;/em&gt;) with the following specifications:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gpt-4&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;api_key&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NULL&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://192.168.68.114:1234/v1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Empowering Autogen: Enabling Seamless Java Code Execution</title>
      <link>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</link>
      <pubDate>Sun, 10 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/empowering-autogen-enabling-seamless-java-code-execution/</guid>
      <description>&lt;p&gt;In the pursuit of enhancing Autogen&amp;rsquo;s capabilities, I drew inspiration from &lt;a href=&#34;https://github.com/0xlws/autogen&#34; target=&#34;_blank&#34;&gt;0xlws&amp;rsquo; fork&lt;/a&gt; supporting JavaScript. This led me to embark on a journey to modify Autogen, enabling robust support for Java code execution.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up&#34;&gt;Setting up&lt;/h2&gt;&#xA;&lt;p&gt;Begin by ensuring that Java is installed on your Windows Subsystem for Linux (WSL) using the following command:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install openjdk-17-jdk-headless&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Multi-agent Conservation with Autogen</title>
      <link>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</link>
      <pubDate>Fri, 08 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/multi-agent-conservation-autogen/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll walk you through setting up a multi-agent conservation using Autogen. Building upon the concepts explored in a previous post &lt;a href=&#34;https://seehiong.github.io/archives/2023/exploring-autogen-with-lm-studio-and-local-llm/&#34; target=&#34;_blank&#34;&gt;Exploration with Autogen&lt;/a&gt; and following the example of &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat&#34; target=&#34;_blank&#34;&gt;Automated Multi Agent Chat&lt;/a&gt;, we&amp;rsquo;ll delve into the steps to create a dynamic debate environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;agent-setup&#34;&gt;Agent Setup&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll be setting up two agents: &lt;strong&gt;for_motion&lt;/strong&gt; and &lt;strong&gt;against_motion&lt;/strong&gt;. Each agent will engage in a debate on a given topic, providing examples and substantiating their points. A facilitator will oversee the debate rounds, ensuring that each response exceeds 300 words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG over Java code with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</link>
      <pubDate>Sat, 11 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/rag-over-java-code-with-langchain4j/</guid>
      <description>&lt;p&gt;Expanding upon the concepts introduced in the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; and drawing inspiration from &lt;a href=&#34;https://python.langchain.com/docs/use_cases/question_answering/code_understanding&#34; target=&#34;_blank&#34;&gt;RAG over code&lt;/a&gt;, this article dives into the integration of a Retrieval-Augmented Generation (RAG) service. The goal is to empower users to query their Java codebase effectively.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To embark on this journey, I&amp;rsquo;ve opted for &lt;a href=&#34;https://javaparser.org/&#34; target=&#34;_blank&#34;&gt;Java Parser&lt;/a&gt; , a powerful tool for traversing Java source code. Let&amp;rsquo;s begin by incorporating the latest version of Java Parser into our build.gradle file:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building an AI Application with Langchain4j</title>
      <link>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</link>
      <pubDate>Tue, 07 Nov 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-an-ai-application-with-langchain4j/</guid>
      <description>&lt;p&gt;In this blog post, I&amp;rsquo;ll walk you through my journey of harnessing the capabilities of &lt;a href=&#34;https://github.com/langchain4j/langchain4j/&#34; target=&#34;_blank&#34;&gt;langchain4j&lt;/a&gt; to craft a powerful AI application using Java, specifically with a local language model. Unlike my previous exploration with Python, this post focuses on the Java implementation with Langchain4j.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;&#xA;&lt;p&gt;To kick things off, I&amp;rsquo;ve chosen &lt;a href=&#34;https://spring.io/tools&#34; target=&#34;_blank&#34;&gt;STS4&lt;/a&gt; as my Integrated Development Environment (IDE) and opted for &lt;a href=&#34;https://adoptium.net/temurin/archive/?version=17&#34; target=&#34;_blank&#34;&gt;Java 17&lt;/a&gt; as my programming language. Leveraging &lt;a href=&#34;https://www.postman.com/&#34; target=&#34;_blank&#34;&gt;Postman&lt;/a&gt; as my API platform and &lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34;&gt;Spring Boot&lt;/a&gt; as the framework of choice, let&amp;rsquo;s delve into the process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unlocking the Power of Machine Learning with MLC LLM</title>
      <link>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</link>
      <pubDate>Sat, 02 Sep 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unlocking-the-power-of-machine-learning-with-mlc-llm/</guid>
      <description>&lt;p&gt;Machine Learning Compilation for LLM, or &lt;a href=&#34;https://mlc.ai/mlc-llm/docs/index.html&#34; target=&#34;_blank&#34;&gt;MLC LLM&lt;/a&gt;, is a cutting-edge universal deployment solution for large language models. In this blog post, we&amp;rsquo;ll guide you through the setup process and show you how to harness the immense potential of MLC LLM.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-your-environment&#34;&gt;Setting Up Your Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started with MLC LLM, you need to set up your environment properly. Follow these steps:&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-install-tvm&#34;&gt;1. Install TVM&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://tvm.apache.org/docs/install/index.html&#34; target=&#34;_blank&#34;&gt;TVM&lt;/a&gt;  is a critical component for MLC LLM. You can install it locally using pip:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Utilizing vLLM for Efficient Language Model Serving</title>
      <link>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</link>
      <pubDate>Sun, 20 Aug 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/utilizing-vllm-for-efficient-language-model-serving/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://vllm.ai/&#34; target=&#34;_blank&#34;&gt;vLLM&lt;/a&gt; is an open-source library designed for rapid LLM (Large Language Model) inference and deployment. It leverages their novel algorithm called &lt;strong&gt;PagedAttention&lt;/strong&gt;, which optimizes the management of attention keys and values.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;In this blog post, I will share my experience of utilizing vLLM on a WSL (Windows Subsystem for Linux) instance running Ubuntu 22.04. Let&amp;rsquo;s start by setting up the environment:&lt;/p&gt;&#xA;&lt;h3 id=&#34;installing-wsl-and-configuring-ubuntu&#34;&gt;Installing WSL and Configuring Ubuntu&lt;/h3&gt;&#xA;&lt;p&gt;Begin by installing WSL and configuring it to use Ubuntu as the default distribution:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
