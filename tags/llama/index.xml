<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLaMA on See Hiong&#39;s Blog</title>
    <link>https://seehiong.github.io/tags/llama/</link>
    <description>Recent content in LLaMA on See Hiong&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 13 Jan 2024 20:00:00 +0800</lastBuildDate>
    <atom:link href="https://seehiong.github.io/tags/llama/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploying LLMs with WasmEdge in HomeLab</title>
      <link>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</link>
      <pubDate>Sat, 13 Jan 2024 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/2024/deploying-llms-with-wasmedge-in-homelab/</guid>
      <description>&lt;p&gt;In this post, we delve into the deployment of Lightweight Language Models (LLMs) using &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge&#34; target=&#34;_blank&#34;&gt;WasmEdge&lt;/a&gt;, a lightweight, high-performance, and extensible WebAssembly runtime. This setup is tailored to run LLMs in our previously configured HomeLab environment.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;To establish an OpenAI-compatible &lt;a href=&#34;https://github.com/LlamaEdge/LlamaEdge/tree/main/llama-api-server&#34; target=&#34;_blank&#34;&gt;API server&lt;/a&gt;, begin by downloading the API server application:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For Rust-based &lt;a href=&#34;https://wasmedge.org/docs/develop/rust/wasinn/llm_inference/&#34; target=&#34;_blank&#34;&gt;Llama 2 inference&lt;/a&gt;, we require the &lt;a href=&#34;https://wasmedge.org/docs/contribute/source/plugin/wasi_nn/&#34; target=&#34;_blank&#34;&gt;Wasi-NN&lt;/a&gt; plugin. The &lt;em&gt;Dockerfile&lt;/em&gt; below reflects this configuration:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying OpenAI-Compatible LLAMA CPP Server with K3S</title>
      <link>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</link>
      <pubDate>Fri, 22 Dec 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/deploying-openai-compatible-llama-cpp-server-with-k3s/</guid>
      <description>&lt;p&gt;Commencing my week-long Christmas break, I extend the concepts from my &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; to establish an OpenAI-compatible server in my &lt;a href=&#34;https://seehiong.github.io/archives/2023/setting-up-k3s/&#34; target=&#34;_blank&#34;&gt;Home Lab&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;technical-setup&#34;&gt;Technical Setup&lt;/h2&gt;&#xA;&lt;p&gt;After fine-tuning a sample &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python/blob/main/docker/openblas_simple/Dockerfile&#34; target=&#34;_blank&#34;&gt;Dockerfile&lt;/a&gt;, I reinstalled my Ubuntu server, incorporating necessary adjustments. The subsequent setup commands, reflecting my Home Lab&amp;rsquo;s new IP address (192.168.68.115), include:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt update &amp;amp; sudo apt upgrade -y&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install docker&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install docker.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo usermod -aG docker pi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install Anaconda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chmod +x Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;./Anaconda3-2023.09-0-Linux-x86_64.sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Init conda&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source /home/pi/anaconda3/bin/activate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda init&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create -n docker-llama python&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate docker-llama &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Unleashing the Power of LLaMA Server in Docker Container</title>
      <link>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</link>
      <pubDate>Sat, 15 Jul 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/unleashing-the-power-of-llama-server-in-docker-container/</guid>
      <description>&lt;p&gt;Having recently completed the enlightening &lt;a href=&#34;https://www.coursera.org/learn/generative-ai-with-llms&#34; target=&#34;_blank&#34;&gt;Generative AI with Large Language Models&lt;/a&gt; course, where we gained invaluable knowledge and hands-on skills, we are now excited to share an exhilarating experience of running the LLaMA model in a Dockerized container.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, we&amp;rsquo;ll walk you through the setup and demonstrate how to unleash the full potential of running LLaMA Server within a Docker container.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;the-setup&#34;&gt;The Setup&lt;/h2&gt;&#xA;&lt;p&gt;Before we delve into the magic of LLaMA, let&amp;rsquo;s set up our application structure. To ensure smooth execution, we&amp;rsquo;ve structured our project as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to summarize YouTube Videos in Minutes (II)</title>
      <link>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</link>
      <pubDate>Fri, 16 Jun 2023 20:00:00 +2000</pubDate>
      <guid>https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-2/</guid>
      <description>&lt;p&gt;In continuation with the &lt;a href=&#34;https://seehiong.github.io/archives/2023/how-to-summarize-youtube-videos-in-minutes-1/&#34; target=&#34;_blank&#34;&gt;previous&lt;/a&gt; post, we will explore the power of AI by leveraging the &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34; target=&#34;_blank&#34;&gt;whisper.cpp&lt;/a&gt; library to convert audio to text, extracting audio from YouTube videos using &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34; target=&#34;_blank&#34;&gt;yt-dlp&lt;/a&gt;, and demonstrating how to utilize AI models like GPT4All and OpenAI for summarization.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-the-environment&#34;&gt;Setting Up the Environment&lt;/h2&gt;&#xA;&lt;p&gt;To get started, we need to set up the necessary tools and libraries. Follow the steps below:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Autofill PDF with LangChain and LangFlow</title>
      <link>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</link>
      <pubDate>Fri, 26 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/auto-fill-pdf-with-langchain-and-langflow/</guid>
      <description>&lt;p&gt;In this blog post, we will explore the usage of &lt;a href=&#34;https://pypi.org/project/langflow/&#34; target=&#34;_blank&#34;&gt;LangFlow&lt;/a&gt;, a Python library available on PyPI, to streamline the process of capturing ideas and conducting proof-of-concepts for our intended use case. Considering the current &amp;ldquo;trend&amp;rdquo; of tech layoffs, there might be a time (touch wood) where there is a need to go for interviews and fill-up various interview forms that require filling out personal information. Building upon the previous blog post on running GPT4All for PostgreSQL with LangChain (referenced &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-gpt4all-for-postgresql-with-langchain/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), we will now leverage LangFlow and OpenAI to automate the population of a sample employment form with our personal data stored in PostgreSQL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA server in local machine</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</link>
      <pubDate>Sat, 13 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-server-in-local-machine/</guid>
      <description>&lt;p&gt;Referencing the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, we will run a web server which aims to act as a drop-in replacement for the OpenAI API, which can in turn be used by &lt;a href=&#34;https://seehiong.github.io/archives/2023/developing-byo-gpt-with-flutter/&#34; target=&#34;_blank&#34;&gt;byogpt&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(3 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://packaging.python.org/en/latest/key_projects/#pipenv&#34; target=&#34;_blank&#34;&gt;Pipenv&lt;/a&gt; aims to help users manage environments, dependencies and imported packages and I will be using it in this guide.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install pipenv uvicorn fastapi sse_starlette&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipenv shell&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Building ChatBot for your PDF files with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</link>
      <pubDate>Sun, 07 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-chatbot-for-your-pdf-files-with-langchain/</guid>
      <description>&lt;p&gt;Extending the use case on the &lt;a href=&#34;https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I will demostrate how you could ingest your own PDF file to your own &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;LLaMa model in local machine&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start off by installing &lt;a href=&#34;https://github.com/chroma-core/chroma&#34; target=&#34;_blank&#34;&gt;Chroma&lt;/a&gt;, the open-source embedding database:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install chromadb pypdf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/pdf/langchain-chromadb-install.png&#34; alt=&#34;langchain-chromadb-install&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;ingesting-your-pdf&#34;&gt;Ingesting your PDF&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(5 mins)&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a basic Chain with LangChain</title>
      <link>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</link>
      <pubDate>Mon, 01 May 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/building-a-basic-chain-with-langchain/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://js.langchain.com/docs/&#34; target=&#34;_blank&#34;&gt;LangChain&lt;/a&gt; is a framework for developing applications powered by language models. With the &lt;a href=&#34;https://seehiong.github.io/archives/2023/running-llama-model-locally/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; setup, I will follow closely to using &lt;a href=&#34;https://python.langchain.com/en/latest/ecosystem/llamacpp.html&#34; target=&#34;_blank&#34;&gt;Llama.cpp within LangChain&lt;/a&gt; for building the simplest form of chain with LangChain.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(2 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, installs the required python packages:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo pip install llama-cpp-python langchain &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/langchain/langchain-llama-dependencies.png&#34; alt=&#34;langchain-llama-dependencies&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running LLaMA model locally</title>
      <link>https://seehiong.github.io/archives/2023/running-llama-model-locally/</link>
      <pubDate>Sun, 30 Apr 2023 20:00:00 +0800</pubDate>
      <guid>https://seehiong.github.io/archives/2023/running-llama-model-locally/</guid>
      <description>&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;(30 mins)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34; target=&#34;_blank&#34;&gt;LLaMA&lt;/a&gt; is a collection of foundation language models ranging from 7B to 65B parameters.&lt;/p&gt;&#xA;&lt;p&gt;In this guide, I will be using and following &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34; target=&#34;_blank&#34;&gt;Georgi Gergano&amp;rsquo;s llama.cpp&lt;/a&gt;, a inference of LLaMA model in pure C/C++.&lt;/p&gt;&#xA;&lt;p&gt;I will be setting this up in a Ubuntu machine with 32Gb.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://seehiong.github.io/images/llama/hp-system-info.png&#34; alt=&#34;hp-system-info&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;To prepare for the build system, I installed these:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
